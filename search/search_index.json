{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Roadmap web developer in 2019 The purpose of these roadmap is to give you an idea about the landscape and to guide you if you are confused about what to learn nexet and not to encourage you to pick what is hip and trendy. You should grow some understanding of why one tool would better be suited for some cases than the other and remember hip and trendy never means beast suited for the job. FRONT-END Roadmap BACK-END Roadmap DEV-OPS Roadmap Get more information at here .","title":"Home"},{"location":"#roadmap-web-developer-in-2019","text":"The purpose of these roadmap is to give you an idea about the landscape and to guide you if you are confused about what to learn nexet and not to encourage you to pick what is hip and trendy. You should grow some understanding of why one tool would better be suited for some cases than the other and remember hip and trendy never means beast suited for the job.","title":"Roadmap web developer in 2019"},{"location":"#front-end-roadmap","text":"","title":"FRONT-END Roadmap"},{"location":"#back-end-roadmap","text":"","title":"BACK-END Roadmap"},{"location":"#dev-ops-roadmap","text":"Get more information at here .","title":"DEV-OPS Roadmap"},{"location":"daily-plans/plans-temp/","text":"Temporary","title":"Temporary"},{"location":"daily-plans/plans-temp/#temporary","text":"","title":"Temporary"},{"location":"daily-plans/plans-version-1.1/","text":"Plans for a wonderful day (version 1) List View Items Plans for a wonderful day (version 1) List View Items Time planning Ranking Social Network Ranking Coding Website Ranking Learning Knowledge Story about improving skill Time planning 7 a.m : Starting a day and thinking about dreams (not girls). 7.30 a.m : Go to school with highspeed (the first lesson starts at 7.30 a.m ). 8 -> 9.45 a.m : Learning at school. Get knowledge (not sleep). 10 a.m : Go to Youthdev Campany. It take me 30 - minutes and i stay at campany at 10.45 a.m 11 a.m : Get list of stories (medium + twitter) to read. View Odoo for my daily mission. View pm.youthdev.net to find my task. Have a daily standup with dat.hoang. 11 -> 12.30 a.m :Reading some of stories which is pinned. I will update my document with overview abour stories (markdown + pin link to web story) and my Odoo's task . 12.35 -> 1 p.m : Go out camany to have a lunch. Playing with twitter. 1 -> 1.15 p.m : Get a cup of milk cafe and go back campany to work.Reading about new technique after writing overview to own note. 1.20 -> 2 p.m : Reading a story from some books of Trung Nguyen Cafe. 2 p.m -> 6 p.m : Crazy and Work with high spirits. After i learn a new knowledge, i will pin web's link to my docs as same as some of stories. 6 -> 8 p.m : Eating Dinner and Reading again Stories I read that day. Note sumary and update to my docs over pages which haven't been commited yet. (strick for get new knowledge more benifits). 8 -> 11 p.m : solving a some issues for coding of techniques. improve them for some things better. 11 -> 12 p.m : Get a cup of milk cafe to think about the future. Find values of mine to get things better for new day. 0 -> 3 a.m : Improve My Skills. (coding, algorithm, English). Ranking Social Network Medium Twitter Channel 9 Dev.to Stackoverflow Microsoft Dev Blogs + Xamarin Developers XamGirl XamBoy XamarinInsiders Ranking Coding Website Hackerrank Codingforces Topcoder LeetCode LintCode Ranking Learning Knowledge Geeks For Geeks .NET Foundation Microsoft Courses Story about improving skill Learning deep for improving code. Learning document and coding some lines cool After reading a story, create template of markdown file, and give link to content (pin link and not commit). Go home reading some stories again and note sumay to my docs. Commit and push docs to github and vsts. Improve power of my docs, help it run automatic and async with more enviroment. Manage task at Odoo's task .","title":"Version 1"},{"location":"daily-plans/plans-version-1.1/#plans-for-a-wonderful-day-version-1","text":"","title":"Plans for a wonderful day (version 1)"},{"location":"daily-plans/plans-version-1.1/#list-view-items","text":"Plans for a wonderful day (version 1) List View Items Time planning Ranking Social Network Ranking Coding Website Ranking Learning Knowledge Story about improving skill","title":"List View Items"},{"location":"daily-plans/plans-version-1.1/#time-planning","text":"7 a.m : Starting a day and thinking about dreams (not girls). 7.30 a.m : Go to school with highspeed (the first lesson starts at 7.30 a.m ). 8 -> 9.45 a.m : Learning at school. Get knowledge (not sleep). 10 a.m : Go to Youthdev Campany. It take me 30 - minutes and i stay at campany at 10.45 a.m 11 a.m : Get list of stories (medium + twitter) to read. View Odoo for my daily mission. View pm.youthdev.net to find my task. Have a daily standup with dat.hoang. 11 -> 12.30 a.m :Reading some of stories which is pinned. I will update my document with overview abour stories (markdown + pin link to web story) and my Odoo's task . 12.35 -> 1 p.m : Go out camany to have a lunch. Playing with twitter. 1 -> 1.15 p.m : Get a cup of milk cafe and go back campany to work.Reading about new technique after writing overview to own note. 1.20 -> 2 p.m : Reading a story from some books of Trung Nguyen Cafe. 2 p.m -> 6 p.m : Crazy and Work with high spirits. After i learn a new knowledge, i will pin web's link to my docs as same as some of stories. 6 -> 8 p.m : Eating Dinner and Reading again Stories I read that day. Note sumary and update to my docs over pages which haven't been commited yet. (strick for get new knowledge more benifits). 8 -> 11 p.m : solving a some issues for coding of techniques. improve them for some things better. 11 -> 12 p.m : Get a cup of milk cafe to think about the future. Find values of mine to get things better for new day. 0 -> 3 a.m : Improve My Skills. (coding, algorithm, English).","title":"Time planning"},{"location":"daily-plans/plans-version-1.1/#ranking-social-network","text":"Medium Twitter Channel 9 Dev.to Stackoverflow Microsoft Dev Blogs + Xamarin Developers XamGirl XamBoy XamarinInsiders","title":"Ranking Social Network"},{"location":"daily-plans/plans-version-1.1/#ranking-coding-website","text":"Hackerrank Codingforces Topcoder LeetCode LintCode","title":"Ranking Coding Website"},{"location":"daily-plans/plans-version-1.1/#ranking-learning-knowledge","text":"Geeks For Geeks .NET Foundation Microsoft Courses","title":"Ranking Learning Knowledge"},{"location":"daily-plans/plans-version-1.1/#story-about-improving-skill","text":"Learning deep for improving code. Learning document and coding some lines cool After reading a story, create template of markdown file, and give link to content (pin link and not commit). Go home reading some stories again and note sumay to my docs. Commit and push docs to github and vsts. Improve power of my docs, help it run automatic and async with more enviroment. Manage task at Odoo's task .","title":"Story about improving skill"},{"location":"daily-plans/plans-version-2/","text":"Plans for a wonderful day (version 2) List View Items Plans for a wonderful day (version 2) List View Items Time planning Courses Online on Edx Ranking Social Network Ranking Coding Website Ranking Learning Knowledge Story about improving skill Time planning 7 a.m : Starting a day with 15 minutes about foreign languages and thinking about dreams (not girls). 7.30 a.m : Go to school with highspeed (the first lesson starts at 7.30 a.m ). 8 -> 9.45 a.m : Learning at school. Get knowledge (not sleeping). 10 a.m : Go to Youthdev Company. It take me 30 - minutes and I come company at 10.45 a.m 11 a.m -> 12.30 a.m : View Odoo for my daily mission. View pm.youthdev.net to find my tasks. Have a daily standup with dat.hoang. Learning for algorithms at Geeksforgeeks. Get documents for working. 12.35 -> 1 p.m : Go out company to have a lunch. Connect to twitter network social and Reading one of stories which is pinned. 1 -> 1.15 p.m : Get a cup of milk-cafe and go back campany to work. 1.20 -> 2 p.m : Improving skills coding in languages (C#, Typescript, python) over website coding .Reading a story from some books of Trung Nguyen Cafe. I will update my document with overview about stories (markdown + pin link to web story) and my Odoo's task . 2 p.m -> 6 p.m : Crazy and Work with high spirits. After I learn a new knowledge,I will pin web's link to my docs as same as some of stories. 6 -> 8 p.m : Reading technique news on Twitter network social.Note summary and update to my docs over pages. Learn cources online on Edx . 8 -> 11 p.m : Working on own Project(freelancer, schedules). 11 -> 12 p.m : Get a cup of milk cafe to think about the future. Find values of mine to get things better for new day. 0 -> 3 a.m : Learn Foreign Languages (English and Japanese). Courses Online on Edx Mirosoft Professional Program in DevOps Bitcoin and Cryptocurrencies Blockchain Technology Linux Basics: The Command line interface Introduction to Linux Ranking Social Network Medium Twitter Microsoft Dev Blogs + Xamarin Developers Windows Blog Channel 9 Dev.to Stackoverflow XamGirl XamBoy XamarinInsiders Ranking Coding Website LeetCode LintCode Hackerrank Codingforces Topcoder Stackblitz Ranking Learning Knowledge Geeks For Geeks Microsoft Courses Hackr.io Edx SoloLearn .NET Foundation Story about improving skill Learning deep for improving code. Learning document and coding some lines cool After reading a story, create template of markdown file, and give link to content (pin link and not commit). Go home reading some stories again and note sumay to my docs. Commit and push docs to github and vsts. Improve power of my docs, help it run automatic and async with more enviroment. Manage task at Odoo's task .","title":"Version 2"},{"location":"daily-plans/plans-version-2/#plans-for-a-wonderful-day-version-2","text":"","title":"Plans for a wonderful day (version 2)"},{"location":"daily-plans/plans-version-2/#list-view-items","text":"Plans for a wonderful day (version 2) List View Items Time planning Courses Online on Edx Ranking Social Network Ranking Coding Website Ranking Learning Knowledge Story about improving skill","title":"List View Items"},{"location":"daily-plans/plans-version-2/#time-planning","text":"7 a.m : Starting a day with 15 minutes about foreign languages and thinking about dreams (not girls). 7.30 a.m : Go to school with highspeed (the first lesson starts at 7.30 a.m ). 8 -> 9.45 a.m : Learning at school. Get knowledge (not sleeping). 10 a.m : Go to Youthdev Company. It take me 30 - minutes and I come company at 10.45 a.m 11 a.m -> 12.30 a.m : View Odoo for my daily mission. View pm.youthdev.net to find my tasks. Have a daily standup with dat.hoang. Learning for algorithms at Geeksforgeeks. Get documents for working. 12.35 -> 1 p.m : Go out company to have a lunch. Connect to twitter network social and Reading one of stories which is pinned. 1 -> 1.15 p.m : Get a cup of milk-cafe and go back campany to work. 1.20 -> 2 p.m : Improving skills coding in languages (C#, Typescript, python) over website coding .Reading a story from some books of Trung Nguyen Cafe. I will update my document with overview about stories (markdown + pin link to web story) and my Odoo's task . 2 p.m -> 6 p.m : Crazy and Work with high spirits. After I learn a new knowledge,I will pin web's link to my docs as same as some of stories. 6 -> 8 p.m : Reading technique news on Twitter network social.Note summary and update to my docs over pages. Learn cources online on Edx . 8 -> 11 p.m : Working on own Project(freelancer, schedules). 11 -> 12 p.m : Get a cup of milk cafe to think about the future. Find values of mine to get things better for new day. 0 -> 3 a.m : Learn Foreign Languages (English and Japanese).","title":"Time planning"},{"location":"daily-plans/plans-version-2/#courses-online-on-edx","text":"Mirosoft Professional Program in DevOps Bitcoin and Cryptocurrencies Blockchain Technology Linux Basics: The Command line interface Introduction to Linux","title":"Courses Online on Edx"},{"location":"daily-plans/plans-version-2/#ranking-social-network","text":"Medium Twitter Microsoft Dev Blogs + Xamarin Developers Windows Blog Channel 9 Dev.to Stackoverflow XamGirl XamBoy XamarinInsiders","title":"Ranking Social Network"},{"location":"daily-plans/plans-version-2/#ranking-coding-website","text":"LeetCode LintCode Hackerrank Codingforces Topcoder Stackblitz","title":"Ranking Coding Website"},{"location":"daily-plans/plans-version-2/#ranking-learning-knowledge","text":"Geeks For Geeks Microsoft Courses Hackr.io Edx SoloLearn .NET Foundation","title":"Ranking Learning Knowledge"},{"location":"daily-plans/plans-version-2/#story-about-improving-skill","text":"Learning deep for improving code. Learning document and coding some lines cool After reading a story, create template of markdown file, and give link to content (pin link and not commit). Go home reading some stories again and note sumay to my docs. Commit and push docs to github and vsts. Improve power of my docs, help it run automatic and async with more enviroment. Manage task at Odoo's task .","title":"Story about improving skill"},{"location":"daily-plans/plans-version-3/","text":"Plans for a wonderful day (version 3) Time planning All options which have to gain in a day. :D Learn Foreign Languages: Enslish (advantage), Chinese (basic) Done a task in YouTrack board. Read some posts at Medium story and dev.to . Fighting all of powers to work at Company. Learn a new tech and fix some error for myself about knowledge. Writing code to resolve challenges at LeetCode EveryThing is Synchronize. Go to Future with System Administrator and AI Engineer Courses Online on Edx Mirosoft Professional Program in DevOps Bitcoin and Cryptocurrencies Blockchain Technology Linux Basics: The Command line interface Introduction to Linux Ranking Social Network Dev.to Twitter Medium Channel 9 Microsoft Dev Blogs + Xamarin Developers Stackoverflow Windows Blog XamGirl XamBoy XamarinInsiders Ranking Coding Website LeetCode LintCode Hackerrank Codingforces Topcoder Stackblitz Ranking Learning Knowledge Geeks For Geeks Microsoft Courses Scotch.io DevIQ Hackr.io Edx SoloLearn .NET Foundation","title":"Version 3"},{"location":"daily-plans/plans-version-3/#plans-for-a-wonderful-day-version-3","text":"","title":"Plans for a wonderful day (version 3)"},{"location":"daily-plans/plans-version-3/#time-planning","text":"All options which have to gain in a day. :D Learn Foreign Languages: Enslish (advantage), Chinese (basic) Done a task in YouTrack board. Read some posts at Medium story and dev.to . Fighting all of powers to work at Company. Learn a new tech and fix some error for myself about knowledge. Writing code to resolve challenges at LeetCode EveryThing is Synchronize. Go to Future with System Administrator and AI Engineer","title":"Time planning"},{"location":"daily-plans/plans-version-3/#courses-online-on-edx","text":"Mirosoft Professional Program in DevOps Bitcoin and Cryptocurrencies Blockchain Technology Linux Basics: The Command line interface Introduction to Linux","title":"Courses Online on Edx"},{"location":"daily-plans/plans-version-3/#ranking-social-network","text":"Dev.to Twitter Medium Channel 9 Microsoft Dev Blogs + Xamarin Developers Stackoverflow Windows Blog XamGirl XamBoy XamarinInsiders","title":"Ranking Social Network"},{"location":"daily-plans/plans-version-3/#ranking-coding-website","text":"LeetCode LintCode Hackerrank Codingforces Topcoder Stackblitz","title":"Ranking Coding Website"},{"location":"daily-plans/plans-version-3/#ranking-learning-knowledge","text":"Geeks For Geeks Microsoft Courses Scotch.io DevIQ Hackr.io Edx SoloLearn .NET Foundation","title":"Ranking Learning Knowledge"},{"location":"daily-plans/soft-skills-for-learning/","text":"Soft Skills for Learning. Reading document before working. After reading let's note. Create Sub folder for options have child. Help for managing document and visual. create file index.md to view over features. Create a root page for storing image. name extensionn with type of resources. Thinking about my issues while writing blog or document Writing quickly to have no time for thing about other issues in the past.","title":"Soft Skills for Learning"},{"location":"daily-plans/soft-skills-for-learning/#soft-skills-for-learning","text":"Reading document before working. After reading let's note. Create Sub folder for options have child. Help for managing document and visual. create file index.md to view over features. Create a root page for storing image. name extensionn with type of resources. Thinking about my issues while writing blog or document Writing quickly to have no time for thing about other issues in the past.","title":"Soft Skills for Learning."},{"location":"knowledge/algorithms/analysis-of-algorithms/","text":"Analysis of Algorithms Asymptotic Analysis Given two algoithms for a task, how do we find out which one is better? One native way of soing this is - implwmwnt both the algorithms and run the two programs on your computer for different inputs and see which one takes less time. There are many problems with this approach for analysis of algorithms. It might be possible that for some inputs, first algorithm performs better than the second. And for some inputs second performs better. It might also be possible that for some inputs, first algorithm perform better on one machine and the second works better on other machine for some other inputs. Asymptotic Analysis is the big idea that handles above issues in analyzing algorithm in terms of input size(we don't measure the ectual running time). We calculate, how does the time (or space) taken by an algorithm increases with the input size. References: MIT's Video lecture 1 on Introduction to Algorithms. Worst, Average and Bast Cases Worst Case Analysis(Usually Done) In the worst case analysis, we calculate upper bound on running time of an algorithm. We must know the case that causes maximum number of operations to be executed. Average Case Analysis(Sometimes done) In average case analysis, we take all possible inputs and calculate computing time for all of the inputs. Sum all the calculated values and devide the sum by total number of inputs. We must know (or predict) distribution of cases. Best Case Analysis(Bogus) In the best case analysis, we calculate lower bound on running time of an algorithm. We must know the case that causes minimum number of operations to be executed. Most of the times, we do worst case analysis to analyze algorithms. In the worse analysis, we guarantee an upper bound on the running time of an algorithm which is good information. The average case analysis is not easy to do in most of the practical cases and it is rarely done. In the average case analysis, we must know (or predict) the mathematical distribution of all possible inputs. The Best Case analysis is bogus. Guaranteeing a lower bound on an algorithm doesn't provide any information as in the worse case, an algorithm may take years to run. Asymptotic Notations The main idea of asymptotic analysis is to have a measure of afficiency of algotithms that doesn't depend on machine specific constants, and doesn't require algorithms to be implemented and time taken by progrms to be compared. Asymptotic notations are mathematical tools to represent time complexity of algorihms for asymptotic analysis. 3 asymptotic notations are mostly used to represent time complexity of algorithms. \u0398 Notation : The theta notation bounds a functions from above and below so it defines exact asymprotic behavior. A simple way to get Theta notation of an expression is to drop low order terms and ignore leading constants. For a given function g(n), we denote \u0398(g(n)) is following set of functions. \u0398(g(n)) = {f(n): there exist positive constants c1, c2 and n0 such that 0 <= c1*g(n) <= f(n) <= c2*g(n) for all n >= n0} Big O Notation : The Big O notation defines an upper bound of an algorithm, it bounds a function only from above. The Bid O notation is useful when we only have upper bound on time complexity of an algorithm. Many times we easily find an upper bound by simply looking at the algorithm. O(g(n)) = { f(n): there exist positive constants c and n0 such that 0 <= f(n) <= c*g(n) for all n >= n0} \u03a9 Notation : Just as Big O notation provides an asymptotic upper bound on a function, \u03a9 notation provides an asymptotic lower bound. \u03a9 Notation can be useful when we have lower bound on time complexity of an algorithm. For a given function g(n), we denote by \u03a9(g(n)) the set of functions. \u03a9 (g(n)) = {f(n): there exist positive constants c and n0 such that 0 <= c*g(n) <= f(n) for all n >= n0}. Important Links : There are two more notations called little o and little omega . Analysis of Algorithm| Set 4 (Analysis of Loops) Recent Aritcles on analysis of algorithm . Little o and little omega notations The main idea of asymptotic analysis is to have a measure of efficiency of algorithms that doesn't depend on machine specific constants, mainly because this analysis doesn't require algorithms to be implemeneted nad time taken by programs to be compared. Little o asymptotic notation Definition : Let f(n) and g(n) be functions that map positive integers to positive real numbers. we say that f(n) is o(g(n)) (of f(n) E o(g(n))) if for any real constant c>0, there exists an integer constant n0>= 1 such that 0 <= f(n) < c*g(n). In mathematical relation, f(n) = o(g(n)) means lim f(n)/g(n) = 0 Little omega asymptotic notation Definition : Let f(n) and g(n) ne functions that map positive integers to positive real numbers. We say that f(n) is \u03c9(g(n)) (or f(n) \u2208 \u03c9(g(n))) if for any real constant c > 0, there exists an integer constant n0 >= 1 such that f(n) > c*g(n) >= 0 for every integer n >= n0 f(n) has a higher growth rate than g(n) so main difference between Big Omega(\u03a9) and little omega (\u03c9) lies in their definitions. In the case of Big Omega f(n)=\u03a9(g(n)) and the bound is 0<=cg(n)<=f(n), but in case of little omega, it is true for 0 <= c*g(n) < f(n). We use \u03c9 notation to denote a lower bound that is not asymptotically tight. and, f(n) \u2208 \u03c9(g(n)) if and only if g(n) \u2208 \u03bf((f(n)) In mathematical relation, if f(n) \u2208 \u03c9(g(n)) then lim f(n)/g(n) = \u221e Lower and Upper Bound Theory The Lower and Upper Bound Theory provides a way to find the lowest complexity algorithm to solve a problem. What actually Lower and Upper bounds are. Lower Bound - Let L(n) be the running time of an algorithm A(n), then g(n) is the Lower Bound of A if there exist two constants C and N such that L(n) <= C*g(n) for n > N. Lower bound of an algorithm is shown by the asymtotic notation called Big Omega Upper Bound - Let U(n) be the running time of an algorithm A, then g(n) is the Upper Bound of A if there exist two constants C and N such that U(n) >= C*g(n) for n > N. Upper bound of an algorithm is shown by the asymtotic notation called Big Oh(O) 1. Lower Bound Theory According to the lower bound theory, for a lower bound L(n) of an algorithm, it is not possible to have any other algorithm (for a common problem) whose time complexity is less than L(n) for random input. Also every algorithm must take at least L(n) time in worst case. Note that L(n) here is the minimum of all the possible algorithm, of maximum complexity. Note that our main motive is to get an optimal algorithm, which is the one having its Upper Bound Same as its Lower Bound (U(n)=L(n)). Trivial Lower Bound - It is the easiest method to find the lower bound. The Lower bounds which can be easily observed on the basis of the number of input taken and the number of output produces are called trivial Lower Bound. Multiplication of nxn matrix, where, Input: For 2 matrix we will have 2n2 inputs Output: 1 matrix of order n x n, i.e., n2 outputs Computational Model - The method is for all those alforithms that are comparison based. For example in sorting we have to compare the elements of the list among themselces and then sort them accordingly. Similar is the case with searching and thus we can implement the same in this case. Analysis of Loops O(1) : Time complexity of a function (or set of statements) is considered as O(1) if it doesn't contain loop, recursion and call to any other non-constant time function. // Here c is a constant for (int i = 1; i <= c; i++) { // some O(1) expressions } O(n) : Time Complexity of a loop is considered as O(n) if the loop variables is incremented/decremented by a constant amount. // Here c is a positive integer constant for (int i = 1; i <= n; i += c) { // some O(1) expressions } for (int i = n; i > 0; i -= c) { // some O(1) expressions } O(nc) : Time complexity of nested loops is equal to the number of times the innermost statement is executed. for (int i = 1; i <=n; i += c) { for (int j = 1; j <=n; j += c) { // some O(1) expressions } } for (int i = n; i > 0; i -= c) { for (int j = i+1; j <=n; j += c) { // some O(1) expressions } O(Logn) : Time Complexity of a loop is considered as O(Logn) if the loop variables is devidedd / multiplied by a constant amount. for (int i = 1; i <=n; i *= c) { // some O(1) expressions } for (int i = n; i > 0; i /= c) { // some O(1) expressions } O(LogLogn) : Time complexity of a loop is consider as O(LogLogn) f the loop variables is reduced / increased exponentially by a constant amount. // Here c is a constant greater than 1 for (int i = 2; i <=n; i = pow(i, c)) { // some O(1) expressions } //Here fun is sqrt or cuberoot or any other constant root for (int i = n; i > 1; i = fun(i)) { // some O(1) expressions } How to combine time complexities of consecutive loops? When there are consecutive loops, we calclate time complexity as sum f time complexities of individual loops for (int i = 1; i <=m; i += c) { // some O(1) expressions } for (int i = 1; i <=n; i += c) { // some O(1) expressions } Time complexity of above code is O(m) + O(n) which is O(m+n) If m == n, the time complexity becomes O(2n) which is O(n). Solving Recurrences Substitution Method : We make a guess for the solution and then we use mathematical induction to prove the guess is correct or incorrect. For example consider the recurrence T(n) = 2T(n/2) + n We guess the solution as T(n) = O(nLogn). Now we use induction to prove our guess. We need to prove that T(n) <= cnLogn. We can assume that it is true for values smaller than n. T(n) = 2T(n/2) + n <= cn/2Log(n/2) + n = cnLogn - cnLog2 + n = cnLogn - cn + n <= cnLogn Recurrence Tree Method : In this method, we draw a recurrence tree and calculate the time taken by everu level of tree. Finnaly, we sum the work done at all levels. To draw the recurrence tree, we start from the given recurrence and keep drawing till we find pattern among levels. The pattern is typically a arithmetic or geometric series. For example consider the recurrence relation T(n) = T(n/4) + T(n/2) + cn2 cn2 / \\ T(n/4) T(n/2) If we further break down the expression T(n/4) and T(n/2), we get following recursion tree. cn2 / \\ c(n2)/16 c(n2)/4 / \\ / \\ T(n/16) T(n/8) T(n/8) T(n/4) Breaking down further gives us following cn2 / \\ c(n2)/16 c(n2)/4 / \\ / \\ c(n2)/256 c(n2)/64 c(n2)/64 c(n2)/16 / \\ / \\ / \\ / \\ To know the value of T(n), we need to calculate sum of tree nodes level by level. If we sum the above tree level by level, we get the following series T(n) = c(n^2 + 5(n^2)/16 + 25(n^2)/256) + .... The above series is geometrical progression with ratio 5/16. To get an upper bound, we can sum the infinite series. We get the sum as (n2)/(1 - 5/16) which is O(n2) Master Method : Master Method is a direct way to get the solution. The maser method works only for following type of recurrences or for recurrences that can be transformed to follwoing type. T(n) = aT(n/b) + f(n) where a >= 1 and b > 1 There are following three cases: If f(n) = \u0398(nc) where c < c < Logba then T(b) = \u0398(nLogba) If f(n) = \u0398(nc) where c = Logba then T(n) = \u0398(ncLog n) If f(n) = \u0398(nc) where c > Logba then T(n) = \u0398(f(n)) How does this work? Master methd is mainly derived from recurrence tree method. If we draw recurrence tree of T(n) = aT(n/b) + f(n), we can see that the work done at root is f(n) and work done at all leaves is \u0398(nc) where c is Logba. And the height of recurrence tree is Logbn. In recurrence tree method, we calculate total work done. If the work done at leaves is polynomially, then leaves are the dominant part, and our result becomes the work done leaves (Case 1). If work done at leaves and root is asymptotically same, then our result becomes height multiplied by work done at any level (Case 2). If work done at root is asymptotically more, then our result becomes work done at root(Case 3). Amortized Analysis Introduction Amortized Analysis is used for algorithms where an occasional operation is very slow, but most of the other operations are faster. In Amortized Analysis, we analyze a sequence of oprations and guarantee a worst case average time which is lower than the worst case time of a particular expensive operation. The solution to this trade off problem is to use Dynamic Table (or Arrays). The idea is to increase size of table whenever it becomes full. Following are the steps to follow when table becomes full. Allocate memory for a larger table of size. Copy the contents of old table to new value. Free the old table. If the table has space available, we simply insert new item in available space. What is the time complexity of n insertions using the above scheme? If use simple analysis, the worst case cost of an insertion is O(n). Therefore, worst case cost of n inserts is n * O(n2). This analysis gives an upper bound, nit not a tight upper bound for n insertions as all insertions don't take \u0398(n) time. Folowing are few important notes. Amoritized cost of a sequence of operations can be seen as expenses of a salaried person. The average monthly expense of the person is less than or equal to the salary, but the person can spend more money in a particular month by buying a car or something. In other months, he or she saves money for the expensive month. The above Amoritized Analysis done for Dynamic Array example is called Aggregate Method . We will be discussing the other two methods in separate posts. The amoritized analysis doesn't involve probability. There is also another different notion of average case running time where algorithms use randomization to make them faster and expected running time is faster than worst case running time. These algorithms are analyzed using Randomized Analysis. Sources: : Berkeley Lecture 35: Amoritized Analysis . MIT Lecture 13: Amoritized Algorithms, Table Doubling, Potential Method amoritzed What does 'Space Complexity' mean? Space Complexity: The term Space Complexity is misused for Auxiliary Space at many places. Following are the correct definitions of Auxiliary Space and Space Complexity. Auxiliary Space is the extra space or temporary space used by an alogorithm. Space Complexity of an algorithm is total space taken by the algorithm with respect to the input size. Space complexity includes both Auxiliary space and space used by input. For example, if we want to compare standard sorting algorithms on the basis of space, then Auxiliary Space would be a better criteria than Space Complexity. Merge Sort uses O(n) auxiliary space, Insertion sort and Heap Sort use O(1) auxiliary space. Space complexity of all these sorting algorithms is O(n) though. Pseudo-polynomial Algorithms What is Pseudo-polynomial? An algorithm whose worst case time complexity depends on numeric value of input (not number of inputs) is called Pseudo-polynomial algorithm. On the other hand, an algorithm whose time complexity is only based on number of elements in array (not value) is considered as polynomial time algorithm. Pseudo-polynomial and NP-Completeness Some NP-Complete problems have Pseudo Polynomial time solutions. For example, Dynamic Programming Solutions of 0-1 Knapsack , Subset-Sum and Partition problems are Pseudo-Polynomial. NP complete problems that can be solved using a pseudo-polynomial time algorithms are called weakly NP-complete. Reference: Pseudo-polynomial_time wiki Polynomial Time Approximation Scheme It is a very well know fact that there is no known polynomial time solution for NP Complete problems and these problems occur a lot in real world. So there must be a way to handle them. We have seen algotithms to these problems which are approximate. P olynomial T ime A pproximation S cheme(PTAS) is a type of approximate algorithms that provide user to control over accuracy which is a desirable feature. These algorithms take an additional parameter e > 0 and provide a solution that is (1 + e) approximate for minimization and (1 - e) for maximization. In PTAS algorithms, the exponent of the polynomial can increase dramatically as e reduces. To be continued. Get more information at here .","title":"Analysis of Algorithms"},{"location":"knowledge/algorithms/analysis-of-algorithms/#analysis-of-algorithms","text":"","title":"Analysis of Algorithms"},{"location":"knowledge/algorithms/analysis-of-algorithms/#asymptotic-analysis","text":"","title":"Asymptotic Analysis"},{"location":"knowledge/algorithms/analysis-of-algorithms/#given-two-algoithms-for-a-task-how-do-we-find-out-which-one-is-better","text":"One native way of soing this is - implwmwnt both the algorithms and run the two programs on your computer for different inputs and see which one takes less time. There are many problems with this approach for analysis of algorithms. It might be possible that for some inputs, first algorithm performs better than the second. And for some inputs second performs better. It might also be possible that for some inputs, first algorithm perform better on one machine and the second works better on other machine for some other inputs. Asymptotic Analysis is the big idea that handles above issues in analyzing algorithm in terms of input size(we don't measure the ectual running time). We calculate, how does the time (or space) taken by an algorithm increases with the input size.","title":"Given two algoithms for a task, how do we find out which one is better?"},{"location":"knowledge/algorithms/analysis-of-algorithms/#references","text":"MIT's Video lecture 1 on Introduction to Algorithms.","title":"References:"},{"location":"knowledge/algorithms/analysis-of-algorithms/#worst-average-and-bast-cases","text":"","title":"Worst, Average and Bast Cases"},{"location":"knowledge/algorithms/analysis-of-algorithms/#worst-case-analysisusually-done","text":"In the worst case analysis, we calculate upper bound on running time of an algorithm. We must know the case that causes maximum number of operations to be executed.","title":"Worst Case Analysis(Usually Done)"},{"location":"knowledge/algorithms/analysis-of-algorithms/#average-case-analysissometimes-done","text":"In average case analysis, we take all possible inputs and calculate computing time for all of the inputs. Sum all the calculated values and devide the sum by total number of inputs. We must know (or predict) distribution of cases.","title":"Average Case Analysis(Sometimes done)"},{"location":"knowledge/algorithms/analysis-of-algorithms/#best-case-analysisbogus","text":"In the best case analysis, we calculate lower bound on running time of an algorithm. We must know the case that causes minimum number of operations to be executed. Most of the times, we do worst case analysis to analyze algorithms. In the worse analysis, we guarantee an upper bound on the running time of an algorithm which is good information. The average case analysis is not easy to do in most of the practical cases and it is rarely done. In the average case analysis, we must know (or predict) the mathematical distribution of all possible inputs. The Best Case analysis is bogus. Guaranteeing a lower bound on an algorithm doesn't provide any information as in the worse case, an algorithm may take years to run.","title":"Best Case Analysis(Bogus)"},{"location":"knowledge/algorithms/analysis-of-algorithms/#asymptotic-notations","text":"The main idea of asymptotic analysis is to have a measure of afficiency of algotithms that doesn't depend on machine specific constants, and doesn't require algorithms to be implemented and time taken by progrms to be compared. Asymptotic notations are mathematical tools to represent time complexity of algorihms for asymptotic analysis. 3 asymptotic notations are mostly used to represent time complexity of algorithms. \u0398 Notation : The theta notation bounds a functions from above and below so it defines exact asymprotic behavior. A simple way to get Theta notation of an expression is to drop low order terms and ignore leading constants. For a given function g(n), we denote \u0398(g(n)) is following set of functions. \u0398(g(n)) = {f(n): there exist positive constants c1, c2 and n0 such that 0 <= c1*g(n) <= f(n) <= c2*g(n) for all n >= n0} Big O Notation : The Big O notation defines an upper bound of an algorithm, it bounds a function only from above. The Bid O notation is useful when we only have upper bound on time complexity of an algorithm. Many times we easily find an upper bound by simply looking at the algorithm. O(g(n)) = { f(n): there exist positive constants c and n0 such that 0 <= f(n) <= c*g(n) for all n >= n0} \u03a9 Notation : Just as Big O notation provides an asymptotic upper bound on a function, \u03a9 notation provides an asymptotic lower bound. \u03a9 Notation can be useful when we have lower bound on time complexity of an algorithm. For a given function g(n), we denote by \u03a9(g(n)) the set of functions. \u03a9 (g(n)) = {f(n): there exist positive constants c and n0 such that 0 <= c*g(n) <= f(n) for all n >= n0}. Important Links : There are two more notations called little o and little omega . Analysis of Algorithm| Set 4 (Analysis of Loops) Recent Aritcles on analysis of algorithm .","title":"Asymptotic Notations"},{"location":"knowledge/algorithms/analysis-of-algorithms/#little-o-and-little-omega-notations","text":"The main idea of asymptotic analysis is to have a measure of efficiency of algorithms that doesn't depend on machine specific constants, mainly because this analysis doesn't require algorithms to be implemeneted nad time taken by programs to be compared.","title":"Little o and little omega notations"},{"location":"knowledge/algorithms/analysis-of-algorithms/#little-o-asymptotic-notation","text":"Definition : Let f(n) and g(n) be functions that map positive integers to positive real numbers. we say that f(n) is o(g(n)) (of f(n) E o(g(n))) if for any real constant c>0, there exists an integer constant n0>= 1 such that 0 <= f(n) < c*g(n). In mathematical relation, f(n) = o(g(n)) means lim f(n)/g(n) = 0","title":"Little o asymptotic notation"},{"location":"knowledge/algorithms/analysis-of-algorithms/#little-omega-asymptotic-notation","text":"Definition : Let f(n) and g(n) ne functions that map positive integers to positive real numbers. We say that f(n) is \u03c9(g(n)) (or f(n) \u2208 \u03c9(g(n))) if for any real constant c > 0, there exists an integer constant n0 >= 1 such that f(n) > c*g(n) >= 0 for every integer n >= n0 f(n) has a higher growth rate than g(n) so main difference between Big Omega(\u03a9) and little omega (\u03c9) lies in their definitions. In the case of Big Omega f(n)=\u03a9(g(n)) and the bound is 0<=cg(n)<=f(n), but in case of little omega, it is true for 0 <= c*g(n) < f(n). We use \u03c9 notation to denote a lower bound that is not asymptotically tight. and, f(n) \u2208 \u03c9(g(n)) if and only if g(n) \u2208 \u03bf((f(n)) In mathematical relation, if f(n) \u2208 \u03c9(g(n)) then lim f(n)/g(n) = \u221e","title":"Little omega asymptotic notation"},{"location":"knowledge/algorithms/analysis-of-algorithms/#lower-and-upper-bound-theory","text":"The Lower and Upper Bound Theory provides a way to find the lowest complexity algorithm to solve a problem. What actually Lower and Upper bounds are. Lower Bound - Let L(n) be the running time of an algorithm A(n), then g(n) is the Lower Bound of A if there exist two constants C and N such that L(n) <= C*g(n) for n > N. Lower bound of an algorithm is shown by the asymtotic notation called Big Omega Upper Bound - Let U(n) be the running time of an algorithm A, then g(n) is the Upper Bound of A if there exist two constants C and N such that U(n) >= C*g(n) for n > N. Upper bound of an algorithm is shown by the asymtotic notation called Big Oh(O)","title":"Lower and Upper Bound Theory"},{"location":"knowledge/algorithms/analysis-of-algorithms/#1-lower-bound-theory","text":"According to the lower bound theory, for a lower bound L(n) of an algorithm, it is not possible to have any other algorithm (for a common problem) whose time complexity is less than L(n) for random input. Also every algorithm must take at least L(n) time in worst case. Note that L(n) here is the minimum of all the possible algorithm, of maximum complexity. Note that our main motive is to get an optimal algorithm, which is the one having its Upper Bound Same as its Lower Bound (U(n)=L(n)). Trivial Lower Bound - It is the easiest method to find the lower bound. The Lower bounds which can be easily observed on the basis of the number of input taken and the number of output produces are called trivial Lower Bound. Multiplication of nxn matrix, where, Input: For 2 matrix we will have 2n2 inputs Output: 1 matrix of order n x n, i.e., n2 outputs Computational Model - The method is for all those alforithms that are comparison based. For example in sorting we have to compare the elements of the list among themselces and then sort them accordingly. Similar is the case with searching and thus we can implement the same in this case.","title":"1. Lower Bound Theory"},{"location":"knowledge/algorithms/analysis-of-algorithms/#analysis-of-loops","text":"O(1) : Time complexity of a function (or set of statements) is considered as O(1) if it doesn't contain loop, recursion and call to any other non-constant time function. // Here c is a constant for (int i = 1; i <= c; i++) { // some O(1) expressions } O(n) : Time Complexity of a loop is considered as O(n) if the loop variables is incremented/decremented by a constant amount. // Here c is a positive integer constant for (int i = 1; i <= n; i += c) { // some O(1) expressions } for (int i = n; i > 0; i -= c) { // some O(1) expressions } O(nc) : Time complexity of nested loops is equal to the number of times the innermost statement is executed. for (int i = 1; i <=n; i += c) { for (int j = 1; j <=n; j += c) { // some O(1) expressions } } for (int i = n; i > 0; i -= c) { for (int j = i+1; j <=n; j += c) { // some O(1) expressions } O(Logn) : Time Complexity of a loop is considered as O(Logn) if the loop variables is devidedd / multiplied by a constant amount. for (int i = 1; i <=n; i *= c) { // some O(1) expressions } for (int i = n; i > 0; i /= c) { // some O(1) expressions } O(LogLogn) : Time complexity of a loop is consider as O(LogLogn) f the loop variables is reduced / increased exponentially by a constant amount. // Here c is a constant greater than 1 for (int i = 2; i <=n; i = pow(i, c)) { // some O(1) expressions } //Here fun is sqrt or cuberoot or any other constant root for (int i = n; i > 1; i = fun(i)) { // some O(1) expressions } How to combine time complexities of consecutive loops? When there are consecutive loops, we calclate time complexity as sum f time complexities of individual loops for (int i = 1; i <=m; i += c) { // some O(1) expressions } for (int i = 1; i <=n; i += c) { // some O(1) expressions } Time complexity of above code is O(m) + O(n) which is O(m+n) If m == n, the time complexity becomes O(2n) which is O(n).","title":"Analysis of Loops"},{"location":"knowledge/algorithms/analysis-of-algorithms/#solving-recurrences","text":"Substitution Method : We make a guess for the solution and then we use mathematical induction to prove the guess is correct or incorrect. For example consider the recurrence T(n) = 2T(n/2) + n We guess the solution as T(n) = O(nLogn). Now we use induction to prove our guess. We need to prove that T(n) <= cnLogn. We can assume that it is true for values smaller than n. T(n) = 2T(n/2) + n <= cn/2Log(n/2) + n = cnLogn - cnLog2 + n = cnLogn - cn + n <= cnLogn Recurrence Tree Method : In this method, we draw a recurrence tree and calculate the time taken by everu level of tree. Finnaly, we sum the work done at all levels. To draw the recurrence tree, we start from the given recurrence and keep drawing till we find pattern among levels. The pattern is typically a arithmetic or geometric series. For example consider the recurrence relation T(n) = T(n/4) + T(n/2) + cn2 cn2 / \\ T(n/4) T(n/2) If we further break down the expression T(n/4) and T(n/2), we get following recursion tree. cn2 / \\ c(n2)/16 c(n2)/4 / \\ / \\ T(n/16) T(n/8) T(n/8) T(n/4) Breaking down further gives us following cn2 / \\ c(n2)/16 c(n2)/4 / \\ / \\ c(n2)/256 c(n2)/64 c(n2)/64 c(n2)/16 / \\ / \\ / \\ / \\ To know the value of T(n), we need to calculate sum of tree nodes level by level. If we sum the above tree level by level, we get the following series T(n) = c(n^2 + 5(n^2)/16 + 25(n^2)/256) + .... The above series is geometrical progression with ratio 5/16. To get an upper bound, we can sum the infinite series. We get the sum as (n2)/(1 - 5/16) which is O(n2) Master Method : Master Method is a direct way to get the solution. The maser method works only for following type of recurrences or for recurrences that can be transformed to follwoing type. T(n) = aT(n/b) + f(n) where a >= 1 and b > 1 There are following three cases: If f(n) = \u0398(nc) where c < c < Logba then T(b) = \u0398(nLogba) If f(n) = \u0398(nc) where c = Logba then T(n) = \u0398(ncLog n) If f(n) = \u0398(nc) where c > Logba then T(n) = \u0398(f(n)) How does this work? Master methd is mainly derived from recurrence tree method. If we draw recurrence tree of T(n) = aT(n/b) + f(n), we can see that the work done at root is f(n) and work done at all leaves is \u0398(nc) where c is Logba. And the height of recurrence tree is Logbn. In recurrence tree method, we calculate total work done. If the work done at leaves is polynomially, then leaves are the dominant part, and our result becomes the work done leaves (Case 1). If work done at leaves and root is asymptotically same, then our result becomes height multiplied by work done at any level (Case 2). If work done at root is asymptotically more, then our result becomes work done at root(Case 3).","title":"Solving Recurrences"},{"location":"knowledge/algorithms/analysis-of-algorithms/#amortized-analysis-introduction","text":"Amortized Analysis is used for algorithms where an occasional operation is very slow, but most of the other operations are faster. In Amortized Analysis, we analyze a sequence of oprations and guarantee a worst case average time which is lower than the worst case time of a particular expensive operation. The solution to this trade off problem is to use Dynamic Table (or Arrays). The idea is to increase size of table whenever it becomes full. Following are the steps to follow when table becomes full. Allocate memory for a larger table of size. Copy the contents of old table to new value. Free the old table. If the table has space available, we simply insert new item in available space. What is the time complexity of n insertions using the above scheme? If use simple analysis, the worst case cost of an insertion is O(n). Therefore, worst case cost of n inserts is n * O(n2). This analysis gives an upper bound, nit not a tight upper bound for n insertions as all insertions don't take \u0398(n) time. Folowing are few important notes. Amoritized cost of a sequence of operations can be seen as expenses of a salaried person. The average monthly expense of the person is less than or equal to the salary, but the person can spend more money in a particular month by buying a car or something. In other months, he or she saves money for the expensive month. The above Amoritized Analysis done for Dynamic Array example is called Aggregate Method . We will be discussing the other two methods in separate posts. The amoritized analysis doesn't involve probability. There is also another different notion of average case running time where algorithms use randomization to make them faster and expected running time is faster than worst case running time. These algorithms are analyzed using Randomized Analysis. Sources: : Berkeley Lecture 35: Amoritized Analysis . MIT Lecture 13: Amoritized Algorithms, Table Doubling, Potential Method amoritzed","title":"Amortized Analysis Introduction"},{"location":"knowledge/algorithms/analysis-of-algorithms/#what-does-space-complexity-mean","text":"","title":"What does 'Space Complexity' mean?"},{"location":"knowledge/algorithms/analysis-of-algorithms/#space-complexity","text":"The term Space Complexity is misused for Auxiliary Space at many places. Following are the correct definitions of Auxiliary Space and Space Complexity. Auxiliary Space is the extra space or temporary space used by an alogorithm. Space Complexity of an algorithm is total space taken by the algorithm with respect to the input size. Space complexity includes both Auxiliary space and space used by input. For example, if we want to compare standard sorting algorithms on the basis of space, then Auxiliary Space would be a better criteria than Space Complexity. Merge Sort uses O(n) auxiliary space, Insertion sort and Heap Sort use O(1) auxiliary space. Space complexity of all these sorting algorithms is O(n) though.","title":"Space Complexity:"},{"location":"knowledge/algorithms/analysis-of-algorithms/#pseudo-polynomial-algorithms","text":"","title":"Pseudo-polynomial Algorithms"},{"location":"knowledge/algorithms/analysis-of-algorithms/#what-is-pseudo-polynomial","text":"An algorithm whose worst case time complexity depends on numeric value of input (not number of inputs) is called Pseudo-polynomial algorithm. On the other hand, an algorithm whose time complexity is only based on number of elements in array (not value) is considered as polynomial time algorithm.","title":"What is Pseudo-polynomial?"},{"location":"knowledge/algorithms/analysis-of-algorithms/#pseudo-polynomial-and-np-completeness","text":"Some NP-Complete problems have Pseudo Polynomial time solutions. For example, Dynamic Programming Solutions of 0-1 Knapsack , Subset-Sum and Partition problems are Pseudo-Polynomial. NP complete problems that can be solved using a pseudo-polynomial time algorithms are called weakly NP-complete.","title":"Pseudo-polynomial and NP-Completeness"},{"location":"knowledge/algorithms/analysis-of-algorithms/#reference","text":"Pseudo-polynomial_time wiki","title":"Reference:"},{"location":"knowledge/algorithms/analysis-of-algorithms/#polynomial-time-approximation-scheme","text":"It is a very well know fact that there is no known polynomial time solution for NP Complete problems and these problems occur a lot in real world. So there must be a way to handle them. We have seen algotithms to these problems which are approximate. P olynomial T ime A pproximation S cheme(PTAS) is a type of approximate algorithms that provide user to control over accuracy which is a desirable feature. These algorithms take an additional parameter e > 0 and provide a solution that is (1 + e) approximate for minimization and (1 - e) for maximization. In PTAS algorithms, the exponent of the polynomial can increase dramatically as e reduces. To be continued. Get more information at here .","title":"Polynomial Time Approximation Scheme"},{"location":"knowledge/architectures/three-tier/","text":"Three Tier Architecture Three-tier architecture allows any one of the three tiers to be upgraded or replaced independently. The user interface is implemented on a desktop PC and uses a standard grphical user interface with different modules running on the application server. The relational database management system on the database server contains the computer data storge logic. The middle tiers are usuall multitiered. The three tiers in a three-tier architecture are: Presentation Tier: Occupies the top level and displays information related to services available on a website. This tier communicates with other tiers b sending results to the browser and other tiers in the network. Application Tier: Also called the middle tier, logic tier, business logic or logic tier, this tier is pulled from the presentation tier. It controls application functionality by performing detailed processing. Data Tier: Houses database servers where information is stored and retrieved. Data in this tier is kept independent of application servers or business logic. Get more information at here .","title":"Three tier"},{"location":"knowledge/architectures/three-tier/#three-tier-architecture","text":"Three-tier architecture allows any one of the three tiers to be upgraded or replaced independently. The user interface is implemented on a desktop PC and uses a standard grphical user interface with different modules running on the application server. The relational database management system on the database server contains the computer data storge logic. The middle tiers are usuall multitiered. The three tiers in a three-tier architecture are: Presentation Tier: Occupies the top level and displays information related to services available on a website. This tier communicates with other tiers b sending results to the browser and other tiers in the network. Application Tier: Also called the middle tier, logic tier, business logic or logic tier, this tier is pulled from the presentation tier. It controls application functionality by performing detailed processing. Data Tier: Houses database servers where information is stored and retrieved. Data in this tier is kept independent of application servers or business logic. Get more information at here .","title":"Three Tier Architecture"},{"location":"knowledge/dev-ops/circle-ci/writing-yaml/","text":"Writing YAML This document describes the most important features of YAML for use in CircleCI configuration. How to Write YAML See Also Overview YAML is a human-friendly data serialization standard for all programming languages. It is a strict superset of JSON , another data seralizationn languages. This means it can do everything JSON can.. and more. Circle configuration is stored in a single YAML file located at ~/.circleci/config.yml , where ~ is the root of your project;s directory. Since most of your work with CircleCI occures in this file, it is important to understand the basixs of YAML formatting. How to Write YAML the basic structure of YAML file is a hash map and consists of one or more key-value pairs. key: value you can set another key-value pair as a value by indenting the nested key. key: anothe_key: \"another value\" Multi-line Strings if the value a multi-line string, use the > character, followed by any number of lines. this is especially useful for lengthy commends. haiku: > Consider me As one who loved poetry And persimmons. Note : Quotes are not necessary when using multiline strings. Sequences Keys and values are not restricted to scalars . You may also map a scalar to a sequence. scalar: - never - gonna - give - you - up Items in sequences can also be key-value pairs. simulation: - within: \"a simulation\" - without: a_glitch: \"in the matrix\" Note : Remember to properly indent a key-value when it is the value of an item in a sequence. Achors and Aliases To DRY up your config.yml , use anchors and aliases. Anchors are indentified by an & character, and aliases by an * character. song: - &name Al - You - can - call - me - *name when the a bove list is read by a YAML parser, the literal oytput looks like this. song: - Al - You - can - call - me - Al Mergin Maps Anchors and aliases work for scalar values, but to save maps or sequences, use << to inject the alias. default: &default school: hogwarts harry: <<: *default house: gryffindor draco: <<: *default house: slytherin You can also merge multi maps. name: &harray_name first_name: Harry last_name: Potter address: &harry_address street: 4, Privet Drive district: Little Whinging county: Surrey country: England harry_data: <<: [*harry_name, *harry_address] Note : As mentioned in a YAML repository issue , it is possible to merge maps, but not sequences (also called arrays or lists). for more complex example. see this gist . See Also while YAML has several other feature, the examplws above should be enough to get you started with YAML and keep your CircleCI configuration concise. if you are hungry for moree knowledge, here a few ideas. For a concrete example of keys and values, see the Configuring CircleCI document. If you are unsure whether your config.yml is value YAML, run it through a validator . CircleCI has also developed \"orbs,\" which enable you to use pre-configured and tested packages of configuration elements that you can use in our cnfiguration workflow. Utilizing DRY (Don't Repeat Yourself), orbs enable you to quickly and easily incorporate configuration elements (jobs, executors, commands) in your workflow. For more detailed information about orb: Refer to Orb introduction , for a high-level overview of orbs. Refer to Using Orbs , for more about how to use existing orbs. Refer to Creating Orbs , where you will find step-by-step instructions on how to create your own orb. Refer to Reusing Config for more detailsed examples of reusable orbs, commands, parameters, and executors. For a more exhaustive overvire of YAML, LEarn X in Y Minutes has a great summary.","title":"Writing YAML"},{"location":"knowledge/dev-ops/circle-ci/writing-yaml/#writing-yaml","text":"This document describes the most important features of YAML for use in CircleCI configuration. How to Write YAML See Also","title":"Writing YAML"},{"location":"knowledge/dev-ops/circle-ci/writing-yaml/#overview","text":"YAML is a human-friendly data serialization standard for all programming languages. It is a strict superset of JSON , another data seralizationn languages. This means it can do everything JSON can.. and more. Circle configuration is stored in a single YAML file located at ~/.circleci/config.yml , where ~ is the root of your project;s directory. Since most of your work with CircleCI occures in this file, it is important to understand the basixs of YAML formatting.","title":"Overview"},{"location":"knowledge/dev-ops/circle-ci/writing-yaml/#how-to-write-yaml","text":"the basic structure of YAML file is a hash map and consists of one or more key-value pairs. key: value you can set another key-value pair as a value by indenting the nested key. key: anothe_key: \"another value\"","title":"How to Write YAML"},{"location":"knowledge/dev-ops/circle-ci/writing-yaml/#multi-line-strings","text":"if the value a multi-line string, use the > character, followed by any number of lines. this is especially useful for lengthy commends. haiku: > Consider me As one who loved poetry And persimmons. Note : Quotes are not necessary when using multiline strings.","title":"Multi-line Strings"},{"location":"knowledge/dev-ops/circle-ci/writing-yaml/#sequences","text":"Keys and values are not restricted to scalars . You may also map a scalar to a sequence. scalar: - never - gonna - give - you - up Items in sequences can also be key-value pairs. simulation: - within: \"a simulation\" - without: a_glitch: \"in the matrix\" Note : Remember to properly indent a key-value when it is the value of an item in a sequence.","title":"Sequences"},{"location":"knowledge/dev-ops/circle-ci/writing-yaml/#achors-and-aliases","text":"To DRY up your config.yml , use anchors and aliases. Anchors are indentified by an & character, and aliases by an * character. song: - &name Al - You - can - call - me - *name when the a bove list is read by a YAML parser, the literal oytput looks like this. song: - Al - You - can - call - me - Al","title":"Achors and Aliases"},{"location":"knowledge/dev-ops/circle-ci/writing-yaml/#mergin-maps","text":"Anchors and aliases work for scalar values, but to save maps or sequences, use << to inject the alias. default: &default school: hogwarts harry: <<: *default house: gryffindor draco: <<: *default house: slytherin You can also merge multi maps. name: &harray_name first_name: Harry last_name: Potter address: &harry_address street: 4, Privet Drive district: Little Whinging county: Surrey country: England harry_data: <<: [*harry_name, *harry_address] Note : As mentioned in a YAML repository issue , it is possible to merge maps, but not sequences (also called arrays or lists). for more complex example. see this gist .","title":"Mergin Maps"},{"location":"knowledge/dev-ops/circle-ci/writing-yaml/#see-also","text":"while YAML has several other feature, the examplws above should be enough to get you started with YAML and keep your CircleCI configuration concise. if you are hungry for moree knowledge, here a few ideas. For a concrete example of keys and values, see the Configuring CircleCI document. If you are unsure whether your config.yml is value YAML, run it through a validator . CircleCI has also developed \"orbs,\" which enable you to use pre-configured and tested packages of configuration elements that you can use in our cnfiguration workflow. Utilizing DRY (Don't Repeat Yourself), orbs enable you to quickly and easily incorporate configuration elements (jobs, executors, commands) in your workflow. For more detailed information about orb: Refer to Orb introduction , for a high-level overview of orbs. Refer to Using Orbs , for more about how to use existing orbs. Refer to Creating Orbs , where you will find step-by-step instructions on how to create your own orb. Refer to Reusing Config for more detailsed examples of reusable orbs, commands, parameters, and executors. For a more exhaustive overvire of YAML, LEarn X in Y Minutes has a great summary.","title":"See Also"},{"location":"knowledge/dev-ops/docker/docker-network/","text":"Docker Network Overview One of the reasons Docker containers and services are so powerful is that you can connect them together, or connect them to non-Docker workloads. Docker containers and services do not even need to be aware that they are developed on Docker, or whether their peers are also Docker woekloads or not. Whether your Docker hosts run Linux, Windows, or a mix of the two, you can use Docker to run manage them in a platform-agnostic way. This topic defines some basic Docker networking concepts and prepares you to design and deploy your applications to take full advantage of these capabilities. Most of this content applies to all Docker installations. However, a few advanced features are only available to Docker EE customers. Network drivers Docker's networking subsystem is pluggable, using drivers. Several drivers exist by default, and provide core networking functionality: bridge : The default network driver. If you don't specify a driver, this is the type of network you are creating. Bridge network are usually used when your applications run in standalone containers that need to communicate . See bridge networks . host : For standalone containers, remove network isolation between the container and the Docker host and use the host's network directly. host is only available for swarm services on Docker 17.06 and higher. See use the host network . overlay : Overlay network connect multiple Docker daemons together and enable swarm services to communicate with each other. You can also use overlay networks to facilitate communication between a swarm service and a standalone container, or between two standalone containers on different Docker daemons. this strategy removes the need to do OS-level routing between these containers. See overlay networks . macvlan : Macvlan networks allow you to assign a MAC address to a container, making it appear as a physical device on your network. The Docker daemon routes trffic to containers by their MAC addresses. Using the macvlan driver is sometimes the best choice when dealing with legacy applications that expect to be directly connected to the physical network, rather than routed through the Docker host's network stack. See Macvlan networks . none : For this container, disable all networking. Usually used in conjunction with a custom network driver. none is not available for swarm services. See disable container networking . Network plugins : You can install and use third-party network plugings with Docker. These plugins are available from Docker Hub or from third-party vendors. See the vendor's documentation for installing and using a given network plugin. Network driver summary User-defined brigde networks are best when you need multiple containers to communicate on the same Docker host. Host networks are best when the network stack should not be isolated from the Docker host, but you want other aspects of the container to be isolated. Overlay networks are best when you need containers running on different Docker hosts to communicate, or whrn multiple applications work together using swarm services. Macvlan networks are best when you are migrating from a VM setup or need your containers to look like physical hosts on your network, each with a unique MAC address. Third-party network plugins allow you to integrate Docker with specialized network stacks. Docker EE networking features The following two features are only possible when using Docker EE and managing your Docker services using Univeral Control Plane(UCP): The HTTP rouing mesh allows you to share the same network IP address and port among multiple services. UCP routes the traffic to the appropriate service using the combination of hostname and port, as requested from the client. Session stickness allows you to specify information in the HTTP header which UCP uses to route subsequent requests to the same service task, for applications which require stateful sessions. Networking tutorials Now that you understand the basics about Docker network, deepen your understanding using the following tutorials: Standalone networking tutorial Host networking tutorial Overlay networking tutorial Macvlan networking tutorial Get more information at here .","title":"Docker Network"},{"location":"knowledge/dev-ops/docker/docker-network/#docker-network","text":"","title":"Docker Network"},{"location":"knowledge/dev-ops/docker/docker-network/#overview","text":"One of the reasons Docker containers and services are so powerful is that you can connect them together, or connect them to non-Docker workloads. Docker containers and services do not even need to be aware that they are developed on Docker, or whether their peers are also Docker woekloads or not. Whether your Docker hosts run Linux, Windows, or a mix of the two, you can use Docker to run manage them in a platform-agnostic way. This topic defines some basic Docker networking concepts and prepares you to design and deploy your applications to take full advantage of these capabilities. Most of this content applies to all Docker installations. However, a few advanced features are only available to Docker EE customers.","title":"Overview"},{"location":"knowledge/dev-ops/docker/docker-network/#network-drivers","text":"Docker's networking subsystem is pluggable, using drivers. Several drivers exist by default, and provide core networking functionality: bridge : The default network driver. If you don't specify a driver, this is the type of network you are creating. Bridge network are usually used when your applications run in standalone containers that need to communicate . See bridge networks . host : For standalone containers, remove network isolation between the container and the Docker host and use the host's network directly. host is only available for swarm services on Docker 17.06 and higher. See use the host network . overlay : Overlay network connect multiple Docker daemons together and enable swarm services to communicate with each other. You can also use overlay networks to facilitate communication between a swarm service and a standalone container, or between two standalone containers on different Docker daemons. this strategy removes the need to do OS-level routing between these containers. See overlay networks . macvlan : Macvlan networks allow you to assign a MAC address to a container, making it appear as a physical device on your network. The Docker daemon routes trffic to containers by their MAC addresses. Using the macvlan driver is sometimes the best choice when dealing with legacy applications that expect to be directly connected to the physical network, rather than routed through the Docker host's network stack. See Macvlan networks . none : For this container, disable all networking. Usually used in conjunction with a custom network driver. none is not available for swarm services. See disable container networking . Network plugins : You can install and use third-party network plugings with Docker. These plugins are available from Docker Hub or from third-party vendors. See the vendor's documentation for installing and using a given network plugin.","title":"Network drivers"},{"location":"knowledge/dev-ops/docker/docker-network/#network-driver-summary","text":"User-defined brigde networks are best when you need multiple containers to communicate on the same Docker host. Host networks are best when the network stack should not be isolated from the Docker host, but you want other aspects of the container to be isolated. Overlay networks are best when you need containers running on different Docker hosts to communicate, or whrn multiple applications work together using swarm services. Macvlan networks are best when you are migrating from a VM setup or need your containers to look like physical hosts on your network, each with a unique MAC address. Third-party network plugins allow you to integrate Docker with specialized network stacks.","title":"Network driver summary"},{"location":"knowledge/dev-ops/docker/docker-network/#docker-ee-networking-features","text":"The following two features are only possible when using Docker EE and managing your Docker services using Univeral Control Plane(UCP): The HTTP rouing mesh allows you to share the same network IP address and port among multiple services. UCP routes the traffic to the appropriate service using the combination of hostname and port, as requested from the client. Session stickness allows you to specify information in the HTTP header which UCP uses to route subsequent requests to the same service task, for applications which require stateful sessions.","title":"Docker EE networking features"},{"location":"knowledge/dev-ops/docker/docker-network/#networking-tutorials","text":"Now that you understand the basics about Docker network, deepen your understanding using the following tutorials: Standalone networking tutorial Host networking tutorial Overlay networking tutorial Macvlan networking tutorial Get more information at here .","title":"Networking tutorials"},{"location":"knowledge/mobile-development/overview/","text":"Overview Pathways to Mobile Development Native Approach The native approach is specifically adopted while building apps for a specific platform. Native apps have access to all the platform specific features since it's targeted specifically towards it. Performance is at it's best while using natice. For eg. Java/Kotlin for Native Android. Objective-C/Swift for Natice iOS. Web Approach Cordova These apps are built using web technologies like HTML, CSS, Javascript, etc. and uses webview to render the UI on mobile. It's very easy to build simple apps using Cordova but can have a hard time with big and complex apps. Plugins support is wide for accessing platform-specific features. These technologies have become legacy now and the world is moving towards adoption of cross-platform approach and PWA's. PWA Progressive Web Aps is one of the trending topics for now and utilize service workers to provide offline access to apps once it's cached. You can beautify your apps to peovide native feel. it can also access most of platform & device specific features and makes the user feel like using the natice app. One can use popular frameworks like Angular, React, etc to build these apps and enable PWA functionality. These are more of web app which provides a look and feel of native apps on mobile. You can refer the below link to know what's supported on PWA's as of now: here Cross-Platform Approach A cross-platform approach is specifically adopted to build apps that can be targeted for multiple platforms. It has access to mos of the platform-specific features but not all. Performance is somewhat close to native. Xamarin Xamarin platform is a one-top for the development of mobile and desktop apps from Microsoft. The developer can use C# or F# for development. It fives the feel of native as per platform specific UI Layouts. Its mostly adopted by people who are more familiar with Microsoft dotNet technology to save learning efforts and get going for mobile development. React Native React Native is from Facebook and is widely used in the development of cross-platform mobile apps. It mostly adopted by people familiar with Web Frameworks like React & Angular to save learning efforts foe mobile development. Though it's still in Bete people have already started using it in production. Flutter Flutter platform is recently the latest one from Google and can be considered as a tough contender in the cross-platform world. It uses Dart language for development and widely gaining popularity in recent times. There is also news for the launch of a new mobile OS called Fuchsia and Flutter is supposed to support its development once the OS is launched. Once Fuchsia is launched flutter will probably be the topmost prefered platform for mobile development. What should I go forward with? With the availability of so many options, one will really have a hard time to conclude on a platform. While concluding on the platform one needs to consider the below points: Platform Support : This the first and the most important questionn to ask while evaluating apps requirement. You should probably be asking what will be the supported platform for the application. Will it have the support of only Android, iOS or both or it should also support Web. If it only Android or iOS you should more inclined toward native approaches rather than considering Web or Cross Platform. Application Functional Specs : Its the most important of all to consider what all functionalities the app needs. For example, PWA's doesn't support all device functionalities and if your app demands one you cannot go forward with the PWA's. Native approaches have access to all new platform-specific features and with cross-platfrom or web approaches it takes time to get the support for it. Goto Market Time: Goto market time is bisically the time you need to ship the app to production. If app should have support for multiple platform (Android, iOS, Web) and the timeline is stringent you should consider Cross Platform approaches rather than Native on. Natice apps will need native developers for each platform which adds up time considering you have limited developers. Budge : The budget also helps in deciding the platform. Less budget and multiple platform support will push you to choose a web/cross-platform approach since a single developer can develop apps for multiple targets (Android, iOS). Current Expertise : You should also be considering the current expertise of your team. Every new platform has its own learning curve and it won't be a good choice to change technology every time for a new app. Developers will be more comfortable with platforms they have experience in rather than adopting a new one. If your team is more of the web then you should be inclined towards web-based technologies like React or Angular. If your team is of dotnet developers probably Xamarin will be the way suitable for you. Get more information at here .","title":"Overview"},{"location":"knowledge/mobile-development/overview/#overview-pathways-to-mobile-development","text":"","title":"Overview Pathways to Mobile Development"},{"location":"knowledge/mobile-development/overview/#native-approach","text":"The native approach is specifically adopted while building apps for a specific platform. Native apps have access to all the platform specific features since it's targeted specifically towards it. Performance is at it's best while using natice. For eg. Java/Kotlin for Native Android. Objective-C/Swift for Natice iOS.","title":"Native Approach"},{"location":"knowledge/mobile-development/overview/#web-approach","text":"","title":"Web Approach"},{"location":"knowledge/mobile-development/overview/#cordova","text":"These apps are built using web technologies like HTML, CSS, Javascript, etc. and uses webview to render the UI on mobile. It's very easy to build simple apps using Cordova but can have a hard time with big and complex apps. Plugins support is wide for accessing platform-specific features. These technologies have become legacy now and the world is moving towards adoption of cross-platform approach and PWA's.","title":"Cordova"},{"location":"knowledge/mobile-development/overview/#pwa","text":"Progressive Web Aps is one of the trending topics for now and utilize service workers to provide offline access to apps once it's cached. You can beautify your apps to peovide native feel. it can also access most of platform & device specific features and makes the user feel like using the natice app. One can use popular frameworks like Angular, React, etc to build these apps and enable PWA functionality. These are more of web app which provides a look and feel of native apps on mobile. You can refer the below link to know what's supported on PWA's as of now: here","title":"PWA"},{"location":"knowledge/mobile-development/overview/#cross-platform-approach","text":"A cross-platform approach is specifically adopted to build apps that can be targeted for multiple platforms. It has access to mos of the platform-specific features but not all. Performance is somewhat close to native.","title":"Cross-Platform Approach"},{"location":"knowledge/mobile-development/overview/#xamarin","text":"Xamarin platform is a one-top for the development of mobile and desktop apps from Microsoft. The developer can use C# or F# for development. It fives the feel of native as per platform specific UI Layouts. Its mostly adopted by people who are more familiar with Microsoft dotNet technology to save learning efforts and get going for mobile development.","title":"Xamarin"},{"location":"knowledge/mobile-development/overview/#react-native","text":"React Native is from Facebook and is widely used in the development of cross-platform mobile apps. It mostly adopted by people familiar with Web Frameworks like React & Angular to save learning efforts foe mobile development. Though it's still in Bete people have already started using it in production.","title":"React Native"},{"location":"knowledge/mobile-development/overview/#flutter","text":"Flutter platform is recently the latest one from Google and can be considered as a tough contender in the cross-platform world. It uses Dart language for development and widely gaining popularity in recent times. There is also news for the launch of a new mobile OS called Fuchsia and Flutter is supposed to support its development once the OS is launched. Once Fuchsia is launched flutter will probably be the topmost prefered platform for mobile development.","title":"Flutter"},{"location":"knowledge/mobile-development/overview/#what-should-i-go-forward-with","text":"With the availability of so many options, one will really have a hard time to conclude on a platform. While concluding on the platform one needs to consider the below points: Platform Support : This the first and the most important questionn to ask while evaluating apps requirement. You should probably be asking what will be the supported platform for the application. Will it have the support of only Android, iOS or both or it should also support Web. If it only Android or iOS you should more inclined toward native approaches rather than considering Web or Cross Platform. Application Functional Specs : Its the most important of all to consider what all functionalities the app needs. For example, PWA's doesn't support all device functionalities and if your app demands one you cannot go forward with the PWA's. Native approaches have access to all new platform-specific features and with cross-platfrom or web approaches it takes time to get the support for it. Goto Market Time: Goto market time is bisically the time you need to ship the app to production. If app should have support for multiple platform (Android, iOS, Web) and the timeline is stringent you should consider Cross Platform approaches rather than Native on. Natice apps will need native developers for each platform which adds up time considering you have limited developers. Budge : The budget also helps in deciding the platform. Less budget and multiple platform support will push you to choose a web/cross-platform approach since a single developer can develop apps for multiple targets (Android, iOS). Current Expertise : You should also be considering the current expertise of your team. Every new platform has its own learning curve and it won't be a good choice to change technology every time for a new app. Developers will be more comfortable with platforms they have experience in rather than adopting a new one. If your team is more of the web then you should be inclined towards web-based technologies like React or Angular. If your team is of dotnet developers probably Xamarin will be the way suitable for you. Get more information at here .","title":"What should I go forward with?"},{"location":"knowledge/mobile-development/xamarin/controls-reference/","text":"Controls Reference A Description of all the visual elements used to construct a Xamarin.Forms application . The visual interface of a Xamarin.Forms application is constructed of objects that map to the native controls of each target platform. This allows platform-specific applications for iOS, Android, and the Universal Windows Platform to use Xamarin.Forms code contained in a .NET Standard library or a Shared Project . The four main control groups used to create the user interface of a Xamarin.Forms application are shown in these four articles: Pages Layouts Views Cells A Xamarin.Forms page generally occupies the entire screen. The page usually contains a layout, which contains views and possibly other layouts. Cells are specialied components used in connection with TableView and ListView . In the four articles on Pages , Layouts , Views and Cells , each type of control is described with links to its API documentation, an article describing its use (if one exists), and one or more sample programs(if they exist). Each type of control is also accompanied by a screenshot showing a page from the FormsGallery sample running on iOS, Android, and UWP devices. Below each screenshot are links to the source code fot the C# page, the quivalent Xaml page, and (when appropriate) the C# code-behind file for the Xaml page. A cell is a specialized element used for items in a table and describes how each item in a list should be rendered. The Cell class derives from Element , from which VisualElement also derives. A cell is not itself a visual element; it is instead a template for creating a visual element. Cells Xamarin.Forms supports the following cell types: TextCell A TextCell displays one or two text strings. Set the Text property and, optionally, the Detail property to these text strings. ImageCell The ImageCell displays the same information as TextCell but includes a bitmap that you set with the Source property. SwitchCell The SwitchCell contains text set with the Text ' property and on/off switch initiallly set with the Boolean On property. Handle the Onchanged event to be notified when the On property changes. EntryCell The EntryCell defines a Label property that identifies the cell and a single line of editable text in the Text property. Handle the Completed event to be notified when the user has completed the text entry. Get more information at here .","title":"Controls Reference"},{"location":"knowledge/mobile-development/xamarin/controls-reference/#controls-reference","text":"A Description of all the visual elements used to construct a Xamarin.Forms application . The visual interface of a Xamarin.Forms application is constructed of objects that map to the native controls of each target platform. This allows platform-specific applications for iOS, Android, and the Universal Windows Platform to use Xamarin.Forms code contained in a .NET Standard library or a Shared Project . The four main control groups used to create the user interface of a Xamarin.Forms application are shown in these four articles: Pages Layouts Views Cells A Xamarin.Forms page generally occupies the entire screen. The page usually contains a layout, which contains views and possibly other layouts. Cells are specialied components used in connection with TableView and ListView . In the four articles on Pages , Layouts , Views and Cells , each type of control is described with links to its API documentation, an article describing its use (if one exists), and one or more sample programs(if they exist). Each type of control is also accompanied by a screenshot showing a page from the FormsGallery sample running on iOS, Android, and UWP devices. Below each screenshot are links to the source code fot the C# page, the quivalent Xaml page, and (when appropriate) the C# code-behind file for the Xaml page. A cell is a specialized element used for items in a table and describes how each item in a list should be rendered. The Cell class derives from Element , from which VisualElement also derives. A cell is not itself a visual element; it is instead a template for creating a visual element.","title":"Controls Reference"},{"location":"knowledge/mobile-development/xamarin/controls-reference/#cells","text":"Xamarin.Forms supports the following cell types:","title":"Cells"},{"location":"knowledge/mobile-development/xamarin/controls-reference/#textcell","text":"A TextCell displays one or two text strings. Set the Text property and, optionally, the Detail property to these text strings.","title":"TextCell"},{"location":"knowledge/mobile-development/xamarin/controls-reference/#imagecell","text":"The ImageCell displays the same information as TextCell but includes a bitmap that you set with the Source property.","title":"ImageCell"},{"location":"knowledge/mobile-development/xamarin/controls-reference/#switchcell","text":"The SwitchCell contains text set with the Text ' property and on/off switch initiallly set with the Boolean On property. Handle the Onchanged event to be notified when the On property changes.","title":"SwitchCell"},{"location":"knowledge/mobile-development/xamarin/controls-reference/#entrycell","text":"The EntryCell defines a Label property that identifies the cell and a single line of editable text in the Text property. Handle the Completed event to be notified when the user has completed the text entry. Get more information at here .","title":"EntryCell"},{"location":"knowledge/mobile-development/xamarin/customizing-listview-cell-appearance/","text":"Cusomizing ListView Cell Appearence ListView presents scrollable lists, which can be customized through the use of ViewCells . ViewCells can be used for displaying text and images, indicating a true/false state and receiving user input. Built in Cells Xamarin.Forms comes with built-in cells that work for many simple applications: TextCell - for displaying text ImageCell - for displaying an image with text. Two additional cells, SwitchCell and EntryCell are available, however they aren't commonly used with ListView . See TableView for more information about these cells. TextCell TextCells are rendered as natice controls at runtime, so performace is very good compared to a custom ViewCell . TextCells are cusomizable, allowing you to set: Text : the text that shown on the first line, in large font. Detail : the text is shown underneath the first line, in a smaller font. TextColor : the color of the text. DetailColor : the color of the detail text. ImageCell ImageCell is usedful when you need to display a list of data with a visual aspect, such as a list of contacts or movies. ImageCells are customizable, allowing you to set: Text : the text that is shown on the firt line, in large font. Detail : the text that is shown underneath the first line, in a smaller font. TextColor : the color of the text. DetailColor : the color of the detail text ImageSource : the image to display next to the text. Custom Cells When the built-in cells don't provide the required layout, custom cells implemented the required layout. Build a cell with two lables that have equal weight. A TextCell would be insufficient because the TextCell has one label that is smaller. All custom cells must derive from ViewCell , the same base class that all of the built-inn cell types use. XAML The XAML to create the above layout is below: <?xml version=\"1.0\" encoding=\"UTF-8\"?> <ContentPage xmlns=\"http://xamarin.com/schemas/2014/forms\" xmlns:x=\"http://schemas.microsoft.com/winfx/2009/xaml\" x:Class=\"demoListView.ImageCellPage\"> <ContentPage.Content> <ListView x:Name=\"listView\"> <ListView.ItemTemplate> <DataTemplate> <ViewCell> <StackLayout BackgroundColor=\"#eee\" Orientation=\"Vertical\"> <StackLayout Orientation=\"Horizontal\"> <Image Source=\"{Binding image}\" /> <Label Text=\"{Binding title}\" TextColor=\"#f35e20\" /> <Label Text=\"{Binding subtitle}\" HorizontalOptions=\"EndAndExpand\" TextColor=\"#503026\" /> </StackLayout> </StackLayout> </ViewCell> </DataTemplate> </ListView.ItemTemplate> </ListView> </ContentPage.Content> </ContentPage> The XAML above is doing a lot. Let's break it down: The custom cell is nested inside a DataTemplate , which is inside ListView.ItemTemplate . ViewCell is the type of the custom cell. The child of the DataTemplate element must be of or derive from type ViewCell . Notice that inside the ViewCell , layout is managed by a StackLayout . This layout allows us to customize the backgtound color. Note that any property of StackLayout that bindable can be bound inside a custom cell, although that is not shown here. Inside the ViewCell , layout can be managed by any Xamarin.Forms layout. C# Specifying a custom cell in C# is a bit more verbose than the XAML equivalent. Let's take a look: First, define a custom cell class, with ViewCell as the base class: public class CustomCell : ViewCell { public CustomCell() { //instantiate each of our views var image = new Image (); StackLayout cellWrapper = new StackLayout (); StackLayout horizontalLayout = new StackLayout (); Label left = new Label (); Label right = new Label (); //set bindings left.SetBinding (Label.TextProperty, \"title\"); right.SetBinding (Label.TextProperty, \"subtitle\"); image.SetBinding (Image.SourceProperty, \"image\"); //Set properties for desired design cellWrapper.BackgroundColor = Color.FromHex (\"#eee\"); horizontalLayout.Orientation = StackOrientation.Horizontal; right.HorizontalOptions = LayoutOptions.EndAndExpand; left.TextColor = Color.FromHex (\"#f35e20\"); right.TextColor = Color.FromHex (\"503026\"); //add views to the view hierarchy horizontalLayout.Children.Add (image); horizontalLayout.Children.Add (left); horizontalLayout.Children.Add (right); cellWrapper.Children.Add (horizontalLayout); View = cellWrapper; } } In your constructor for the page with the ListView , set the ListView's ItemTemplate property to a new DataTemplate . public partial class ImageCellPage : ContentPage { public ImageCellPage () { InitializeComponent (); listView.ItemTemplate = new DataTemplate (typeof(CustomCell)); } } Note that the constructor for DataTemplate takes a type. The typeof operator gets the CLR type for CustomCell . Binding Context Changes When binding to a custom cell type's BindableProperty instances, the UI controls displaying the BindableProperty values should use the OnBindingContextChanged override to set the data to be displayed in each cell, rather than the cell contructor, as demonstrated in the following code example: public class CustomCell : ViewCell { Label nameLabel, ageLabel, locationLabel; public static readonly BindableProperty NameProperty = BindableProperty.Create (\"Name\", typeof(string), typeof(CustomCell), \"Name\"); public static readonly BindableProperty AgeProperty = BindableProperty.Create (\"Age\", typeof(int), typeof(CustomCell), 0); public static readonly BindableProperty LocationProperty = BindableProperty.Create (\"Location\", typeof(string), typeof(CustomCell), \"Location\"); public string Name { get { return(string)GetValue (NameProperty); } set { SetValue (NameProperty, value); } } public int Age { get { return(int)GetValue (AgeProperty); } set { SetValue (AgeProperty, value); } } public string Location { get { return(string)GetValue (LocationProperty); } set { SetValue (LocationProperty, value); } } ... protected override void OnBindingContextChanged () { base.OnBindingContextChanged (); if (BindingContext != null) { nameLabel.Text = Name; ageLabel.Text = Age.ToString (); locationLabel.Text = Location; } } } The OnBindingContextChanged override will be called when the BindingContextChanged event fires, in response to the value of the BindingContext property changing. Therefore when the BindingContext changes, the UI controls displaying the BindableProperty values should set their data. Note that the BindingContext should be checked for a null value, as this can be set by Xamarin.Forms for garbage collection, which in turn will result in the OnbindingContextChanged override being called. Alternatively, UI controls can bind to the BindableProperty instances to display their values, which removes the need to override the OnBindingContextChanged method. In XAML, binding the custom cell type to data can be achieved as shown in the following code example: <ListView x:Name=\"listView\"> <ListView.ItemTemplate> <DataTemplate> <local:CustomCell Name=\"{Binding Name}\" Age=\"{Binding Age}\" Location=\"{Binding Location}\" /> </DataTemplate> </ListView.ItemTemplate> </ListView> Get more information at here .","title":"Customizing ListView cell Appearence"},{"location":"knowledge/mobile-development/xamarin/customizing-listview-cell-appearance/#cusomizing-listview-cell-appearence","text":"ListView presents scrollable lists, which can be customized through the use of ViewCells . ViewCells can be used for displaying text and images, indicating a true/false state and receiving user input.","title":"Cusomizing ListView Cell Appearence"},{"location":"knowledge/mobile-development/xamarin/customizing-listview-cell-appearance/#built-in-cells","text":"Xamarin.Forms comes with built-in cells that work for many simple applications: TextCell - for displaying text ImageCell - for displaying an image with text. Two additional cells, SwitchCell and EntryCell are available, however they aren't commonly used with ListView . See TableView for more information about these cells.","title":"Built in Cells"},{"location":"knowledge/mobile-development/xamarin/customizing-listview-cell-appearance/#textcell","text":"TextCells are rendered as natice controls at runtime, so performace is very good compared to a custom ViewCell . TextCells are cusomizable, allowing you to set: Text : the text that shown on the first line, in large font. Detail : the text is shown underneath the first line, in a smaller font. TextColor : the color of the text. DetailColor : the color of the detail text.","title":"TextCell"},{"location":"knowledge/mobile-development/xamarin/customizing-listview-cell-appearance/#imagecell","text":"ImageCell is usedful when you need to display a list of data with a visual aspect, such as a list of contacts or movies. ImageCells are customizable, allowing you to set: Text : the text that is shown on the firt line, in large font. Detail : the text that is shown underneath the first line, in a smaller font. TextColor : the color of the text. DetailColor : the color of the detail text ImageSource : the image to display next to the text.","title":"ImageCell"},{"location":"knowledge/mobile-development/xamarin/customizing-listview-cell-appearance/#custom-cells","text":"When the built-in cells don't provide the required layout, custom cells implemented the required layout. Build a cell with two lables that have equal weight. A TextCell would be insufficient because the TextCell has one label that is smaller. All custom cells must derive from ViewCell , the same base class that all of the built-inn cell types use. XAML The XAML to create the above layout is below: <?xml version=\"1.0\" encoding=\"UTF-8\"?> <ContentPage xmlns=\"http://xamarin.com/schemas/2014/forms\" xmlns:x=\"http://schemas.microsoft.com/winfx/2009/xaml\" x:Class=\"demoListView.ImageCellPage\"> <ContentPage.Content> <ListView x:Name=\"listView\"> <ListView.ItemTemplate> <DataTemplate> <ViewCell> <StackLayout BackgroundColor=\"#eee\" Orientation=\"Vertical\"> <StackLayout Orientation=\"Horizontal\"> <Image Source=\"{Binding image}\" /> <Label Text=\"{Binding title}\" TextColor=\"#f35e20\" /> <Label Text=\"{Binding subtitle}\" HorizontalOptions=\"EndAndExpand\" TextColor=\"#503026\" /> </StackLayout> </StackLayout> </ViewCell> </DataTemplate> </ListView.ItemTemplate> </ListView> </ContentPage.Content> </ContentPage> The XAML above is doing a lot. Let's break it down: The custom cell is nested inside a DataTemplate , which is inside ListView.ItemTemplate . ViewCell is the type of the custom cell. The child of the DataTemplate element must be of or derive from type ViewCell . Notice that inside the ViewCell , layout is managed by a StackLayout . This layout allows us to customize the backgtound color. Note that any property of StackLayout that bindable can be bound inside a custom cell, although that is not shown here. Inside the ViewCell , layout can be managed by any Xamarin.Forms layout. C# Specifying a custom cell in C# is a bit more verbose than the XAML equivalent. Let's take a look: First, define a custom cell class, with ViewCell as the base class: public class CustomCell : ViewCell { public CustomCell() { //instantiate each of our views var image = new Image (); StackLayout cellWrapper = new StackLayout (); StackLayout horizontalLayout = new StackLayout (); Label left = new Label (); Label right = new Label (); //set bindings left.SetBinding (Label.TextProperty, \"title\"); right.SetBinding (Label.TextProperty, \"subtitle\"); image.SetBinding (Image.SourceProperty, \"image\"); //Set properties for desired design cellWrapper.BackgroundColor = Color.FromHex (\"#eee\"); horizontalLayout.Orientation = StackOrientation.Horizontal; right.HorizontalOptions = LayoutOptions.EndAndExpand; left.TextColor = Color.FromHex (\"#f35e20\"); right.TextColor = Color.FromHex (\"503026\"); //add views to the view hierarchy horizontalLayout.Children.Add (image); horizontalLayout.Children.Add (left); horizontalLayout.Children.Add (right); cellWrapper.Children.Add (horizontalLayout); View = cellWrapper; } } In your constructor for the page with the ListView , set the ListView's ItemTemplate property to a new DataTemplate . public partial class ImageCellPage : ContentPage { public ImageCellPage () { InitializeComponent (); listView.ItemTemplate = new DataTemplate (typeof(CustomCell)); } } Note that the constructor for DataTemplate takes a type. The typeof operator gets the CLR type for CustomCell . Binding Context Changes When binding to a custom cell type's BindableProperty instances, the UI controls displaying the BindableProperty values should use the OnBindingContextChanged override to set the data to be displayed in each cell, rather than the cell contructor, as demonstrated in the following code example: public class CustomCell : ViewCell { Label nameLabel, ageLabel, locationLabel; public static readonly BindableProperty NameProperty = BindableProperty.Create (\"Name\", typeof(string), typeof(CustomCell), \"Name\"); public static readonly BindableProperty AgeProperty = BindableProperty.Create (\"Age\", typeof(int), typeof(CustomCell), 0); public static readonly BindableProperty LocationProperty = BindableProperty.Create (\"Location\", typeof(string), typeof(CustomCell), \"Location\"); public string Name { get { return(string)GetValue (NameProperty); } set { SetValue (NameProperty, value); } } public int Age { get { return(int)GetValue (AgeProperty); } set { SetValue (AgeProperty, value); } } public string Location { get { return(string)GetValue (LocationProperty); } set { SetValue (LocationProperty, value); } } ... protected override void OnBindingContextChanged () { base.OnBindingContextChanged (); if (BindingContext != null) { nameLabel.Text = Name; ageLabel.Text = Age.ToString (); locationLabel.Text = Location; } } } The OnBindingContextChanged override will be called when the BindingContextChanged event fires, in response to the value of the BindingContext property changing. Therefore when the BindingContext changes, the UI controls displaying the BindableProperty values should set their data. Note that the BindingContext should be checked for a null value, as this can be set by Xamarin.Forms for garbage collection, which in turn will result in the OnbindingContextChanged override being called. Alternatively, UI controls can bind to the BindableProperty instances to display their values, which removes the need to override the OnBindingContextChanged method. In XAML, binding the custom cell type to data can be achieved as shown in the following code example: <ListView x:Name=\"listView\"> <ListView.ItemTemplate> <DataTemplate> <local:CustomCell Name=\"{Binding Name}\" Age=\"{Binding Age}\" Location=\"{Binding Location}\" /> </DataTemplate> </ListView.ItemTemplate> </ListView> Get more information at here .","title":"Custom Cells"},{"location":"knowledge/mobile-development/xamarin/data-pages-controls/","text":"DataPages Controls Reference Built-in Controls HeroImage ListItem HeroImage The HeroImage control has four properties: Text Detail ImageSource Aspect <pages:HeroImage ImageSource=\"{ DynamicResource HeroImageImage }\" Text=\"Keith Ballinger\" Detail=\"Xamarin\" /> ListItem The ListItem control's layout is similar to native iOS and Android lis or table rows, however it can also used as a regular view. In the example code below it is shown hosted inside a StackLayout , but it can also be used in data-bound scolling list controls. There are five properties: Title Detail ImageSource PlaceholdImageSource Aspect <StackLayout Spacing=\"0\"> <pages:ListItemControl Detail=\"Xamarin\" ImageSource=\"{ DynamicResource UserImage }\" Title=\"Miguel de Icaza\" PlaceholdImageSource=\"{ DynamicResource IconImage }\" /> Custom Control Example The goal of this custom CardView control is to resemble the native Android CardView. It will contain three properties: Text Detail ImageSource The goal is a custom control that will look like the code below(note that a custom xmlns:local is required that references the current assembly): <local:CardView ImageSource=\"{ DynamicResource CardViewImage }\" Text=\"CardView Text\" Detail=\"CardView Detail\" /> Building the Custom CarView DataView Subclass The C# subclass of DataView defines the bindable properties for the control. public class CardView : DataView { public static readonly BindableProperty TextProperty = BindableProperty.Create (\"Text\", typeof (string), typeof (CardView), null, BindingMode.TwoWay); public string Text { get { return (string)GetValue (TextProperty); } set { SetValue (TextProperty, value); } } public static readonly BindableProperty DetailProperty = BindableProperty.Create (\"Detail\", typeof (string), typeof (CardView), null, BindingMode.TwoWay); public string Detail { get { return (string)GetValue (DetailProperty); } set { SetValue (DetailProperty, value); } } public static readonly BindableProperty ImageSourceProperty = BindableProperty.Create (\"ImageSource\", typeof (ImageSource), typeof (CardView), null, BindingMode.TwoWay); public ImageSource ImageSource { get { return (ImageSource)GetValue (ImageSourceProperty); } set { SetValue (ImageSourceProperty, value); } } public CardView() { } } Define Font, Layout, and Margins The control designer would figure out these values as part of the user-interface design for the custom control. Where platform-specific specifications are required, the OnPlatform element is used. Note that some values refer to StaticResource s - these will be defined in step 5 . <!-- CARDVIEW FONT SIZES --> <OnPlatform x:TypeArguments=\"x:Double\" x:Key=\"CardViewTextFontSize\"> <On Platform=\"iOS, Android\" Value=\"15\" /> </OnPlatform> <OnPlatform x:TypeArguments=\"x:Double\" x:Key=\"CardViewDetailFontSize\"> <On Platform=\"iOS, Android\" Value=\"13\" /> </OnPlatform> <OnPlatform x:TypeArguments=\"Color\" x:Key=\"CardViewTextTextColor\"> <On Platform=\"iOS\" Value=\"{StaticResource iOSCardViewTextTextColor}\" /> <On Platform=\"Android\" Value=\"{StaticResource AndroidCardViewTextTextColor}\" /> </OnPlatform> <OnPlatform x:TypeArguments=\"Thickness\" x:Key=\"CardViewTextlMargin\"> <On Platform=\"iOS\" Value=\"12,10,12,4\" /> <On Platform=\"Android\" Value=\"20,0,20,5\" /> </OnPlatform> <OnPlatform x:TypeArguments=\"Color\" x:Key=\"CardViewDetailTextColor\"> <On Platform=\"iOS\" Value=\"{StaticResource iOSCardViewDetailTextColor}\" /> <On Platform=\"Android\" Value=\"{StaticResource AndroidCardViewDetailTextColor}\" /> </OnPlatform> <OnPlatform x:TypeArguments=\"Thickness\" x:Key=\"CardViewDetailMargin\"> <On Platform=\"iOS\" Value=\"12,0,10,12\" /> <On Platform=\"Android\" Value=\"20,0,20,20\" /> </OnPlatform> <OnPlatform x:TypeArguments=\"Color\" x:Key=\"CardViewBackgroundColor\"> <On Platform=\"iOS\" Value=\"{StaticResource iOSCardViewBackgroundColor}\" /> <On Platform=\"Android\" Value=\"{StaticResource AndroidCardViewBackgroundColor}\" /> </OnPlatform> <OnPlatform x:TypeArguments=\"x:Double\" x:Key=\"CardViewShadowSize\"> <On Platform=\"iOS\" Value=\"2\" /> <On Platform=\"Android\" Value=\"5\" /> </OnPlatform> <OnPlatform x:TypeArguments=\"x:Double\" x:Key=\"CardViewCornerRadius\"> <On Platform=\"iOS\" Value=\"0\" /> <On Platform=\"Android\" Value=\"4\" /> </OnPlatform> <OnPlatform x:TypeArguments=\"Color\" x:Key=\"CardViewShadowColor\"> <On Platform=\"iOS, Android\" Value=\"#CDCDD1\" /> </OnPlatform> Create Styles for the Control's Children Reference all the elements defined about to create the children that will be used in the custom control: <!-- EXPLICIT STYLES (will be Classes) --> <Style TargetType=\"Label\" x:Key=\"CardViewTextStyle\"> <Setter Property=\"FontSize\" Value=\"{ StaticResource CardViewTextFontSize }\" /> <Setter Property=\"TextColor\" Value=\"{ StaticResource CardViewTextTextColor }\" /> <Setter Property=\"HorizontalOptions\" Value=\"Start\" /> <Setter Property=\"Margin\" Value=\"{ StaticResource CardViewTextlMargin }\" /> <Setter Property=\"HorizontalTextAlignment\" Value=\"Start\" /> </Style> <Style TargetType=\"Label\" x:Key=\"CardViewDetailStyle\"> <Setter Property=\"HorizontalTextAlignment\" Value=\"Start\" /> <Setter Property=\"TextColor\" Value=\"{ StaticResource CardViewDetailTextColor }\" /> <Setter Property=\"FontSize\" Value=\"{ StaticResource CardViewDetailFontSize }\" /> <Setter Property=\"HorizontalOptions\" Value=\"Start\" /> <Setter Property=\"Margin\" Value=\"{ StaticResource CardViewDetailMargin }\" /> </Style> <Style TargetType=\"Image\" x:Key=\"CardViewImageImageStyle\"> <Setter Property=\"HorizontalOptions\" Value=\"Center\" /> <Setter Property=\"VerticalOptions\" Value=\"Center\" /> <Setter Property=\"WidthRequest\" Value=\"220\"/> <Setter Property=\"HeightRequest\" Value=\"165\"/> </Style> Create the Control Layout Template The visual design of the custom control is explicitly declared in the control template, using the resources defined above: <!--- CARDVIEW --> <ControlTemplate x:Key=\"CardViewControlControlTemplate\"> <StackLayout Spacing=\"0\" BackgroundColor=\"{ TemplateBinding BackgroundColor }\" > <!-- CARDVIEW IMAGE --> <Image Source=\"{ TemplateBinding ImageSource }\" HorizontalOptions=\"FillAndExpand\" VerticalOptions=\"StartAndExpand\" Aspect=\"AspectFill\" Style=\"{ StaticResource CardViewImageImageStyle }\" /> <!-- CARDVIEW TEXT --> <Label Text=\"{ TemplateBinding Text }\" LineBreakMode=\"WordWrap\" VerticalOptions=\"End\" Style=\"{ StaticResource CardViewTextStyle }\" /> <!-- CARDVIEW DETAIL --> <Label Text=\"{ TemplateBinding Detail }\" LineBreakMode=\"WordWrap\" VerticalOptions=\"End\" Style=\"{ StaticResource CardViewDetailStyle }\" /> </StackLayout> </ControlTemplate> Add the Theme-specific Resources Because this is a custom control, add the resources that match the theme you are using the rersource dictionary: Light Theme Colors <Color x:Key=\"iOSCardViewBackgroundColor\">#FFFFFF</Color> <Color x:Key=\"AndroidCardViewBackgroundColor\">#FFFFFF</Color> <Color x:Key=\"AndroidCardViewTextTextColor\">#030303</Color> <Color x:Key=\"iOSCardViewTextTextColor\">#030303</Color> <Color x:Key=\"AndroidCardViewDetailTextColor\">#8F8E94</Color> <Color x:Key=\"iOSCardViewDetailTextColor\">#8F8E94</Color> Dark Theme Colors <!-- CARD VIEW COLORS --> <Color x:Key=\"iOSCardViewBackgroundColor\">#404040</Color> <Color x:Key=\"AndroidCardViewBackgroundColor\">#404040</Color> <Color x:Key=\"AndroidCardViewTextTextColor\">#FFFFFF</Color> <Color x:Key=\"iOSCardViewTextTextColor\">#FFFFFF</Color> <Color x:Key=\"AndroidCardViewDetailTextColor\">#B5B4B9</Color> <Color x:Key=\"iOSCardViewDetailTextColor\">#B5B4B9</Color> Set the ControlTemplate for the CardView class Finnally, ensure the C# class created in step 1 uses the control template defined in step 4 using a Style Setter element <Style TargetType=\"local:CardView\"> <Setter Property=\"ControlTemplate\" Value=\"{ StaticResource CardViewControlControlTemplate }\" /> ... some custom styling omitted <Setter Property=\"BackgroundColor\" Value=\"{ StaticResource CardViewBackgroundColor }\" /> </Style> Add the Control to a Page The CardView control can now be added to a page. The example below shows it hosted in a StackLayout <StackLayout Spacing=\"0\"> <local:CardView Margin=\"12,6\" ImageSource=\"{ DynamicResource CardViewImage }\" Text=\"CardView Text\" Detail=\"CardView Detail\" /> </StackLayout> More information at here .","title":"Data Pages Controls"},{"location":"knowledge/mobile-development/xamarin/data-pages-controls/#datapages-controls-reference","text":"","title":"DataPages Controls Reference"},{"location":"knowledge/mobile-development/xamarin/data-pages-controls/#built-in-controls","text":"HeroImage ListItem","title":"Built-in Controls"},{"location":"knowledge/mobile-development/xamarin/data-pages-controls/#heroimage","text":"The HeroImage control has four properties: Text Detail ImageSource Aspect <pages:HeroImage ImageSource=\"{ DynamicResource HeroImageImage }\" Text=\"Keith Ballinger\" Detail=\"Xamarin\" />","title":"HeroImage"},{"location":"knowledge/mobile-development/xamarin/data-pages-controls/#listitem","text":"The ListItem control's layout is similar to native iOS and Android lis or table rows, however it can also used as a regular view. In the example code below it is shown hosted inside a StackLayout , but it can also be used in data-bound scolling list controls. There are five properties: Title Detail ImageSource PlaceholdImageSource Aspect <StackLayout Spacing=\"0\"> <pages:ListItemControl Detail=\"Xamarin\" ImageSource=\"{ DynamicResource UserImage }\" Title=\"Miguel de Icaza\" PlaceholdImageSource=\"{ DynamicResource IconImage }\" />","title":"ListItem"},{"location":"knowledge/mobile-development/xamarin/data-pages-controls/#custom-control-example","text":"The goal of this custom CardView control is to resemble the native Android CardView. It will contain three properties: Text Detail ImageSource The goal is a custom control that will look like the code below(note that a custom xmlns:local is required that references the current assembly): <local:CardView ImageSource=\"{ DynamicResource CardViewImage }\" Text=\"CardView Text\" Detail=\"CardView Detail\" />","title":"Custom Control Example"},{"location":"knowledge/mobile-development/xamarin/data-pages-controls/#building-the-custom-carview","text":"","title":"Building the Custom CarView"},{"location":"knowledge/mobile-development/xamarin/data-pages-controls/#dataview-subclass","text":"The C# subclass of DataView defines the bindable properties for the control. public class CardView : DataView { public static readonly BindableProperty TextProperty = BindableProperty.Create (\"Text\", typeof (string), typeof (CardView), null, BindingMode.TwoWay); public string Text { get { return (string)GetValue (TextProperty); } set { SetValue (TextProperty, value); } } public static readonly BindableProperty DetailProperty = BindableProperty.Create (\"Detail\", typeof (string), typeof (CardView), null, BindingMode.TwoWay); public string Detail { get { return (string)GetValue (DetailProperty); } set { SetValue (DetailProperty, value); } } public static readonly BindableProperty ImageSourceProperty = BindableProperty.Create (\"ImageSource\", typeof (ImageSource), typeof (CardView), null, BindingMode.TwoWay); public ImageSource ImageSource { get { return (ImageSource)GetValue (ImageSourceProperty); } set { SetValue (ImageSourceProperty, value); } } public CardView() { } }","title":"DataView Subclass"},{"location":"knowledge/mobile-development/xamarin/data-pages-controls/#define-font-layout-and-margins","text":"The control designer would figure out these values as part of the user-interface design for the custom control. Where platform-specific specifications are required, the OnPlatform element is used. Note that some values refer to StaticResource s - these will be defined in step 5 . <!-- CARDVIEW FONT SIZES --> <OnPlatform x:TypeArguments=\"x:Double\" x:Key=\"CardViewTextFontSize\"> <On Platform=\"iOS, Android\" Value=\"15\" /> </OnPlatform> <OnPlatform x:TypeArguments=\"x:Double\" x:Key=\"CardViewDetailFontSize\"> <On Platform=\"iOS, Android\" Value=\"13\" /> </OnPlatform> <OnPlatform x:TypeArguments=\"Color\" x:Key=\"CardViewTextTextColor\"> <On Platform=\"iOS\" Value=\"{StaticResource iOSCardViewTextTextColor}\" /> <On Platform=\"Android\" Value=\"{StaticResource AndroidCardViewTextTextColor}\" /> </OnPlatform> <OnPlatform x:TypeArguments=\"Thickness\" x:Key=\"CardViewTextlMargin\"> <On Platform=\"iOS\" Value=\"12,10,12,4\" /> <On Platform=\"Android\" Value=\"20,0,20,5\" /> </OnPlatform> <OnPlatform x:TypeArguments=\"Color\" x:Key=\"CardViewDetailTextColor\"> <On Platform=\"iOS\" Value=\"{StaticResource iOSCardViewDetailTextColor}\" /> <On Platform=\"Android\" Value=\"{StaticResource AndroidCardViewDetailTextColor}\" /> </OnPlatform> <OnPlatform x:TypeArguments=\"Thickness\" x:Key=\"CardViewDetailMargin\"> <On Platform=\"iOS\" Value=\"12,0,10,12\" /> <On Platform=\"Android\" Value=\"20,0,20,20\" /> </OnPlatform> <OnPlatform x:TypeArguments=\"Color\" x:Key=\"CardViewBackgroundColor\"> <On Platform=\"iOS\" Value=\"{StaticResource iOSCardViewBackgroundColor}\" /> <On Platform=\"Android\" Value=\"{StaticResource AndroidCardViewBackgroundColor}\" /> </OnPlatform> <OnPlatform x:TypeArguments=\"x:Double\" x:Key=\"CardViewShadowSize\"> <On Platform=\"iOS\" Value=\"2\" /> <On Platform=\"Android\" Value=\"5\" /> </OnPlatform> <OnPlatform x:TypeArguments=\"x:Double\" x:Key=\"CardViewCornerRadius\"> <On Platform=\"iOS\" Value=\"0\" /> <On Platform=\"Android\" Value=\"4\" /> </OnPlatform> <OnPlatform x:TypeArguments=\"Color\" x:Key=\"CardViewShadowColor\"> <On Platform=\"iOS, Android\" Value=\"#CDCDD1\" /> </OnPlatform>","title":"Define Font, Layout, and Margins"},{"location":"knowledge/mobile-development/xamarin/data-pages-controls/#create-styles-for-the-controls-children","text":"Reference all the elements defined about to create the children that will be used in the custom control: <!-- EXPLICIT STYLES (will be Classes) --> <Style TargetType=\"Label\" x:Key=\"CardViewTextStyle\"> <Setter Property=\"FontSize\" Value=\"{ StaticResource CardViewTextFontSize }\" /> <Setter Property=\"TextColor\" Value=\"{ StaticResource CardViewTextTextColor }\" /> <Setter Property=\"HorizontalOptions\" Value=\"Start\" /> <Setter Property=\"Margin\" Value=\"{ StaticResource CardViewTextlMargin }\" /> <Setter Property=\"HorizontalTextAlignment\" Value=\"Start\" /> </Style> <Style TargetType=\"Label\" x:Key=\"CardViewDetailStyle\"> <Setter Property=\"HorizontalTextAlignment\" Value=\"Start\" /> <Setter Property=\"TextColor\" Value=\"{ StaticResource CardViewDetailTextColor }\" /> <Setter Property=\"FontSize\" Value=\"{ StaticResource CardViewDetailFontSize }\" /> <Setter Property=\"HorizontalOptions\" Value=\"Start\" /> <Setter Property=\"Margin\" Value=\"{ StaticResource CardViewDetailMargin }\" /> </Style> <Style TargetType=\"Image\" x:Key=\"CardViewImageImageStyle\"> <Setter Property=\"HorizontalOptions\" Value=\"Center\" /> <Setter Property=\"VerticalOptions\" Value=\"Center\" /> <Setter Property=\"WidthRequest\" Value=\"220\"/> <Setter Property=\"HeightRequest\" Value=\"165\"/> </Style>","title":"Create Styles for the Control's Children"},{"location":"knowledge/mobile-development/xamarin/data-pages-controls/#create-the-control-layout-template","text":"The visual design of the custom control is explicitly declared in the control template, using the resources defined above: <!--- CARDVIEW --> <ControlTemplate x:Key=\"CardViewControlControlTemplate\"> <StackLayout Spacing=\"0\" BackgroundColor=\"{ TemplateBinding BackgroundColor }\" > <!-- CARDVIEW IMAGE --> <Image Source=\"{ TemplateBinding ImageSource }\" HorizontalOptions=\"FillAndExpand\" VerticalOptions=\"StartAndExpand\" Aspect=\"AspectFill\" Style=\"{ StaticResource CardViewImageImageStyle }\" /> <!-- CARDVIEW TEXT --> <Label Text=\"{ TemplateBinding Text }\" LineBreakMode=\"WordWrap\" VerticalOptions=\"End\" Style=\"{ StaticResource CardViewTextStyle }\" /> <!-- CARDVIEW DETAIL --> <Label Text=\"{ TemplateBinding Detail }\" LineBreakMode=\"WordWrap\" VerticalOptions=\"End\" Style=\"{ StaticResource CardViewDetailStyle }\" /> </StackLayout> </ControlTemplate>","title":"Create the Control Layout Template"},{"location":"knowledge/mobile-development/xamarin/data-pages-controls/#add-the-theme-specific-resources","text":"Because this is a custom control, add the resources that match the theme you are using the rersource dictionary: Light Theme Colors <Color x:Key=\"iOSCardViewBackgroundColor\">#FFFFFF</Color> <Color x:Key=\"AndroidCardViewBackgroundColor\">#FFFFFF</Color> <Color x:Key=\"AndroidCardViewTextTextColor\">#030303</Color> <Color x:Key=\"iOSCardViewTextTextColor\">#030303</Color> <Color x:Key=\"AndroidCardViewDetailTextColor\">#8F8E94</Color> <Color x:Key=\"iOSCardViewDetailTextColor\">#8F8E94</Color> Dark Theme Colors <!-- CARD VIEW COLORS --> <Color x:Key=\"iOSCardViewBackgroundColor\">#404040</Color> <Color x:Key=\"AndroidCardViewBackgroundColor\">#404040</Color> <Color x:Key=\"AndroidCardViewTextTextColor\">#FFFFFF</Color> <Color x:Key=\"iOSCardViewTextTextColor\">#FFFFFF</Color> <Color x:Key=\"AndroidCardViewDetailTextColor\">#B5B4B9</Color> <Color x:Key=\"iOSCardViewDetailTextColor\">#B5B4B9</Color>","title":"Add the Theme-specific Resources"},{"location":"knowledge/mobile-development/xamarin/data-pages-controls/#set-the-controltemplate-for-the-cardview-class","text":"Finnally, ensure the C# class created in step 1 uses the control template defined in step 4 using a Style Setter element <Style TargetType=\"local:CardView\"> <Setter Property=\"ControlTemplate\" Value=\"{ StaticResource CardViewControlControlTemplate }\" /> ... some custom styling omitted <Setter Property=\"BackgroundColor\" Value=\"{ StaticResource CardViewBackgroundColor }\" /> </Style>","title":"Set the ControlTemplate for the CardView class"},{"location":"knowledge/mobile-development/xamarin/data-pages-controls/#add-the-control-to-a-page","text":"The CardView control can now be added to a page. The example below shows it hosted in a StackLayout <StackLayout Spacing=\"0\"> <local:CardView Margin=\"12,6\" ImageSource=\"{ DynamicResource CardViewImage }\" Text=\"CardView Text\" Detail=\"CardView Detail\" /> </StackLayout> More information at here .","title":"Add the Control to a Page"},{"location":"knowledge/mobile-development/xamarin/data-pages/","text":"Data Pages Get Started To get started building a simplw data-driven page using the DataPages Preview, follow the steps below. This demo uses a hardcoded style (\"Events\") in the Preview builds that only works with the specific JSON format in the code. Add Nuget Packages Add these Nuget packages to your Xamarin.Forms .NET Standard library and application projects: Xamarin.Forms.Pages Xamarin.Forms.Theme.Base A theme implementation Nuget(eg. Xamarin.Forms.Themes.Light) Add Theme Reference In the App.xaml file, add a custom xmlns:mytheme for the theme and ensure the theme is merged into the application's resource dictionary: <Application xmlns=\"http://xamarin.com/schemas/2014/forms\" xmlns:x=\"http://schemas.microsoft.com/winfx/2009/xaml\" xmlns:mytheme=\"clr-namespace:Xamarin.Forms.Themes;assembly=Xamarin.Forms.Theme.Light\" x:Class=\"DataPagesDemo.App\"> <Application.Resources> <ResourceDictionary MergedWith=\"mytheme:LightThemeResources\" /> </Application.Resources> </Application> Add a XAML Page Add a new XAML page to the Xamarin.Forms application, and change the base class form ContentPage to Xamarin.Forms.Pages.ListDataPage. This has to be done in both the C# and the XAML C# file public partial class SessionDataPage : Xamarin.Forms.Pages.ListDataPage // was ContentPage { public SessionDataPage () { InitializeComponent (); } } Add the DataSource Delete the Content element and replace it with a p:ListDataPage.DataSource to populate the page with data. In the example below a remote Json data file is being loaded from a URL. Note : the preview requires a StyleClass attribute to provide rendering hints for the data source. The StyleClass=\"Events\" refers to a layout that is predefined in the preview and contains styles hardcoded to match the JSON data source being used. <?xml version=\"1.0\" encoding=\"UTF-8\"?> <p:ListDataPage xmlns=\"http://xamarin.com/schemas/2014/forms\" xmlns:x=\"http://schemas.microsoft.com/winfx/2009/xaml\" xmlns:p=\"clr-namespace:Xamarin.Forms.Pages;assembly=Xamarin.Forms.Pages\" x:Class=\"DataPagesDemo.SessionDataPage\" Title=\"Sessions\" StyleClass=\"Events\"> <p:ListDataPage.DataSource> <p:JsonDataSource Source=\"http://demo3143189.mockable.io/sessions\" /> </p:ListDataPage.DataSource> </p:ListDataPage> JSON data [ { \"end\": \"2016-04-27T18:00:00Z\", \"start\": \"2016-04-27T17:15:00Z\", \"abstract\": \"The new Apple TV has been released, and YOU can be one of the first developers to write apps for it. To make things even better, you can build these apps in C#! This session will introduce the basics of how to create a tvOS app with Xamarin, including: differences between tvOS and iOS APIs, TV user interface best practices, responding to user input, as well as the capabilities and limitations of building apps for a television. Grab some popcorn\u2014this is going to be good!\", \"title\": \"As Seen On TV \u2026 Bringing C# to the Living Room\", \"presenter\": \"Matthew Soucoup\", \"biography\": \"Matthew is a Xamarin MVP and Certified Xamarin Developer from Madison, WI. He founded his company Code Mill Technologies and started the Madison Mobile .Net Developers Group. Matt regularly speaks on .Net and Xamarin development at user groups, code camps and conferences throughout the Midwest. Matt gardens hot peppers, rides bikes, and loves Wisconsin micro-brews and cheese.\", \"image\": \"http://i.imgur.com/ASj60DP.jpg\", \"avatar\": \"http://i.imgur.com/ASj60DP.jpg\", \"room\": \"Crick\" } ] Run! This works because the pre-built style \"Events\" exists in the Light Theme Nuget package and has styles defined that match the data source.(eg. \"title\", \"image\", \"presenter\"). The \"Events\" StyleClass is built to display the ListDataPage control with a custom CardView control that is defined in Xamarin.Forms.Pages. The CardView control has three properties: ImageSource , Text , Detail . The theme is hardcoded to bind the datasource's three fields(from the JSON file) to these properties for display. Customize The inherite style can be overrideden by specifying a template and using data source bindings. The Xaml below declares a custom template for each row using the new ListItemControl and {p:DataSourceBinding} syntax which is includeed in the Xamarin.Forms.Pages Nuget: <p:ListDataPage.DefaultItemTemplate> <DataTemplate> <ViewCell> <p:ListItemControl Title=\"{p:DataSourceBinding title}\" Detail=\"{p:DataSourceBinding room}\" ImageSource=\"{p:DataSourceBinding image}\" DataSource=\"{Binding Value}\" HeightRequest=\"90\" > </p:ListItemControl> </ViewCell> </DataTemplate> </p:ListDataPage.DefaultItemTemplate> By providing a DataTemplate this code overrides the StyleClass and instead uses the default layout for a ListItemControl . Developers that prefer C# to XAML can create data source bindings too (remember to include a using Xamarin.Form.Pages; statement) SetBinding (TitleProperty, new DataSourceBinding (\"title\")); More information at here .","title":"Data Pages"},{"location":"knowledge/mobile-development/xamarin/data-pages/#data-pages","text":"","title":"Data Pages"},{"location":"knowledge/mobile-development/xamarin/data-pages/#get-started","text":"To get started building a simplw data-driven page using the DataPages Preview, follow the steps below. This demo uses a hardcoded style (\"Events\") in the Preview builds that only works with the specific JSON format in the code. Add Nuget Packages Add these Nuget packages to your Xamarin.Forms .NET Standard library and application projects: Xamarin.Forms.Pages Xamarin.Forms.Theme.Base A theme implementation Nuget(eg. Xamarin.Forms.Themes.Light) Add Theme Reference In the App.xaml file, add a custom xmlns:mytheme for the theme and ensure the theme is merged into the application's resource dictionary: <Application xmlns=\"http://xamarin.com/schemas/2014/forms\" xmlns:x=\"http://schemas.microsoft.com/winfx/2009/xaml\" xmlns:mytheme=\"clr-namespace:Xamarin.Forms.Themes;assembly=Xamarin.Forms.Theme.Light\" x:Class=\"DataPagesDemo.App\"> <Application.Resources> <ResourceDictionary MergedWith=\"mytheme:LightThemeResources\" /> </Application.Resources> </Application> Add a XAML Page Add a new XAML page to the Xamarin.Forms application, and change the base class form ContentPage to Xamarin.Forms.Pages.ListDataPage. This has to be done in both the C# and the XAML C# file public partial class SessionDataPage : Xamarin.Forms.Pages.ListDataPage // was ContentPage { public SessionDataPage () { InitializeComponent (); } } Add the DataSource Delete the Content element and replace it with a p:ListDataPage.DataSource to populate the page with data. In the example below a remote Json data file is being loaded from a URL. Note : the preview requires a StyleClass attribute to provide rendering hints for the data source. The StyleClass=\"Events\" refers to a layout that is predefined in the preview and contains styles hardcoded to match the JSON data source being used. <?xml version=\"1.0\" encoding=\"UTF-8\"?> <p:ListDataPage xmlns=\"http://xamarin.com/schemas/2014/forms\" xmlns:x=\"http://schemas.microsoft.com/winfx/2009/xaml\" xmlns:p=\"clr-namespace:Xamarin.Forms.Pages;assembly=Xamarin.Forms.Pages\" x:Class=\"DataPagesDemo.SessionDataPage\" Title=\"Sessions\" StyleClass=\"Events\"> <p:ListDataPage.DataSource> <p:JsonDataSource Source=\"http://demo3143189.mockable.io/sessions\" /> </p:ListDataPage.DataSource> </p:ListDataPage> JSON data [ { \"end\": \"2016-04-27T18:00:00Z\", \"start\": \"2016-04-27T17:15:00Z\", \"abstract\": \"The new Apple TV has been released, and YOU can be one of the first developers to write apps for it. To make things even better, you can build these apps in C#! This session will introduce the basics of how to create a tvOS app with Xamarin, including: differences between tvOS and iOS APIs, TV user interface best practices, responding to user input, as well as the capabilities and limitations of building apps for a television. Grab some popcorn\u2014this is going to be good!\", \"title\": \"As Seen On TV \u2026 Bringing C# to the Living Room\", \"presenter\": \"Matthew Soucoup\", \"biography\": \"Matthew is a Xamarin MVP and Certified Xamarin Developer from Madison, WI. He founded his company Code Mill Technologies and started the Madison Mobile .Net Developers Group. Matt regularly speaks on .Net and Xamarin development at user groups, code camps and conferences throughout the Midwest. Matt gardens hot peppers, rides bikes, and loves Wisconsin micro-brews and cheese.\", \"image\": \"http://i.imgur.com/ASj60DP.jpg\", \"avatar\": \"http://i.imgur.com/ASj60DP.jpg\", \"room\": \"Crick\" } ] Run! This works because the pre-built style \"Events\" exists in the Light Theme Nuget package and has styles defined that match the data source.(eg. \"title\", \"image\", \"presenter\"). The \"Events\" StyleClass is built to display the ListDataPage control with a custom CardView control that is defined in Xamarin.Forms.Pages. The CardView control has three properties: ImageSource , Text , Detail . The theme is hardcoded to bind the datasource's three fields(from the JSON file) to these properties for display. Customize The inherite style can be overrideden by specifying a template and using data source bindings. The Xaml below declares a custom template for each row using the new ListItemControl and {p:DataSourceBinding} syntax which is includeed in the Xamarin.Forms.Pages Nuget: <p:ListDataPage.DefaultItemTemplate> <DataTemplate> <ViewCell> <p:ListItemControl Title=\"{p:DataSourceBinding title}\" Detail=\"{p:DataSourceBinding room}\" ImageSource=\"{p:DataSourceBinding image}\" DataSource=\"{Binding Value}\" HeightRequest=\"90\" > </p:ListItemControl> </ViewCell> </DataTemplate> </p:ListDataPage.DefaultItemTemplate> By providing a DataTemplate this code overrides the StyleClass and instead uses the default layout for a ListItemControl . Developers that prefer C# to XAML can create data source bindings too (remember to include a using Xamarin.Form.Pages; statement) SetBinding (TitleProperty, new DataSourceBinding (\"title\")); More information at here .","title":"Get Started"},{"location":"knowledge/mobile-development/xamarin/triggers/","text":"Xamarin.Forms Trigger Triggers allow yu to express actionsn declaratively in XAML that change the appearance of controls based on events or property changes. You can assign a trigger directly to a control, or add it to a page-level or app-level resource dictionary to be appied to multiple controls. There are four types of trigger: Xamarin.Forms Trigger Property Triggers Applying a Trigger using a Style Data Triggers Event Triggers Multi Triggers Building a \"require all\" multi trigger EnterActions and ExitActions Property Triggers A simple trigger can be expresses purely in XAML, adding a Trigger element to control's trigger collection. This example shows a trigger that changes an Entry background color whenit receives forcus: <Entry Placeholder=\"enter name\"> <Entry.Triggers> <Trigger TargetType=\"Entry\" Property=\"IsFocused\" Value=\"True\"> <Setter Property=\"BackgroundColor\" Value=\"Yellow\" /> </Trigger> </Entry.Triggers> </Entry> The import parts of the trigger's declaration are: TargetType - the control type that th trigger applies to. Property - the property on the control that is monitored. Value - the value, when it occurs for the monitored property, that causes the trigger to activate. Setter - a collection of Setter elements cab be added and when the trigger condition is met. You must specify the Property and Value to set. ExterActions and ExitAction (not shown) - are written in code and can be used addition to (or instead of) Setter elements. They are described blow . Applying a Trigger using a Style Triggers can also be added to a Style declaration on a control, in a page, pr an applocation ResourceDictionary . This example declares an implicit style (ie. no Key is set) which means it will apply to all Entry controls on the page. <ContentPage.Resources> <ResourceDictionary> <Style TargetType=\"Entry\"> <Style.Triggers> <Trigger TargetType=\"Entry\" Property=\"IsFocused\" Value=\"True\"> <Setter Property=\"BackgroundColor\" Value=\"Yellow\" /> </Trigger> </Style.Triggers> </Style> </ResourceDictionary> </ContentPage.Resources> Data Triggers Data triggers use data binding to monitor another to cause the Setters to get called. Instead of the Property attribute in property trigger, set the Binding attribute to monitoe for the specified value. The example below uses the data binding syntax {Binding Source={x:Reference entry}, Path=Text.length} which is how we refer to another control's properties. When the length of the entry is zero, the trigger is activated. In this sample the trigger disables the button whrn the input is empty. <!-- the x:Name is referenced below in DataTrigger--> <!-- tip: make sure to set the Text=\"\" (or some other default) --> <Entry x:Name=\"entry\" Text=\"\" Placeholder=\"required field\" /> <Button x:Name=\"button\" Text=\"Save\" FontSize=\"Large\" HorizontalOptions=\"Center\"> <Button.Triggers> <DataTrigger TargetType=\"Button\" Binding=\"{Binding Source={x:Reference entry}, Path=Text.Length}\" Value=\"0\"> <Setter Property=\"IsEnabled\" Value=\"False\" /> </DataTrigger> </Button.Triggers> </Button> Tip: when avaluating Path=Text.Lenth always provide a default value for the target property (eg. Text=\"\" because otherwise it will be null and the trigger won't work like you expect.) In addition to specifying Setter s you can also provide EnterActions and ExitActions Event Triggers The EventTrigger element requires only an Entry property, such as \"Clicked\" ing the example below. <EventTrigger Event=\"Clicked\"> <local:NumericValidationTriggerAction /> </EventTrigger> Notice that there no Setter elements but rather a reference to a class defined by local:NumbericValidationTriggerAction which requires the xmlns:local to be declared in the page's XAML: <ContentPage xmlns=\"http://xamarin.com/schemas/2014/forms\" xmlns:x=\"http://schemas.microsoft.com/winfx/2009/xaml\" xmlns:local=\"clr-namespace:WorkingWithTriggers;assembly=WorkingWithTriggers\" The class itself implements TriggerAction which means it should provide an override for the Invoke method that is called whenver the trigger event occurs. a Trigger action implementation should: Implement the generic TriggerAction<T> class, with the generic parameter corresponding with the type of control the trigger will be applied to. You can use supperclasses such as VisualElement to write trigger actions that work with a variety of controls, or specify a control type like Entry . Override the Invoke method - this is called whenever the trigger criteria are met. Optionally expose properties that can be set in the XAML when the trigger is declared (such as Anchor , Scale , and Length in this example). public class NumericValidationTriggerAction : TriggerAction<Entry> { protected override void Invoke (Entry entry) { double result; bool isValid = Double.TryParse (entry.Text, out result); entry.TextColor = isValid ? Color.Default : Color.Red; } } The properties exposed by the trigger action can be set in the XAML declaration as follows: <EventTrigger Event=\"TextChanged\"> <local:NumericValidationTriggerAction /> </EventTrigger> Be careful whrn sharing triggers in a ResourceDictionay , one instance will be shared among controls so any state that is configured once will apply to them all. Note that event triggers do not support Enteractions and ExitActions decribed below . Multi Triggers A MultiTrigger looks similar to a Trigger or DataTrigger except there can be more than one condition. All the conditions must be true before the Setter s are trigged. Here's an example of a trigger for a button that binds to two different inputs ( email and phone ): <MultiTrigger TargetType=\"Button\"> <MultiTrigger.Conditions> <BindingCondition Binding=\"{Binding Source={x:Reference email}, Path=Text.Length}\" Value=\"0\" /> <BindingCondition Binding=\"{Binding Source={x:Reference phone}, Path=Text.Length}\" Value=\"0\" /> </MultiTrigger.Conditions> <Setter Property=\"IsEnabled\" Value=\"False\" /> <!-- multiple Setter elements are allowed --> </MultiTrigger> The Conditions collection could also contain PropertyCondition elements like this: <PropertyCondition Property=\"Text\" Value=\"OK\" /> Building a \"require all\" multi trigger The multi trigger only updates its control when all conditions are true. Testing for \"all field lengths are zero\" (such as a login page where all inputs must be complete) is tricky because you want a condition \"where Text.Length >0 \" but this can't be expressed in XAML. This can be done with an IValueConverter . The converter code below trnsforms the Text.Length binding into a bool that indicates whether a field is empty or not: public class MultiTriggerConverter : IValueConverter { public object Convert(object value, Type targetType, object parameter, CultureInfo culture) { if ((int)value > 0) // length > 0 ? return true; // some data has been entered else return false; // input is empty } public object ConvertBack(object value, Type targetType, object parameter, CultureInfo culture) { throw new NotSupportedException (); } } To use this converter in a multi trigger, first add it to the page's resource dictionary (along with a custom xmls:local namespace definition): <ResourceDictionary> <local:MultiTriggerConverter x:Key=\"dataHasBeenEntered\" /> </ResourceDictionary> The XAML is shown below. Note the following differences from the first multi trigger example: The button has IsEnable=\"false\" set by default. The multi trigger conditions use the converter to turn the Text.Length value into a booleam . When all the conditions are true , the setter makes the button's IsEnabeld property true . <Entry x:Name=\"user\" Text=\"\" Placeholder=\"user name\" /> <Entry x:Name=\"pwd\" Text=\"\" Placeholder=\"password\" /> <Button x:Name=\"loginButton\" Text=\"Login\" FontSize=\"Large\" HorizontalOptions=\"Center\" IsEnabled=\"false\"> <Button.Triggers> <MultiTrigger TargetType=\"Button\"> <MultiTrigger.Conditions> <BindingCondition Binding=\"{Binding Source={x:Reference user}, Path=Text.Length, Converter={StaticResource dataHasBeenEntered}}\" Value=\"true\" /> <BindingCondition Binding=\"{Binding Source={x:Reference pwd}, Path=Text.Length, Converter={StaticResource dataHasBeenEntered}}\" Value=\"true\" /> </MultiTrigger.Conditions> <Setter Property=\"IsEnabled\" Value=\"True\" /> </MultiTrigger> </Button.Triggers> </Button> These screenshots show the difference between the two multi trigger examples above. In the top part of the creens, text input in just one Entry is enough to enable the S.ave button. In the bottom part of the screens, the Login button remains inactive until both fields contain data. EnterActions and ExitActions Another way to implement changes when a trigger occurs is by adding EnterActions and ExitActions collections and specifying TriggerAction<T> implementations. You can provide both EnterActions and ExitActions as well as Setter s in a trigger, but be aware that the Setter are called immediately (they do not wait for the EnterAction or ExitAction to complete). Alternatively you can perform everything in the code and not use Setter s at all. <Entry Placeholder=\"enter job title\"> <Entry.Triggers> <Trigger TargetType=\"Entry\" Property=\"Entry.IsFocused\" Value=\"True\"> <Trigger.EnterActions> <local:FadeTriggerAction StartsFrom=\"0\"\" /> </Trigger.EnterActions> <Trigger.ExitActions> <local:FadeTriggerAction StartsFrom=\"1\" /> </Trigger.ExitActions> <!-- You can use both Enter/Exit and Setter together if required --> </Trigger> </Entry.Triggers> </Entry> As alaws, when a class is referenced in XAML you should declare a namespace such as xmlns:local as shown here: <ContentPage xmlns=\"http://xamarin.com/schemas/2014/forms\" xmlns:x=\"http://schemas.microsoft.com/winfx/2009/xaml\" xmlns:local=\"clr-namespace:WorkingWithTriggers;assembly=WorkingWithTriggers\" The FadetriggerAction code is shown below: public class FadeTriggerAction : TriggerAction<VisualElement> { public FadeTriggerAction() {} public int StartsFrom { set; get; } protected override void Invoke (VisualElement visual) { visual.Animate(\"\", new Animation( (d)=>{ var val = StartsFrom==1 ? d : 1-d; visual.BackgroundColor = Color.FromRgb(1, val, 1); }), length:1000, // milliseconds easing: Easing.Linear); } } Note: EnterActions and ExitActions are ignored on Envent Triggers . Get more information at here","title":"Triggers"},{"location":"knowledge/mobile-development/xamarin/triggers/#xamarinforms-trigger","text":"Triggers allow yu to express actionsn declaratively in XAML that change the appearance of controls based on events or property changes. You can assign a trigger directly to a control, or add it to a page-level or app-level resource dictionary to be appied to multiple controls. There are four types of trigger: Xamarin.Forms Trigger Property Triggers Applying a Trigger using a Style Data Triggers Event Triggers Multi Triggers Building a \"require all\" multi trigger EnterActions and ExitActions","title":"Xamarin.Forms Trigger"},{"location":"knowledge/mobile-development/xamarin/triggers/#property-triggers","text":"A simple trigger can be expresses purely in XAML, adding a Trigger element to control's trigger collection. This example shows a trigger that changes an Entry background color whenit receives forcus: <Entry Placeholder=\"enter name\"> <Entry.Triggers> <Trigger TargetType=\"Entry\" Property=\"IsFocused\" Value=\"True\"> <Setter Property=\"BackgroundColor\" Value=\"Yellow\" /> </Trigger> </Entry.Triggers> </Entry> The import parts of the trigger's declaration are: TargetType - the control type that th trigger applies to. Property - the property on the control that is monitored. Value - the value, when it occurs for the monitored property, that causes the trigger to activate. Setter - a collection of Setter elements cab be added and when the trigger condition is met. You must specify the Property and Value to set. ExterActions and ExitAction (not shown) - are written in code and can be used addition to (or instead of) Setter elements. They are described blow .","title":"Property Triggers"},{"location":"knowledge/mobile-development/xamarin/triggers/#applying-a-trigger-using-a-style","text":"Triggers can also be added to a Style declaration on a control, in a page, pr an applocation ResourceDictionary . This example declares an implicit style (ie. no Key is set) which means it will apply to all Entry controls on the page. <ContentPage.Resources> <ResourceDictionary> <Style TargetType=\"Entry\"> <Style.Triggers> <Trigger TargetType=\"Entry\" Property=\"IsFocused\" Value=\"True\"> <Setter Property=\"BackgroundColor\" Value=\"Yellow\" /> </Trigger> </Style.Triggers> </Style> </ResourceDictionary> </ContentPage.Resources>","title":"Applying a Trigger using a Style"},{"location":"knowledge/mobile-development/xamarin/triggers/#data-triggers","text":"Data triggers use data binding to monitor another to cause the Setters to get called. Instead of the Property attribute in property trigger, set the Binding attribute to monitoe for the specified value. The example below uses the data binding syntax {Binding Source={x:Reference entry}, Path=Text.length} which is how we refer to another control's properties. When the length of the entry is zero, the trigger is activated. In this sample the trigger disables the button whrn the input is empty. <!-- the x:Name is referenced below in DataTrigger--> <!-- tip: make sure to set the Text=\"\" (or some other default) --> <Entry x:Name=\"entry\" Text=\"\" Placeholder=\"required field\" /> <Button x:Name=\"button\" Text=\"Save\" FontSize=\"Large\" HorizontalOptions=\"Center\"> <Button.Triggers> <DataTrigger TargetType=\"Button\" Binding=\"{Binding Source={x:Reference entry}, Path=Text.Length}\" Value=\"0\"> <Setter Property=\"IsEnabled\" Value=\"False\" /> </DataTrigger> </Button.Triggers> </Button> Tip: when avaluating Path=Text.Lenth always provide a default value for the target property (eg. Text=\"\" because otherwise it will be null and the trigger won't work like you expect.) In addition to specifying Setter s you can also provide EnterActions and ExitActions","title":"Data Triggers"},{"location":"knowledge/mobile-development/xamarin/triggers/#event-triggers","text":"The EventTrigger element requires only an Entry property, such as \"Clicked\" ing the example below. <EventTrigger Event=\"Clicked\"> <local:NumericValidationTriggerAction /> </EventTrigger> Notice that there no Setter elements but rather a reference to a class defined by local:NumbericValidationTriggerAction which requires the xmlns:local to be declared in the page's XAML: <ContentPage xmlns=\"http://xamarin.com/schemas/2014/forms\" xmlns:x=\"http://schemas.microsoft.com/winfx/2009/xaml\" xmlns:local=\"clr-namespace:WorkingWithTriggers;assembly=WorkingWithTriggers\" The class itself implements TriggerAction which means it should provide an override for the Invoke method that is called whenver the trigger event occurs. a Trigger action implementation should: Implement the generic TriggerAction<T> class, with the generic parameter corresponding with the type of control the trigger will be applied to. You can use supperclasses such as VisualElement to write trigger actions that work with a variety of controls, or specify a control type like Entry . Override the Invoke method - this is called whenever the trigger criteria are met. Optionally expose properties that can be set in the XAML when the trigger is declared (such as Anchor , Scale , and Length in this example). public class NumericValidationTriggerAction : TriggerAction<Entry> { protected override void Invoke (Entry entry) { double result; bool isValid = Double.TryParse (entry.Text, out result); entry.TextColor = isValid ? Color.Default : Color.Red; } } The properties exposed by the trigger action can be set in the XAML declaration as follows: <EventTrigger Event=\"TextChanged\"> <local:NumericValidationTriggerAction /> </EventTrigger> Be careful whrn sharing triggers in a ResourceDictionay , one instance will be shared among controls so any state that is configured once will apply to them all. Note that event triggers do not support Enteractions and ExitActions decribed below .","title":"Event Triggers"},{"location":"knowledge/mobile-development/xamarin/triggers/#multi-triggers","text":"A MultiTrigger looks similar to a Trigger or DataTrigger except there can be more than one condition. All the conditions must be true before the Setter s are trigged. Here's an example of a trigger for a button that binds to two different inputs ( email and phone ): <MultiTrigger TargetType=\"Button\"> <MultiTrigger.Conditions> <BindingCondition Binding=\"{Binding Source={x:Reference email}, Path=Text.Length}\" Value=\"0\" /> <BindingCondition Binding=\"{Binding Source={x:Reference phone}, Path=Text.Length}\" Value=\"0\" /> </MultiTrigger.Conditions> <Setter Property=\"IsEnabled\" Value=\"False\" /> <!-- multiple Setter elements are allowed --> </MultiTrigger> The Conditions collection could also contain PropertyCondition elements like this: <PropertyCondition Property=\"Text\" Value=\"OK\" />","title":"Multi Triggers"},{"location":"knowledge/mobile-development/xamarin/triggers/#building-a-require-all-multi-trigger","text":"The multi trigger only updates its control when all conditions are true. Testing for \"all field lengths are zero\" (such as a login page where all inputs must be complete) is tricky because you want a condition \"where Text.Length >0 \" but this can't be expressed in XAML. This can be done with an IValueConverter . The converter code below trnsforms the Text.Length binding into a bool that indicates whether a field is empty or not: public class MultiTriggerConverter : IValueConverter { public object Convert(object value, Type targetType, object parameter, CultureInfo culture) { if ((int)value > 0) // length > 0 ? return true; // some data has been entered else return false; // input is empty } public object ConvertBack(object value, Type targetType, object parameter, CultureInfo culture) { throw new NotSupportedException (); } } To use this converter in a multi trigger, first add it to the page's resource dictionary (along with a custom xmls:local namespace definition): <ResourceDictionary> <local:MultiTriggerConverter x:Key=\"dataHasBeenEntered\" /> </ResourceDictionary> The XAML is shown below. Note the following differences from the first multi trigger example: The button has IsEnable=\"false\" set by default. The multi trigger conditions use the converter to turn the Text.Length value into a booleam . When all the conditions are true , the setter makes the button's IsEnabeld property true . <Entry x:Name=\"user\" Text=\"\" Placeholder=\"user name\" /> <Entry x:Name=\"pwd\" Text=\"\" Placeholder=\"password\" /> <Button x:Name=\"loginButton\" Text=\"Login\" FontSize=\"Large\" HorizontalOptions=\"Center\" IsEnabled=\"false\"> <Button.Triggers> <MultiTrigger TargetType=\"Button\"> <MultiTrigger.Conditions> <BindingCondition Binding=\"{Binding Source={x:Reference user}, Path=Text.Length, Converter={StaticResource dataHasBeenEntered}}\" Value=\"true\" /> <BindingCondition Binding=\"{Binding Source={x:Reference pwd}, Path=Text.Length, Converter={StaticResource dataHasBeenEntered}}\" Value=\"true\" /> </MultiTrigger.Conditions> <Setter Property=\"IsEnabled\" Value=\"True\" /> </MultiTrigger> </Button.Triggers> </Button> These screenshots show the difference between the two multi trigger examples above. In the top part of the creens, text input in just one Entry is enough to enable the S.ave button. In the bottom part of the screens, the Login button remains inactive until both fields contain data.","title":"Building a \"require all\" multi trigger"},{"location":"knowledge/mobile-development/xamarin/triggers/#enteractions-and-exitactions","text":"Another way to implement changes when a trigger occurs is by adding EnterActions and ExitActions collections and specifying TriggerAction<T> implementations. You can provide both EnterActions and ExitActions as well as Setter s in a trigger, but be aware that the Setter are called immediately (they do not wait for the EnterAction or ExitAction to complete). Alternatively you can perform everything in the code and not use Setter s at all. <Entry Placeholder=\"enter job title\"> <Entry.Triggers> <Trigger TargetType=\"Entry\" Property=\"Entry.IsFocused\" Value=\"True\"> <Trigger.EnterActions> <local:FadeTriggerAction StartsFrom=\"0\"\" /> </Trigger.EnterActions> <Trigger.ExitActions> <local:FadeTriggerAction StartsFrom=\"1\" /> </Trigger.ExitActions> <!-- You can use both Enter/Exit and Setter together if required --> </Trigger> </Entry.Triggers> </Entry> As alaws, when a class is referenced in XAML you should declare a namespace such as xmlns:local as shown here: <ContentPage xmlns=\"http://xamarin.com/schemas/2014/forms\" xmlns:x=\"http://schemas.microsoft.com/winfx/2009/xaml\" xmlns:local=\"clr-namespace:WorkingWithTriggers;assembly=WorkingWithTriggers\" The FadetriggerAction code is shown below: public class FadeTriggerAction : TriggerAction<VisualElement> { public FadeTriggerAction() {} public int StartsFrom { set; get; } protected override void Invoke (VisualElement visual) { visual.Animate(\"\", new Animation( (d)=>{ var val = StartsFrom==1 ? d : 1-d; visual.BackgroundColor = Color.FromRgb(1, val, 1); }), length:1000, // milliseconds easing: Easing.Linear); } } Note: EnterActions and ExitActions are ignored on Envent Triggers . Get more information at here","title":"EnterActions and ExitActions"},{"location":"knowledge/mobile-development/xamarin/xamarin-forms-shell/","text":"Xamarin.Forms Shell Overview Xamarin.Forms Shell reduces the complexity of mobile application development by providing the fundamental features that most mobile application require, including: A single place to describe the visual hierarchy of an application. A common navigation user experience A URI-based navigation scheme that permits navigation to any page in the application. An integrated search handler. In addition,Shell applications benefit from an increased rendering speed, and reduced memory consumption. Shell navigation experience Shell provides an opinionated navigation experience, based on flyouts and tabs. The top level of navigation in a Shell application is either a flyout or a bottom tab bar, depending on the navigation requirements of the application. Selecting a flyout item results in the bottom tab that represents the item being selected and displayed: Note: when the flyour isn't open the bottom tab bar can be considered to be the top level of navigation in the application. Each tab displays a ContentPage . However, if a bottom tab contains more than one page, the pages are navigable by the top tab bar: Within each tab, additional ContentPage objects can be navigated to: Get more information at here .","title":"Xamarin Forms Shell"},{"location":"knowledge/mobile-development/xamarin/xamarin-forms-shell/#xamarinforms-shell","text":"","title":"Xamarin.Forms Shell"},{"location":"knowledge/mobile-development/xamarin/xamarin-forms-shell/#overview","text":"Xamarin.Forms Shell reduces the complexity of mobile application development by providing the fundamental features that most mobile application require, including: A single place to describe the visual hierarchy of an application. A common navigation user experience A URI-based navigation scheme that permits navigation to any page in the application. An integrated search handler. In addition,Shell applications benefit from an increased rendering speed, and reduced memory consumption.","title":"Overview"},{"location":"knowledge/mobile-development/xamarin/xamarin-forms-shell/#shell-navigation-experience","text":"Shell provides an opinionated navigation experience, based on flyouts and tabs. The top level of navigation in a Shell application is either a flyout or a bottom tab bar, depending on the navigation requirements of the application. Selecting a flyout item results in the bottom tab that represents the item being selected and displayed: Note: when the flyour isn't open the bottom tab bar can be considered to be the top level of navigation in the application. Each tab displays a ContentPage . However, if a bottom tab contains more than one page, the pages are navigable by the top tab bar: Within each tab, additional ContentPage objects can be navigated to: Get more information at here .","title":"Shell navigation experience"},{"location":"knowledge/mobile-development/xamarin/xamarin-forms-theme/","text":"Xamarin.Forms Themes StyleClass The StypeClass property allows a view's appearance to be changed according to a definition provided by a theme. The Light and Dark themes both define three different appearances for a BoxView: horizontalRul, Circel, and Rounded. This markup show three different BoxViews with different style classes applied: <StackLayout Padding=\"40\"> <BoxView StyleClass=\"HorizontalRule\" /> <BoxView StyleClass=\"Circle\" /> <BoxView StyleClass=\"Rounded\" /> </StackLayout> Built-in Classes In addition to automatically styling the common controls the light and dark themes currently support the following classes that can be applied by setting the StyleClass on these controls. BoxView HorizontalRule Circle Rounded Image Circle Rounded Thumbnail Button Default Primary Success Info Warning Danger Link Small Large Label Header Subheader Body Link Inverse Troubleshooting Could not load file or assembly 'Xamarin.Form.Theme.Light' or one of it;s dependencies In the preview release, themes may not be able to load at runtime. Add the code shown below into the relevant project to fix this error. iOS In the AppDelegate.cs add the following lines after LoadApplication var x = typeof(Xamarin.Forms.Themes.DarkThemeResources); x = typeof(Xamarin.Forms.Themes.LightThemeResources); x = typeof(Xamarin.Forms.Themes.iOS.UnderlineEffect); Android In the MainActivity.cs add the following lines after LoadApplication var x = typeof(Xamarin.Forms.Themes.DarkThemeResources); x = typeof(Xamarin.Forms.Themes.LightThemeResources); x = typeof(Xamarin.Forms.Themes.Android.UnderlineEffect); Xamarin.Forms Light Theme Add Nuget packages Xamarin.Forms.Theme.Base Xamarin.Forms.Theme.Light Add to the Resource Dictionary In the App.xaml file add a new custom xmlns for the theme, and then ensure the theme's resources are merged with the application's resource dictionary. An example XAML file is shown below: <?xml version=\"1.0\" encoding=\"utf-8\"?> <Application xmlns=\"http://xamarin.com/schemas/2014/forms\" xmlns:x=\"http://schemas.microsoft.com/winfx/2009/xaml\" x:Class=\"EvolveApp.App\" xmlns:light=\"clr-namespace:Xamarin.Forms.Themes;assembly=Xamarin.Forms.Theme.Light\"> <Application.Resources> <ResourceDictionary MergedWith=\"light:LightThemeResources\" /> </Application.Resources> </Application> Load theme classes Follow troubleshooting step. Use StyleClass <StackLayout Padding=\"20\"> <Button Text=\"Button Default\" /> <Button Text=\"Button Class Default\" StyleClass=\"Default\" /> <Button Text=\"Button Class Primary\" StyleClass=\"Primary\" /> <Button Text=\"Button Class Success\" StyleClass=\"Success\" /> <Button Text=\"Button Class Info\" StyleClass=\"Info\" /> <Button Text=\"Button Class Warning\" StyleClass=\"Warning\" /> <Button Text=\"Button Class Danger\" StyleClass=\"Danger\" /> <Button Text=\"Button Class Link\" StyleClass=\"Link\" /> <Button Text=\"Button Class Default Small\" StyleClass=\"Small\" /> <Button Text=\"Button Class Default Large\" StyleClass=\"Large\" /> </StackLayout> Creating a Custom Xamarin.Forms Theme Note the Class attribute for Style (as opposed to the x:Key attribute available in earlier version of Xamarin.Forms) <ResourceDictionary> <!-- DEFINE ANY CONSTANTS --> <Color x:Key=\"SeparatorLineColor\">#CCCCCC</Color> <Color x:Key=\"iOSDefaultTintColor\">#007aff</Color> <Color x:Key=\"AndroidDefaultAccentColorColor\">#1FAECE</Color> <OnPlatform x:TypeArguments=\"Color\" x:Key=\"AccentColor\"> <On Platform=\"iOS\" Value=\"{StaticResource iOSDefaultTintColor}\" /> <On Platform=\"Android\" Value=\"{StaticResource AndroidDefaultAccentColorColor}\" /> </OnPlatform> <!-- BOXVIEW CLASSES --> <Style TargetType=\"BoxView\" Class=\"HorizontalRule\"> <Setter Property=\"BackgroundColor\" Value=\"{ StaticResource SeparatorLineColor }\" /> <Setter Property=\"HeightRequest\" Value=\"1\" /> </Style> <Style TargetType=\"BoxView\" Class=\"Circle\"> <Setter Property=\"BackgroundColor\" Value=\"{ StaticResource AccentColor }\" /> <Setter Property=\"WidthRequest\" Value=\"34\"/> <Setter Property=\"HeightRequest\" Value=\"34\"/> <Setter Property=\"HorizontalOptions\" Value=\"Start\" /> <Setter Property=\"local:ThemeEffects.Circle\" Value=\"True\" /> </Style> <Style TargetType=\"BoxView\" Class=\"Rounded\"> <Setter Property=\"BackgroundColor\" Value=\"{ StaticResource AccentColor }\" /> <Setter Property=\"HorizontalOptions\" Value=\"Start\" /> <Setter Property=\"BackgroundColor\" Value=\"{ StaticResource AccentColor }\" /> <Setter Property=\"local:ThemeEffects.CornerRadius\" Value=\"4\" /> </Style> </ResourceDictionary> You'll notice that the Rounded class refers to a custom effext CornerRadius. The code for this effect is given below - to reference it correctly a custom xmlns must be added to the App.xaml's root element: xmlns:local=\"clr-namespace:ThemesDemo;assembly=ThemesDemo\" C# code in the .NET Standard library project or Shared Project The code for creating a round-corner BoxView use effects. The corner radius is applied using a BindableProperty and is implemented by applying an effect. The effect requires platform-specific code in the iOS and Android projects(shown below). namespace ThemesDemo { public static class ThemeEffects { public static readonly BindableProperty CornerRadiusProperty = BindableProperty.CreateAttached(\"CornerRadius\", typeof(double), typeof(ThemeEffects), 0.0, propertyChanged: OnChanged<CornerRadiusEffect, double>); private static void OnChanged<TEffect, TProp>(BindableObject bindable, object oldValue, object newValue) where TEffect : Effect, new() { if (!(bindable is View view)) { return; } if (EqualityComparer<TProp>.Equals(newValue, default(TProp))) { var toRemove = view.Effects.FirstOrDefault(e => e is TEffect); if (toRemove != null) { view.Effects.Remove(toRemove); } } else { view.Effects.Add(new TEffect()); } } public static void SetCornerRadius(BindableObject view, double radius) { view.SetValue(CornerRadiusProperty, radius); } public static double GetCornerRadius(BindableObject view) { return (double)view.GetValue(CornerRadiusProperty); } private class CornerRadiusEffect : RoutingEffect { public CornerRadiusEffect() : base(\"Xamarin.CornerRadiusEffect\") { } } } } C# code in the iOS project using System; using Xamarin.Forms; using Xamarin.Forms.Platform.iOS; using CoreGraphics; using Foundation; using XFThemes; namespace ThemesDemo.iOS { public class CornerRadiusEffect : PlatformEffect { private nfloat _originalRadius; protected override void OnAttached() { if (Container != null) { _originalRadius = Container.Layer.CornerRadius; Container.ClipsToBounds = true; UpdateCorner(); } } protected override void OnDetached() { if (Container != null) { Container.Layer.CornerRadius = _originalRadius; Container.ClipsToBounds = false; } } protected override void OnElementPropertyChanged(System.ComponentModel.PropertyChangedEventArgs args) { base.OnElementPropertyChanged(args); if (args.PropertyName == ThemeEffects.CornerRadiusProperty.PropertyName) { UpdateCorner(); } } private void UpdateCorner() { Container.Layer.CornerRadius = (nfloat)ThemeEffects.GetCornerRadius(Element); } } } C# code in the Android project using System; using Xamarin.Forms.Platform; using Xamarin.Forms.Platform.Android; using Android.Views; using Android.Graphics; namespace ThemesDemo.Droid { public class CornerRadiusEffect : BaseEffect { private ViewOutlineProvider _originalProvider; protected override bool CanBeApplied() { return Container != null && Android.OS.Build.VERSION.SdkInt >= Android.OS.BuildVersionCodes.Lollipop; } protected override void OnAttachedInternal() { _originalProvider = Container.OutlineProvider; Container.OutlineProvider = new CornerRadiusOutlineProvider(Element); Container.ClipToOutline = true; } protected override void OnDetachedInternal() { Container.OutlineProvider = _originalProvider; Container.ClipToOutline = false; } protected override void OnElementPropertyChanged(System.ComponentModel.PropertyChangedEventArgs args) { base.OnElementPropertyChanged(args); if (!Attached) { return; } if (args.PropertyName == ThemeEffects.CornerRadiusProperty.PropertyName) { Container.Invalidate(); } } private class CornerRadiusOutlineProvider : ViewOutlineProvider { private Xamarin.Forms.Element _element; public CornerRadiusOutlineProvider(Xamarin.Forms.Element element) { _element = element; } public override void GetOutline(Android.Views.View view, Outline outline) { var pixels = (float)ThemeEffects.GetCornerRadius(_element) * view.Resources.DisplayMetrics.Density; outline.SetRoundRect(new Rect(0, 0, view.Width, view.Height), (int)pixels); } } } } Summary A custom theme can be created by defining styles for each control that requires custom appearance. Multiple styles for a control should be distinguished by different Class attributes in the resource dictionary, and then applied the StyleClass attribute on the control. Get more information at here .","title":"Themes"},{"location":"knowledge/mobile-development/xamarin/xamarin-forms-theme/#xamarinforms-themes","text":"","title":"Xamarin.Forms Themes"},{"location":"knowledge/mobile-development/xamarin/xamarin-forms-theme/#styleclass","text":"The StypeClass property allows a view's appearance to be changed according to a definition provided by a theme. The Light and Dark themes both define three different appearances for a BoxView: horizontalRul, Circel, and Rounded. This markup show three different BoxViews with different style classes applied: <StackLayout Padding=\"40\"> <BoxView StyleClass=\"HorizontalRule\" /> <BoxView StyleClass=\"Circle\" /> <BoxView StyleClass=\"Rounded\" /> </StackLayout>","title":"StyleClass"},{"location":"knowledge/mobile-development/xamarin/xamarin-forms-theme/#built-in-classes","text":"In addition to automatically styling the common controls the light and dark themes currently support the following classes that can be applied by setting the StyleClass on these controls.","title":"Built-in Classes"},{"location":"knowledge/mobile-development/xamarin/xamarin-forms-theme/#boxview","text":"HorizontalRule Circle Rounded","title":"BoxView"},{"location":"knowledge/mobile-development/xamarin/xamarin-forms-theme/#image","text":"Circle Rounded Thumbnail","title":"Image"},{"location":"knowledge/mobile-development/xamarin/xamarin-forms-theme/#button","text":"Default Primary Success Info Warning Danger Link Small Large","title":"Button"},{"location":"knowledge/mobile-development/xamarin/xamarin-forms-theme/#label","text":"Header Subheader Body Link Inverse","title":"Label"},{"location":"knowledge/mobile-development/xamarin/xamarin-forms-theme/#troubleshooting","text":"Could not load file or assembly 'Xamarin.Form.Theme.Light' or one of it;s dependencies In the preview release, themes may not be able to load at runtime. Add the code shown below into the relevant project to fix this error. iOS In the AppDelegate.cs add the following lines after LoadApplication var x = typeof(Xamarin.Forms.Themes.DarkThemeResources); x = typeof(Xamarin.Forms.Themes.LightThemeResources); x = typeof(Xamarin.Forms.Themes.iOS.UnderlineEffect); Android In the MainActivity.cs add the following lines after LoadApplication var x = typeof(Xamarin.Forms.Themes.DarkThemeResources); x = typeof(Xamarin.Forms.Themes.LightThemeResources); x = typeof(Xamarin.Forms.Themes.Android.UnderlineEffect);","title":"Troubleshooting"},{"location":"knowledge/mobile-development/xamarin/xamarin-forms-theme/#xamarinforms-light-theme","text":"Add Nuget packages Xamarin.Forms.Theme.Base Xamarin.Forms.Theme.Light Add to the Resource Dictionary In the App.xaml file add a new custom xmlns for the theme, and then ensure the theme's resources are merged with the application's resource dictionary. An example XAML file is shown below: <?xml version=\"1.0\" encoding=\"utf-8\"?> <Application xmlns=\"http://xamarin.com/schemas/2014/forms\" xmlns:x=\"http://schemas.microsoft.com/winfx/2009/xaml\" x:Class=\"EvolveApp.App\" xmlns:light=\"clr-namespace:Xamarin.Forms.Themes;assembly=Xamarin.Forms.Theme.Light\"> <Application.Resources> <ResourceDictionary MergedWith=\"light:LightThemeResources\" /> </Application.Resources> </Application> Load theme classes Follow troubleshooting step. Use StyleClass <StackLayout Padding=\"20\"> <Button Text=\"Button Default\" /> <Button Text=\"Button Class Default\" StyleClass=\"Default\" /> <Button Text=\"Button Class Primary\" StyleClass=\"Primary\" /> <Button Text=\"Button Class Success\" StyleClass=\"Success\" /> <Button Text=\"Button Class Info\" StyleClass=\"Info\" /> <Button Text=\"Button Class Warning\" StyleClass=\"Warning\" /> <Button Text=\"Button Class Danger\" StyleClass=\"Danger\" /> <Button Text=\"Button Class Link\" StyleClass=\"Link\" /> <Button Text=\"Button Class Default Small\" StyleClass=\"Small\" /> <Button Text=\"Button Class Default Large\" StyleClass=\"Large\" /> </StackLayout>","title":"Xamarin.Forms Light Theme"},{"location":"knowledge/mobile-development/xamarin/xamarin-forms-theme/#creating-a-custom-xamarinforms-theme","text":"Note the Class attribute for Style (as opposed to the x:Key attribute available in earlier version of Xamarin.Forms) <ResourceDictionary> <!-- DEFINE ANY CONSTANTS --> <Color x:Key=\"SeparatorLineColor\">#CCCCCC</Color> <Color x:Key=\"iOSDefaultTintColor\">#007aff</Color> <Color x:Key=\"AndroidDefaultAccentColorColor\">#1FAECE</Color> <OnPlatform x:TypeArguments=\"Color\" x:Key=\"AccentColor\"> <On Platform=\"iOS\" Value=\"{StaticResource iOSDefaultTintColor}\" /> <On Platform=\"Android\" Value=\"{StaticResource AndroidDefaultAccentColorColor}\" /> </OnPlatform> <!-- BOXVIEW CLASSES --> <Style TargetType=\"BoxView\" Class=\"HorizontalRule\"> <Setter Property=\"BackgroundColor\" Value=\"{ StaticResource SeparatorLineColor }\" /> <Setter Property=\"HeightRequest\" Value=\"1\" /> </Style> <Style TargetType=\"BoxView\" Class=\"Circle\"> <Setter Property=\"BackgroundColor\" Value=\"{ StaticResource AccentColor }\" /> <Setter Property=\"WidthRequest\" Value=\"34\"/> <Setter Property=\"HeightRequest\" Value=\"34\"/> <Setter Property=\"HorizontalOptions\" Value=\"Start\" /> <Setter Property=\"local:ThemeEffects.Circle\" Value=\"True\" /> </Style> <Style TargetType=\"BoxView\" Class=\"Rounded\"> <Setter Property=\"BackgroundColor\" Value=\"{ StaticResource AccentColor }\" /> <Setter Property=\"HorizontalOptions\" Value=\"Start\" /> <Setter Property=\"BackgroundColor\" Value=\"{ StaticResource AccentColor }\" /> <Setter Property=\"local:ThemeEffects.CornerRadius\" Value=\"4\" /> </Style> </ResourceDictionary> You'll notice that the Rounded class refers to a custom effext CornerRadius. The code for this effect is given below - to reference it correctly a custom xmlns must be added to the App.xaml's root element: xmlns:local=\"clr-namespace:ThemesDemo;assembly=ThemesDemo\"","title":"Creating a Custom Xamarin.Forms Theme"},{"location":"knowledge/mobile-development/xamarin/xamarin-forms-theme/#c-code-in-the-net-standard-library-project-or-shared-project","text":"The code for creating a round-corner BoxView use effects. The corner radius is applied using a BindableProperty and is implemented by applying an effect. The effect requires platform-specific code in the iOS and Android projects(shown below). namespace ThemesDemo { public static class ThemeEffects { public static readonly BindableProperty CornerRadiusProperty = BindableProperty.CreateAttached(\"CornerRadius\", typeof(double), typeof(ThemeEffects), 0.0, propertyChanged: OnChanged<CornerRadiusEffect, double>); private static void OnChanged<TEffect, TProp>(BindableObject bindable, object oldValue, object newValue) where TEffect : Effect, new() { if (!(bindable is View view)) { return; } if (EqualityComparer<TProp>.Equals(newValue, default(TProp))) { var toRemove = view.Effects.FirstOrDefault(e => e is TEffect); if (toRemove != null) { view.Effects.Remove(toRemove); } } else { view.Effects.Add(new TEffect()); } } public static void SetCornerRadius(BindableObject view, double radius) { view.SetValue(CornerRadiusProperty, radius); } public static double GetCornerRadius(BindableObject view) { return (double)view.GetValue(CornerRadiusProperty); } private class CornerRadiusEffect : RoutingEffect { public CornerRadiusEffect() : base(\"Xamarin.CornerRadiusEffect\") { } } } }","title":"C# code in the .NET Standard library project or Shared Project"},{"location":"knowledge/mobile-development/xamarin/xamarin-forms-theme/#c-code-in-the-ios-project","text":"using System; using Xamarin.Forms; using Xamarin.Forms.Platform.iOS; using CoreGraphics; using Foundation; using XFThemes; namespace ThemesDemo.iOS { public class CornerRadiusEffect : PlatformEffect { private nfloat _originalRadius; protected override void OnAttached() { if (Container != null) { _originalRadius = Container.Layer.CornerRadius; Container.ClipsToBounds = true; UpdateCorner(); } } protected override void OnDetached() { if (Container != null) { Container.Layer.CornerRadius = _originalRadius; Container.ClipsToBounds = false; } } protected override void OnElementPropertyChanged(System.ComponentModel.PropertyChangedEventArgs args) { base.OnElementPropertyChanged(args); if (args.PropertyName == ThemeEffects.CornerRadiusProperty.PropertyName) { UpdateCorner(); } } private void UpdateCorner() { Container.Layer.CornerRadius = (nfloat)ThemeEffects.GetCornerRadius(Element); } } }","title":"C# code in the iOS project"},{"location":"knowledge/mobile-development/xamarin/xamarin-forms-theme/#c-code-in-the-android-project","text":"using System; using Xamarin.Forms.Platform; using Xamarin.Forms.Platform.Android; using Android.Views; using Android.Graphics; namespace ThemesDemo.Droid { public class CornerRadiusEffect : BaseEffect { private ViewOutlineProvider _originalProvider; protected override bool CanBeApplied() { return Container != null && Android.OS.Build.VERSION.SdkInt >= Android.OS.BuildVersionCodes.Lollipop; } protected override void OnAttachedInternal() { _originalProvider = Container.OutlineProvider; Container.OutlineProvider = new CornerRadiusOutlineProvider(Element); Container.ClipToOutline = true; } protected override void OnDetachedInternal() { Container.OutlineProvider = _originalProvider; Container.ClipToOutline = false; } protected override void OnElementPropertyChanged(System.ComponentModel.PropertyChangedEventArgs args) { base.OnElementPropertyChanged(args); if (!Attached) { return; } if (args.PropertyName == ThemeEffects.CornerRadiusProperty.PropertyName) { Container.Invalidate(); } } private class CornerRadiusOutlineProvider : ViewOutlineProvider { private Xamarin.Forms.Element _element; public CornerRadiusOutlineProvider(Xamarin.Forms.Element element) { _element = element; } public override void GetOutline(Android.Views.View view, Outline outline) { var pixels = (float)ThemeEffects.GetCornerRadius(_element) * view.Resources.DisplayMetrics.Density; outline.SetRoundRect(new Rect(0, 0, view.Width, view.Height), (int)pixels); } } } }","title":"C# code in the Android project"},{"location":"knowledge/mobile-development/xamarin/xamarin-forms-theme/#summary","text":"A custom theme can be created by defining styles for each control that requires custom appearance. Multiple styles for a control should be distinguished by different Class attributes in the resource dictionary, and then applied the StyleClass attribute on the control. Get more information at here .","title":"Summary"},{"location":"knowledge/mobile-development/xamarin/skiasharp/draw-basics/","text":"Draw Basics The SkCanvasView occupies the entire content area of the page. You can alternatively combine an SKCanvasView with other Xamarin.Forms View derivatives, as you'll see in other examples. The PaintSurface event handler is where you do all your drawing. This method can be called multiple times while your program is running, so it should maintain all the information necessary to recreate the praphics display: void OnCanvasViewPaintSurface(object sender, SKPaintSurfaceEventArgs args) { ... } The SkPaintSurfaceEventArgs object that accompanies the event has two properties: Info of type SKImageInfo . Surfae of type SKSurface . The SKImageInfo structure contains information about the drawing surface, most importantly, its width and height in pixels. The SKSurface object represents the drawing surface itself. The most important property of SKSurface is Canvas of type SKCanvas . This class is graphics drawing context that you use to perform the actual drawing. The SKCanvas object encapsulates a graphics state, which includes graphics transforms and clipping. To specify the color and other characteristic of the line, you create and initialize an SKPaint object: void OnCanvasViewPaintSurface(object sender, SKPaintSurfaceEventArgs args) { ... SKPaint paint = new SKPaint { Style = SKPaintStyle.Stroke, Color = Color.Red.ToSKColor(), StrokeWidth = 25 }; ... } The Style prperty indicates that you want to stroke a line(in this case the outline of the circle) rather than fill the interior. The three members of the SKPaintStyle enumeration are as follows: Fill Stroke StrokeAndFill The default is Fill . Use the third option to stroke the line and fill the interior with the same color. Set the Color property to a value of type SKColor . One way to get an SKColor value is by converting a Xamarin.Forms Color value to an SKColor value using the extension method ToSKColor . The Extensions class in the SkiaSharp.Views.Forms namespace includes other methods that convert between Xamarin.Forms values and SkiaSharp values. The StrokeWidth property indicates the thickness of the line. Here it's set to 25 pixels. Get more information at here .","title":"Draw Basics"},{"location":"knowledge/mobile-development/xamarin/skiasharp/draw-basics/#draw-basics","text":"The SkCanvasView occupies the entire content area of the page. You can alternatively combine an SKCanvasView with other Xamarin.Forms View derivatives, as you'll see in other examples. The PaintSurface event handler is where you do all your drawing. This method can be called multiple times while your program is running, so it should maintain all the information necessary to recreate the praphics display: void OnCanvasViewPaintSurface(object sender, SKPaintSurfaceEventArgs args) { ... } The SkPaintSurfaceEventArgs object that accompanies the event has two properties: Info of type SKImageInfo . Surfae of type SKSurface . The SKImageInfo structure contains information about the drawing surface, most importantly, its width and height in pixels. The SKSurface object represents the drawing surface itself. The most important property of SKSurface is Canvas of type SKCanvas . This class is graphics drawing context that you use to perform the actual drawing. The SKCanvas object encapsulates a graphics state, which includes graphics transforms and clipping. To specify the color and other characteristic of the line, you create and initialize an SKPaint object: void OnCanvasViewPaintSurface(object sender, SKPaintSurfaceEventArgs args) { ... SKPaint paint = new SKPaint { Style = SKPaintStyle.Stroke, Color = Color.Red.ToSKColor(), StrokeWidth = 25 }; ... } The Style prperty indicates that you want to stroke a line(in this case the outline of the circle) rather than fill the interior. The three members of the SKPaintStyle enumeration are as follows: Fill Stroke StrokeAndFill The default is Fill . Use the third option to stroke the line and fill the interior with the same color. Set the Color property to a value of type SKColor . One way to get an SKColor value is by converting a Xamarin.Forms Color value to an SKColor value using the extension method ToSKColor . The Extensions class in the SkiaSharp.Views.Forms namespace includes other methods that convert between Xamarin.Forms values and SkiaSharp values. The StrokeWidth property indicates the thickness of the line. Here it's set to 25 pixels. Get more information at here .","title":"Draw Basics"},{"location":"knowledge/mobile-development/xamarin/templates/control-templates/","text":"Xamarin.Forms Control Templates Introduction Xamarin.Forms control templates provide the ability to easily theme and re-theme application pages at runtime. This article provides an introduction to control templates. Controls have different properties, such as BackgroundColor and TextColor , that can define aspects of the control's appearance. These properties can be set using styles , which can be chaned at runtime to implement basic theming. However, styles don't maintain a clean separation between the appearance of a page and its content, and the changes that can be made by setting such properties are limited. Control templates provide a clean separation between the appearance of a page and its content, therefore enabling the creation of pages that can easily be themed. Creating a ControlTemplate Control templates can be defined at the application level or at the page level. This article demonstrates how to create and consume control templates. A ControlTemplate specifies the appearence of a page or view, and contains a root layout, and within the layout, the controls that implement the template. Typically, a ControlTemplate will utilize a ContentPresenter to mark where the content to be displayed bu the page or view will appear. The page or view that consumes the ControlTemplate will then define content to be displayed by the ContentPredenter . The following diageam illustrates a ControlTemplate for a page that contains a number of controls, including a ContentPresenter marked by a blue rectangle: A [ ControlTemplate ] can be applied to the following types by setting their ControlTemplate properties: ContentPage ContentView TemplatedPage TemplatedView When a ControlTemplate is created and assigned to these types, any existing appearance is replaced with the appearance defined in the ControlTemplate . in addtion, as well as setting appearance by using the ControlTemplate property, control templats can also be applied by using styles to further expand theme ability. Control templates can be created in XAML and in C#: Control templates created in XAML are defined in a ResourceDictionary that's assigned to the Resource collection of a page, or more typically to the Resources collection of the application. Control templates created in C# are typically defined in the page's class, or in a class that can be globally accessed. Choosing where to define a ControlTemplate instance impacts where it can be used: ControlTemplate instances defined at the page-level can only be applied to the page. ControlTemplate instances defined at the application-level can be applied to page throughout the application. Creating a ControlTemplate in XAML To define a ControlTemplate at the application level, a ResourceDictionary must be added to the App class. By default, all Xamarin.Forms application created from a template use the App class to implement the Application subclass. To declare a ControlTemplate at the application level, in the application's ResourceDictionary using XAM, the default App class must be replaced with a XAML App class and associated code-behind, as shown in the following code example: <Application xmlns=\"http://xamarin.com/schemas/2014/forms\" xmlns:x=\"http://schemas.microsoft.com/winfx/2009/xaml\" x:Class=\"SimpleTheme.App\"> <Application.Resources> <ResourceDictionary> <ControlTemplate x:Key=\"TealTemplate\"> <Grid> ... <BoxView ... /> <Label Text=\"Control Template Demo App\" TextColor=\"White\" VerticalOptions=\"Center\" ... /> <ContentPresenter ... /> <BoxView Color=\"Teal\" ... /> <Label Text=\"(c) Xamarin 2016\" TextColor=\"White\" VerticalOptions=\"Center\" ... /> </Grid> </ControlTemplate> <ControlTemplate x:Key=\"AquaTemplate\"> ... </ControlTemplate> </ResourceDictionary> </Application.Resources> </Application> Each ControlTemplate instance is created as a reusable object in a ResourceDictionary . This ia achieved by giving each declaration a unique x:Key attribute, which provides it with a descriptive key in the ResourceDictionary . The following code example shows the associated App code-behind: public partial class App : Application { public App () { InitializeComponent (); MainPage = new HomePage (); } } As well as setting the MainPage property, the code-behind must also call the InitializeComponent method to load and parse the associated XAML. The following code example shows a ContentPage applying the TealTemplate to the ContentView : <ContentPage xmlns=\"http://xamarin.com/schemas/2014/forms\" xmlns:x=\"http://schemas.microsoft.com/winfx/2009/xaml\" x:Class=\"SimpleTheme.HomePage\"> <ContentView x:Name=\"contentView\" Padding=\"0,20,0,0\" ControlTemplate=\"{StaticResource TealTemplate}\"> <StackLayout VerticalOptions=\"CenterAndExpand\"> <Label Text=\"Welcome to the app!\" HorizontalOptions=\"Center\" /> <Button Text=\"Change Theme\" Clicked=\"OnButtonClicked\" /> </StackLayout> </ContentView> </ContentPage> The TealTemplate is assigned to the ContentView.ControlTemplate property by using the StaticResource markup extension. The ContentView.Content property is set to a StackLayout that defines the content to be displayed on the ContentPage . This content will be displayed by the ContentPresenter contained in the TealTemplate . This results in the appearance shown in the following screenshots: Re-theming an Application at Runtime Clicking the Change Theme button executes the OnButtonClicked method, which is shown in the following code example: void OnButtonClicked (object sender, EventArgs e) { originalTemplate = !originalTemplate; contentView.ControlTemplate = (originalTemplate) ? tealTemplate : aquaTemplate; } Binding from a ControlTemplate Template bindings allow controls in a control temlate to data bind to pubic properties, enabling property calues on controls in the conteol template to be easily changed. This article demonstrates using template bindings to perform data binding from a control template. A TemplateBinding is used to bind a control's property in a control template to a bindable property on the parent of the target view that owns the control template. For example, rather than defining the text displayed by Lable instances inside the ControlTemplate , you can use a template binding to bind the Label.Text property to bindable properties that define the text to be displayed. Note that using a TemplateBinding outside of a ControlTemplate is not supported. Creating a TemplateBinding in XAML In XAML, a TemplateBinding is created using the TemplateBinding markup extension, as demonstrated in the following code example: <ControlTemplate x:Key=\"TealTemplate\"> <Grid> ... <Label Text=\"{TemplateBinding Parent.HeaderText}\" ... /> ... <Label Text=\"{TemplateBinding Parent.FooterText}\" ... /> </Grid> </ControlTemplate> Rather than set the Label.Text properties to static text, the properties can use template bindings to bind to bindable properties on the parent of the target view that owns the ControlTemplate . However, not that the template bindings bind to Parent.HeaderText and Parent.FooterText , rather HeaderText and FooterText . This is because in this example, the bindable properties are defined on the grandparent of the target view, rather than the paren, as demonstrated in the following code example: <ContentPage ...> <ContentView ... ControlTemplate=\"{StaticResource TealTemplate}\"> ... </ContentView> </ContentPage> The template binding uses the Parent property to return the parent element of the ContentView instance, which is the ContentPage instance. Therefore, using a TemplateBinding in the ControlTemplate to bind to Parent.HeaderText and Parent.FooterText locates the bindable properties that are defined on the ContentPage , as demonstrated in the following code example: public static readonly BindableProperty HeaderTextProperty = BindableProperty.Create (\"HeaderText\", typeof(string), typeof(HomePage), \"Control Template Demo App\"); public static readonly BindableProperty FooterTextProperty = BindableProperty.Create (\"FooterText\", typeof(string), typeof(HomePage), \"(c) Xamarin 2016\"); public string HeaderText { get { return (string)GetValue (HeaderTextProperty); } } public string FooterText { get { return (string)GetValue (FooterTextProperty); } } Get more information at here","title":"Control Templates"},{"location":"knowledge/mobile-development/xamarin/templates/control-templates/#xamarinforms-control-templates","text":"","title":"Xamarin.Forms Control Templates"},{"location":"knowledge/mobile-development/xamarin/templates/control-templates/#introduction","text":"Xamarin.Forms control templates provide the ability to easily theme and re-theme application pages at runtime. This article provides an introduction to control templates. Controls have different properties, such as BackgroundColor and TextColor , that can define aspects of the control's appearance. These properties can be set using styles , which can be chaned at runtime to implement basic theming. However, styles don't maintain a clean separation between the appearance of a page and its content, and the changes that can be made by setting such properties are limited. Control templates provide a clean separation between the appearance of a page and its content, therefore enabling the creation of pages that can easily be themed.","title":"Introduction"},{"location":"knowledge/mobile-development/xamarin/templates/control-templates/#creating-a-controltemplate","text":"Control templates can be defined at the application level or at the page level. This article demonstrates how to create and consume control templates. A ControlTemplate specifies the appearence of a page or view, and contains a root layout, and within the layout, the controls that implement the template. Typically, a ControlTemplate will utilize a ContentPresenter to mark where the content to be displayed bu the page or view will appear. The page or view that consumes the ControlTemplate will then define content to be displayed by the ContentPredenter . The following diageam illustrates a ControlTemplate for a page that contains a number of controls, including a ContentPresenter marked by a blue rectangle: A [ ControlTemplate ] can be applied to the following types by setting their ControlTemplate properties: ContentPage ContentView TemplatedPage TemplatedView When a ControlTemplate is created and assigned to these types, any existing appearance is replaced with the appearance defined in the ControlTemplate . in addtion, as well as setting appearance by using the ControlTemplate property, control templats can also be applied by using styles to further expand theme ability. Control templates can be created in XAML and in C#: Control templates created in XAML are defined in a ResourceDictionary that's assigned to the Resource collection of a page, or more typically to the Resources collection of the application. Control templates created in C# are typically defined in the page's class, or in a class that can be globally accessed. Choosing where to define a ControlTemplate instance impacts where it can be used: ControlTemplate instances defined at the page-level can only be applied to the page. ControlTemplate instances defined at the application-level can be applied to page throughout the application.","title":"Creating a ControlTemplate"},{"location":"knowledge/mobile-development/xamarin/templates/control-templates/#creating-a-controltemplate-in-xaml","text":"To define a ControlTemplate at the application level, a ResourceDictionary must be added to the App class. By default, all Xamarin.Forms application created from a template use the App class to implement the Application subclass. To declare a ControlTemplate at the application level, in the application's ResourceDictionary using XAM, the default App class must be replaced with a XAML App class and associated code-behind, as shown in the following code example: <Application xmlns=\"http://xamarin.com/schemas/2014/forms\" xmlns:x=\"http://schemas.microsoft.com/winfx/2009/xaml\" x:Class=\"SimpleTheme.App\"> <Application.Resources> <ResourceDictionary> <ControlTemplate x:Key=\"TealTemplate\"> <Grid> ... <BoxView ... /> <Label Text=\"Control Template Demo App\" TextColor=\"White\" VerticalOptions=\"Center\" ... /> <ContentPresenter ... /> <BoxView Color=\"Teal\" ... /> <Label Text=\"(c) Xamarin 2016\" TextColor=\"White\" VerticalOptions=\"Center\" ... /> </Grid> </ControlTemplate> <ControlTemplate x:Key=\"AquaTemplate\"> ... </ControlTemplate> </ResourceDictionary> </Application.Resources> </Application> Each ControlTemplate instance is created as a reusable object in a ResourceDictionary . This ia achieved by giving each declaration a unique x:Key attribute, which provides it with a descriptive key in the ResourceDictionary . The following code example shows the associated App code-behind: public partial class App : Application { public App () { InitializeComponent (); MainPage = new HomePage (); } } As well as setting the MainPage property, the code-behind must also call the InitializeComponent method to load and parse the associated XAML. The following code example shows a ContentPage applying the TealTemplate to the ContentView : <ContentPage xmlns=\"http://xamarin.com/schemas/2014/forms\" xmlns:x=\"http://schemas.microsoft.com/winfx/2009/xaml\" x:Class=\"SimpleTheme.HomePage\"> <ContentView x:Name=\"contentView\" Padding=\"0,20,0,0\" ControlTemplate=\"{StaticResource TealTemplate}\"> <StackLayout VerticalOptions=\"CenterAndExpand\"> <Label Text=\"Welcome to the app!\" HorizontalOptions=\"Center\" /> <Button Text=\"Change Theme\" Clicked=\"OnButtonClicked\" /> </StackLayout> </ContentView> </ContentPage> The TealTemplate is assigned to the ContentView.ControlTemplate property by using the StaticResource markup extension. The ContentView.Content property is set to a StackLayout that defines the content to be displayed on the ContentPage . This content will be displayed by the ContentPresenter contained in the TealTemplate . This results in the appearance shown in the following screenshots:","title":"Creating a ControlTemplate in XAML"},{"location":"knowledge/mobile-development/xamarin/templates/control-templates/#re-theming-an-application-at-runtime","text":"Clicking the Change Theme button executes the OnButtonClicked method, which is shown in the following code example: void OnButtonClicked (object sender, EventArgs e) { originalTemplate = !originalTemplate; contentView.ControlTemplate = (originalTemplate) ? tealTemplate : aquaTemplate; }","title":"Re-theming an Application at Runtime"},{"location":"knowledge/mobile-development/xamarin/templates/control-templates/#binding-from-a-controltemplate","text":"Template bindings allow controls in a control temlate to data bind to pubic properties, enabling property calues on controls in the conteol template to be easily changed. This article demonstrates using template bindings to perform data binding from a control template. A TemplateBinding is used to bind a control's property in a control template to a bindable property on the parent of the target view that owns the control template. For example, rather than defining the text displayed by Lable instances inside the ControlTemplate , you can use a template binding to bind the Label.Text property to bindable properties that define the text to be displayed. Note that using a TemplateBinding outside of a ControlTemplate is not supported.","title":"Binding from a ControlTemplate"},{"location":"knowledge/mobile-development/xamarin/templates/control-templates/#creating-a-templatebinding-in-xaml","text":"In XAML, a TemplateBinding is created using the TemplateBinding markup extension, as demonstrated in the following code example: <ControlTemplate x:Key=\"TealTemplate\"> <Grid> ... <Label Text=\"{TemplateBinding Parent.HeaderText}\" ... /> ... <Label Text=\"{TemplateBinding Parent.FooterText}\" ... /> </Grid> </ControlTemplate> Rather than set the Label.Text properties to static text, the properties can use template bindings to bind to bindable properties on the parent of the target view that owns the ControlTemplate . However, not that the template bindings bind to Parent.HeaderText and Parent.FooterText , rather HeaderText and FooterText . This is because in this example, the bindable properties are defined on the grandparent of the target view, rather than the paren, as demonstrated in the following code example: <ContentPage ...> <ContentView ... ControlTemplate=\"{StaticResource TealTemplate}\"> ... </ContentView> </ContentPage> The template binding uses the Parent property to return the parent element of the ContentView instance, which is the ContentPage instance. Therefore, using a TemplateBinding in the ControlTemplate to bind to Parent.HeaderText and Parent.FooterText locates the bindable properties that are defined on the ContentPage , as demonstrated in the following code example: public static readonly BindableProperty HeaderTextProperty = BindableProperty.Create (\"HeaderText\", typeof(string), typeof(HomePage), \"Control Template Demo App\"); public static readonly BindableProperty FooterTextProperty = BindableProperty.Create (\"FooterText\", typeof(string), typeof(HomePage), \"(c) Xamarin 2016\"); public string HeaderText { get { return (string)GetValue (HeaderTextProperty); } } public string FooterText { get { return (string)GetValue (FooterTextProperty); } } Get more information at here","title":"Creating a TemplateBinding in XAML"},{"location":"knowledge/mobile-development/xamarin/templates/data-templates/","text":"Xamarin.Forms Data Templates Introduction Xamarin.Forms data templates provide the ability to dfine the presentation of data on supported controls. This article provides an introduction to data templates, examining why they are necessary. Creating a DataTemplate Data templates can be created inline, in a ResourceDictionary , or fom a custom type or appropriate Xamarin.Forms cell type. An inline template should be used if there's no need to reuse the data template elsewhere. Alternatively, a data template can be reused by definng it as a custom type, or as a control-level, page-level, or application-level resource. A DataTemplate is used to specify the appearance of data, and typically uses data binding to display data. Its common usage scenario is when displaying data from a collection of objects in a ListView . A DataTemplate can be used as a value for the following properties: ListView.HeaderTemplate ListView.FooterTemplate ListView.GroupHeaderTemplate ItemsView.ItemTemplate , which is in herited by ListView . MultiPage.ItemTemplate , which is ingerited by CarouselPage , MasterDetailPage . and TabbedPage . Creating an Inline DataTemplate The elements specified in the DataTemplate define the appearance of each cell, as shown in the following XAML code example: <ListView Margin=\"0,20,0,0\"> <ListView.ItemsSource> <x:Array Type=\"{x:Type local:Person}\"> <local:Person Name=\"Steve\" Age=\"21\" Location=\"USA\" /> <local:Person Name=\"John\" Age=\"37\" Location=\"USA\" /> <local:Person Name=\"Tom\" Age=\"42\" Location=\"UK\" /> <local:Person Name=\"Lucas\" Age=\"29\" Location=\"Germany\" /> <local:Person Name=\"Tariq\" Age=\"39\" Location=\"UK\" /> <local:Person Name=\"Jane\" Age=\"30\" Location=\"USA\" /> </x:Array> </ListView.ItemsSource> <ListView.ItemTemplate> <DataTemplate> <ViewCell> <Grid> ... <Label Text=\"{Binding Name}\" FontAttributes=\"Bold\" /> <Label Grid.Column=\"1\" Text=\"{Binding Age}\" /> <Label Grid.Column=\"2\" Text=\"{Binding Location}\" HorizontalTextAlignment=\"End\" /> </Grid> </ViewCell> </DataTemplate> </ListView.ItemTemplate> </ListView> The child of an inline DataTemplate must be of, or derive from, type Cell . This example uses a ViewCell , which derives from Cell . Layout inside the ViewCell is managed here by a Grid . The Grid contains three Label instances that bind their Text properties to the appropriate properties of each Person object in the collection. Creating a DataTemplate with a Type The ListView.Itemplate property can also be set to a DataTemplate that's created from a cell type. The advantage og this approach is that the appearance defined by the cell type can be reused by multiple data templates throughout the application. The following XAML code shows an example of this approach: <ContentPage xmlns=\"http://xamarin.com/schemas/2014/forms\" xmlns:x=\"http://schemas.microsoft.com/winfx/2009/xaml\" xmlns:local=\"clr-namespace:DataTemplates\" ...> <StackLayout Margin=\"20\"> ... <ListView Margin=\"0,20,0,0\"> <ListView.ItemsSource> <x:Array Type=\"{x:Type local:Person}\"> <local:Person Name=\"Steve\" Age=\"21\" Location=\"USA\" /> ... </x:Array> </ListView.ItemsSource> <ListView.ItemTemplate> <DataTemplate> <local:PersonCell /> </DataTemplate> </ListView.ItemTemplate> </ListView> </StackLayout> </ContentPage> Here, the ListView.ItemTemplate property is set to a DataTemplate that's created from a custom type that defines the cell appearance. The custom type must derive from type ViewCell , as shown in the following code example: <ViewCell xmlns=\"http://xamarin.com/schemas/2014/forms\" xmlns:x=\"http://schemas.microsoft.com/winfx/2009/xaml\" x:Class=\"DataTemplates.PersonCell\"> <Grid> <Grid.ColumnDefinitions> <ColumnDefinition Width=\"0.5*\" /> <ColumnDefinition Width=\"0.2*\" /> <ColumnDefinition Width=\"0.3*\" /> </Grid.ColumnDefinitions> <Label Text=\"{Binding Name}\" FontAttributes=\"Bold\" /> <Label Grid.Column=\"1\" Text=\"{Binding Age}\" /> <Label Grid.Column=\"2\" Text=\"{Binding Location}\" HorizontalTextAlignment=\"End\" /> </Grid> </ViewCell> Creating a DataTemplate as a Resource Data templates can also be created as reusable objects in a ResourceDictionary . This is achieved by giving each declaration a unique x:Key attribute, which provides it with a descriptive key in the Resource Dictionary , as shown in the following XAML code example: <ContentPage xmlns=\"http://xamarin.com/schemas/2014/forms\" xmlns:x=\"http://schemas.microsoft.com/winfx/2009/xaml\" ...> <ContentPage.Resources> <ResourceDictionary> <DataTemplate x:Key=\"personTemplate\"> <ViewCell> <Grid> ... </Grid> </ViewCell> </DataTemplate> </ResourceDictionary> </ContentPage.Resources> <StackLayout Margin=\"20\"> ... <ListView ItemTemplate=\"{StaticResource personTemplate}\" Margin=\"0,20,0,0\"> <ListView.ItemsSource> <x:Array Type=\"{x:Type local:Person}\"> <local:Person Name=\"Steve\" Age=\"21\" Location=\"USA\" /> ... </x:Array> </ListView.ItemsSource> </ListView> </StackLayout> </ContentPage> Creating a DataTemplateSelector A DataTemplateSelector can be used to choose a DataTemplate at runtime based on the value of a data-bound property. This enables multiple DataTemplate instances to be applied to the same type of object, to customize the appearance of particular objects. This article demonstrates how to create and consume a DataTemplateSelector . A data template selector is implemented by creating a class that inherits from DataTemplateSelector . The OnSelectTemplate method is the overridden to return a particular DataTemplate , as shown in the following code example: public class PersonDataTemplateSelector : DataTemplateSelector { public DataTemplate ValidTemplate { get; set; } public DataTemplate InvalidTemplate { get; set; } protected override DataTemplate OnSelectTemplate (object item, BindableObject container) { return ((Person)item).DateOfBirth.Year >= 1980 ? ValidTemplate : InvalidTemplate; } } Limitations DataTemplateSelector instances have the following limitations: The DataTemplateSelector subclass must always return the same template for the same data if queried multiple times. The DataTemplateSelector subclass must not return another DataTemplateSelector subclass. The DataTemplateSelector sub class must no return new instances of a DataTemplate on each call. Instead, the same instance must be returned. Failure to do so will create a memory leak and will disable virtualization. On Android, there can be no more than 20 different data templates per ListView . Consuming a DataTemlateSelector in XAML In XAML, the PersonDataTemplateSelector can be instantiated by declaring it as a resource, as shown in the following code example: <ContentPage xmlns=\"http://xamarin.com/schemas/2014/forms\" xmlns:x=\"http://schemas.microsoft.com/winfx/2009/xaml\" xmlns:local=\"clr-namespace:Selector;assembly=Selector\" x:Class=\"Selector.HomePage\"> <ContentPage.Resources> <ResourceDictionary> <DataTemplate x:Key=\"validPersonTemplate\"> <ViewCell> ... </ViewCell> </DataTemplate> <DataTemplate x:Key=\"invalidPersonTemplate\"> <ViewCell> ... </ViewCell> </DataTemplate> <local:PersonDataTemplateSelector x:Key=\"personDataTemplateSelector\" ValidTemplate=\"{StaticResource validPersonTemplate}\" InvalidTemplate=\"{StaticResource invalidPersonTemplate}\" /> </ResourceDictionary> </ContentPage.Resources> ... </ContentPage> The PersonDataTemplateSelector instance is consumed by assigned it to the ListView.Itemplate property, as shown in the following code example: <ListView x:Name=\"listView\" ItemTemplate=\"{StaticResource personDataTemplateSelector}\" /> Get more information at here","title":"Data Templates"},{"location":"knowledge/mobile-development/xamarin/templates/data-templates/#xamarinforms-data-templates","text":"","title":"Xamarin.Forms Data Templates"},{"location":"knowledge/mobile-development/xamarin/templates/data-templates/#introduction","text":"Xamarin.Forms data templates provide the ability to dfine the presentation of data on supported controls. This article provides an introduction to data templates, examining why they are necessary.","title":"Introduction"},{"location":"knowledge/mobile-development/xamarin/templates/data-templates/#creating-a-datatemplate","text":"Data templates can be created inline, in a ResourceDictionary , or fom a custom type or appropriate Xamarin.Forms cell type. An inline template should be used if there's no need to reuse the data template elsewhere. Alternatively, a data template can be reused by definng it as a custom type, or as a control-level, page-level, or application-level resource. A DataTemplate is used to specify the appearance of data, and typically uses data binding to display data. Its common usage scenario is when displaying data from a collection of objects in a ListView . A DataTemplate can be used as a value for the following properties: ListView.HeaderTemplate ListView.FooterTemplate ListView.GroupHeaderTemplate ItemsView.ItemTemplate , which is in herited by ListView . MultiPage.ItemTemplate , which is ingerited by CarouselPage , MasterDetailPage . and TabbedPage .","title":"Creating a DataTemplate"},{"location":"knowledge/mobile-development/xamarin/templates/data-templates/#creating-an-inline-datatemplate","text":"The elements specified in the DataTemplate define the appearance of each cell, as shown in the following XAML code example: <ListView Margin=\"0,20,0,0\"> <ListView.ItemsSource> <x:Array Type=\"{x:Type local:Person}\"> <local:Person Name=\"Steve\" Age=\"21\" Location=\"USA\" /> <local:Person Name=\"John\" Age=\"37\" Location=\"USA\" /> <local:Person Name=\"Tom\" Age=\"42\" Location=\"UK\" /> <local:Person Name=\"Lucas\" Age=\"29\" Location=\"Germany\" /> <local:Person Name=\"Tariq\" Age=\"39\" Location=\"UK\" /> <local:Person Name=\"Jane\" Age=\"30\" Location=\"USA\" /> </x:Array> </ListView.ItemsSource> <ListView.ItemTemplate> <DataTemplate> <ViewCell> <Grid> ... <Label Text=\"{Binding Name}\" FontAttributes=\"Bold\" /> <Label Grid.Column=\"1\" Text=\"{Binding Age}\" /> <Label Grid.Column=\"2\" Text=\"{Binding Location}\" HorizontalTextAlignment=\"End\" /> </Grid> </ViewCell> </DataTemplate> </ListView.ItemTemplate> </ListView> The child of an inline DataTemplate must be of, or derive from, type Cell . This example uses a ViewCell , which derives from Cell . Layout inside the ViewCell is managed here by a Grid . The Grid contains three Label instances that bind their Text properties to the appropriate properties of each Person object in the collection.","title":"Creating an Inline DataTemplate"},{"location":"knowledge/mobile-development/xamarin/templates/data-templates/#creating-a-datatemplate-with-a-type","text":"The ListView.Itemplate property can also be set to a DataTemplate that's created from a cell type. The advantage og this approach is that the appearance defined by the cell type can be reused by multiple data templates throughout the application. The following XAML code shows an example of this approach: <ContentPage xmlns=\"http://xamarin.com/schemas/2014/forms\" xmlns:x=\"http://schemas.microsoft.com/winfx/2009/xaml\" xmlns:local=\"clr-namespace:DataTemplates\" ...> <StackLayout Margin=\"20\"> ... <ListView Margin=\"0,20,0,0\"> <ListView.ItemsSource> <x:Array Type=\"{x:Type local:Person}\"> <local:Person Name=\"Steve\" Age=\"21\" Location=\"USA\" /> ... </x:Array> </ListView.ItemsSource> <ListView.ItemTemplate> <DataTemplate> <local:PersonCell /> </DataTemplate> </ListView.ItemTemplate> </ListView> </StackLayout> </ContentPage> Here, the ListView.ItemTemplate property is set to a DataTemplate that's created from a custom type that defines the cell appearance. The custom type must derive from type ViewCell , as shown in the following code example: <ViewCell xmlns=\"http://xamarin.com/schemas/2014/forms\" xmlns:x=\"http://schemas.microsoft.com/winfx/2009/xaml\" x:Class=\"DataTemplates.PersonCell\"> <Grid> <Grid.ColumnDefinitions> <ColumnDefinition Width=\"0.5*\" /> <ColumnDefinition Width=\"0.2*\" /> <ColumnDefinition Width=\"0.3*\" /> </Grid.ColumnDefinitions> <Label Text=\"{Binding Name}\" FontAttributes=\"Bold\" /> <Label Grid.Column=\"1\" Text=\"{Binding Age}\" /> <Label Grid.Column=\"2\" Text=\"{Binding Location}\" HorizontalTextAlignment=\"End\" /> </Grid> </ViewCell>","title":"Creating a DataTemplate with a Type"},{"location":"knowledge/mobile-development/xamarin/templates/data-templates/#creating-a-datatemplate-as-a-resource","text":"Data templates can also be created as reusable objects in a ResourceDictionary . This is achieved by giving each declaration a unique x:Key attribute, which provides it with a descriptive key in the Resource Dictionary , as shown in the following XAML code example: <ContentPage xmlns=\"http://xamarin.com/schemas/2014/forms\" xmlns:x=\"http://schemas.microsoft.com/winfx/2009/xaml\" ...> <ContentPage.Resources> <ResourceDictionary> <DataTemplate x:Key=\"personTemplate\"> <ViewCell> <Grid> ... </Grid> </ViewCell> </DataTemplate> </ResourceDictionary> </ContentPage.Resources> <StackLayout Margin=\"20\"> ... <ListView ItemTemplate=\"{StaticResource personTemplate}\" Margin=\"0,20,0,0\"> <ListView.ItemsSource> <x:Array Type=\"{x:Type local:Person}\"> <local:Person Name=\"Steve\" Age=\"21\" Location=\"USA\" /> ... </x:Array> </ListView.ItemsSource> </ListView> </StackLayout> </ContentPage>","title":"Creating a DataTemplate as a Resource"},{"location":"knowledge/mobile-development/xamarin/templates/data-templates/#creating-a-datatemplateselector","text":"A DataTemplateSelector can be used to choose a DataTemplate at runtime based on the value of a data-bound property. This enables multiple DataTemplate instances to be applied to the same type of object, to customize the appearance of particular objects. This article demonstrates how to create and consume a DataTemplateSelector . A data template selector is implemented by creating a class that inherits from DataTemplateSelector . The OnSelectTemplate method is the overridden to return a particular DataTemplate , as shown in the following code example: public class PersonDataTemplateSelector : DataTemplateSelector { public DataTemplate ValidTemplate { get; set; } public DataTemplate InvalidTemplate { get; set; } protected override DataTemplate OnSelectTemplate (object item, BindableObject container) { return ((Person)item).DateOfBirth.Year >= 1980 ? ValidTemplate : InvalidTemplate; } }","title":"Creating a DataTemplateSelector"},{"location":"knowledge/mobile-development/xamarin/templates/data-templates/#limitations","text":"DataTemplateSelector instances have the following limitations: The DataTemplateSelector subclass must always return the same template for the same data if queried multiple times. The DataTemplateSelector subclass must not return another DataTemplateSelector subclass. The DataTemplateSelector sub class must no return new instances of a DataTemplate on each call. Instead, the same instance must be returned. Failure to do so will create a memory leak and will disable virtualization. On Android, there can be no more than 20 different data templates per ListView .","title":"Limitations"},{"location":"knowledge/mobile-development/xamarin/templates/data-templates/#consuming-a-datatemlateselector-in-xaml","text":"In XAML, the PersonDataTemplateSelector can be instantiated by declaring it as a resource, as shown in the following code example: <ContentPage xmlns=\"http://xamarin.com/schemas/2014/forms\" xmlns:x=\"http://schemas.microsoft.com/winfx/2009/xaml\" xmlns:local=\"clr-namespace:Selector;assembly=Selector\" x:Class=\"Selector.HomePage\"> <ContentPage.Resources> <ResourceDictionary> <DataTemplate x:Key=\"validPersonTemplate\"> <ViewCell> ... </ViewCell> </DataTemplate> <DataTemplate x:Key=\"invalidPersonTemplate\"> <ViewCell> ... </ViewCell> </DataTemplate> <local:PersonDataTemplateSelector x:Key=\"personDataTemplateSelector\" ValidTemplate=\"{StaticResource validPersonTemplate}\" InvalidTemplate=\"{StaticResource invalidPersonTemplate}\" /> </ResourceDictionary> </ContentPage.Resources> ... </ContentPage> The PersonDataTemplateSelector instance is consumed by assigned it to the ListView.Itemplate property, as shown in the following code example: <ListView x:Name=\"listView\" ItemTemplate=\"{StaticResource personDataTemplateSelector}\" /> Get more information at here","title":"Consuming a DataTemlateSelector in XAML"},{"location":"knowledge/mobile-development/xamarin/templates/overview/","text":"Xamarin.Forms Templates Control Templates Xamarin.Forms control templates provide the ability to easily theme and re-theme application pages at runtime. Data Templates Xamarin.Forms data templates provide the ability to define the presentation of data on supported controls. Get more information at here","title":"Overview"},{"location":"knowledge/mobile-development/xamarin/templates/overview/#xamarinforms-templates","text":"","title":"Xamarin.Forms Templates"},{"location":"knowledge/mobile-development/xamarin/templates/overview/#control-templates","text":"Xamarin.Forms control templates provide the ability to easily theme and re-theme application pages at runtime.","title":"Control Templates"},{"location":"knowledge/mobile-development/xamarin/templates/overview/#data-templates","text":"Xamarin.Forms data templates provide the ability to define the presentation of data on supported controls. Get more information at here","title":"Data Templates"},{"location":"knowledge/pattern/aggregate-pattern/","text":"Aggregate Pattern Aggregates are alla bout transactional consistency. When there is a strong consistency rule between multiple entities, we make one of them responsible for enforcing it. Problem Imagine that you're writing yet another e-commerce appilcation and you're tasked with writing the all-important Order class: @Entity public class Order { private String id; private Instant date; private List<OrderPosition> orderPositions; // stuff } @Entity public class OrderPosition { private String id; private String name; private BigDecimal quantity; // stuff } Until the order is packed and ready to ship, customers are allowed to add a position to it: public class OrderService { // fields, c-tor public void addPosition(String orderId, String productId, BigDecimal quantity) { // stuff } } Now, the business has asked you to limit the possibility to order more than 100 hundred items or for more than $5,000. How do you ensure that these criteria are met, considering the fact that you're working in a concurrent environment and using a relational database? Aggregate Pattern This is aclassic usage example of the fmous Aggregate Pattern. Although some try to identify aggregates by looking for matching domain concepts, the pattern's real purpose is to help you ensure transactional consistency and performing persistence operations just on this entity. We call such entity an aggregate root. Any other entities that are \"ensured\" consistent have to be saved in a single transaction by saving the aggregate root itself. Applying this to our example, we could say the Order class should be an aggregate over OrderPosition class. This implies the following: The Order class is responsible for ensuring the maximum amount of postions and their maximum balance. The Order class requires some concurrency mechanism e.g. optimistic locking (AKA versioning). There will be an OrderRepository class in the system, but not an OrderPositionRepository class. The addPosition operation has to be performed in a transaction. @Entity public class Order { private String id; private Long version; // optimistic locking! // other fields, c-tor public void addPosition(Product product, BigDecimal quantity) { if (hasMaxPositions()) { throw new TooManyPositionsException(id); } if (valueTooHigh(product.getPrice().times(quantity))) { throw new ValueTooHighException(id); } // stuff } } public class OrderService { // fields, c-tor @Transactional public void addPosition(String orderId, String productId, BigDecimal quantity) { Order order = orderRepository.findById(orderId); Product product = productRepository.findById(productId); order.addPosition(product, quantity); orderRepository.save(order); } } One Aggregate Per transaction There's a good rule for working with aggregates that says that we should not update more than one aggregate per transaction . It makes perect sense. Look, if I'm saying that consistency rules lie within the aggregate, then I should be able to save only this one, single aggregate - and everything should stay consistent. If that's not the case, maybe I'm missing an \"uber-aggregate\" on top of the two that I want to update in a single transaction. Or maybe these rules don't call for transactional consistency at all! The rule of one aggregate per transaction has also a usability argument on its side. Imagine the following scenario: If our users are trying to deal with the same aggregates at the same time, the risk of having a failed transaction grows. User 2 could prevent the tow other users from saving their work and, at the same time, any of the two other users could prevent user 3 from saving his or her work. That's not a desirable state of things. By limiting the transactions to one aggregate, we reduce the risk of failed concerrent transactions and therefore improve the usability of our application. Keep The Aggregates Small The rule above and the way programmers think combine in a pretty strong force towards big aggregates. Let's say that someone added a status field to the Order class: public enum OrderStatus { UNPAID, PAID, IN_DELIVERY, COMPLETED } The business asks you to change the status to PAID once 99% of the order value is paid, to IN_DELIVERY once the guy at the warehouse starts to fill the parcels, and to COMPLETED once all parcels ae delivered. Does that mean that the Order class should now take care of payments, parcels, and trackng the guy at the warehouse? The resulting aggregate would be a giant class with lots of responsibilities, just for the sake of maintaining consistency. Also, performance would suffer badly and the risk of a failed transaction would grow significantly. That's a pretty extreme vision, but it shows the limits of the Aggregate pattern's applicability. Eventual Consistency Things are usually not that bad. If you go and ask the business whether ist's okay for the order status to be updated a few seconds after the payment has been received or the parcel is delivered, they will most likely answer positively. Programmers often envision temporary lack of consistency as something evil and dangerous. That's a nerd point of view. When the business guys say consistent, they usually mean eventually consistent, and often, it could be even later than a few seconds. There fore, talk to the business guys and embrace eventual consistency, when the domain allows it, to keep your aggregates small and your code simple. Get more information at here .","title":"Aggregate Pattern"},{"location":"knowledge/pattern/aggregate-pattern/#aggregate-pattern","text":"Aggregates are alla bout transactional consistency. When there is a strong consistency rule between multiple entities, we make one of them responsible for enforcing it.","title":"Aggregate Pattern"},{"location":"knowledge/pattern/aggregate-pattern/#problem","text":"Imagine that you're writing yet another e-commerce appilcation and you're tasked with writing the all-important Order class: @Entity public class Order { private String id; private Instant date; private List<OrderPosition> orderPositions; // stuff } @Entity public class OrderPosition { private String id; private String name; private BigDecimal quantity; // stuff } Until the order is packed and ready to ship, customers are allowed to add a position to it: public class OrderService { // fields, c-tor public void addPosition(String orderId, String productId, BigDecimal quantity) { // stuff } } Now, the business has asked you to limit the possibility to order more than 100 hundred items or for more than $5,000. How do you ensure that these criteria are met, considering the fact that you're working in a concurrent environment and using a relational database?","title":"Problem"},{"location":"knowledge/pattern/aggregate-pattern/#aggregate-pattern_1","text":"This is aclassic usage example of the fmous Aggregate Pattern. Although some try to identify aggregates by looking for matching domain concepts, the pattern's real purpose is to help you ensure transactional consistency and performing persistence operations just on this entity. We call such entity an aggregate root. Any other entities that are \"ensured\" consistent have to be saved in a single transaction by saving the aggregate root itself. Applying this to our example, we could say the Order class should be an aggregate over OrderPosition class. This implies the following: The Order class is responsible for ensuring the maximum amount of postions and their maximum balance. The Order class requires some concurrency mechanism e.g. optimistic locking (AKA versioning). There will be an OrderRepository class in the system, but not an OrderPositionRepository class. The addPosition operation has to be performed in a transaction. @Entity public class Order { private String id; private Long version; // optimistic locking! // other fields, c-tor public void addPosition(Product product, BigDecimal quantity) { if (hasMaxPositions()) { throw new TooManyPositionsException(id); } if (valueTooHigh(product.getPrice().times(quantity))) { throw new ValueTooHighException(id); } // stuff } } public class OrderService { // fields, c-tor @Transactional public void addPosition(String orderId, String productId, BigDecimal quantity) { Order order = orderRepository.findById(orderId); Product product = productRepository.findById(productId); order.addPosition(product, quantity); orderRepository.save(order); } }","title":"Aggregate Pattern"},{"location":"knowledge/pattern/aggregate-pattern/#one-aggregate-per-transaction","text":"There's a good rule for working with aggregates that says that we should not update more than one aggregate per transaction . It makes perect sense. Look, if I'm saying that consistency rules lie within the aggregate, then I should be able to save only this one, single aggregate - and everything should stay consistent. If that's not the case, maybe I'm missing an \"uber-aggregate\" on top of the two that I want to update in a single transaction. Or maybe these rules don't call for transactional consistency at all! The rule of one aggregate per transaction has also a usability argument on its side. Imagine the following scenario: If our users are trying to deal with the same aggregates at the same time, the risk of having a failed transaction grows. User 2 could prevent the tow other users from saving their work and, at the same time, any of the two other users could prevent user 3 from saving his or her work. That's not a desirable state of things. By limiting the transactions to one aggregate, we reduce the risk of failed concerrent transactions and therefore improve the usability of our application.","title":"One Aggregate Per transaction"},{"location":"knowledge/pattern/aggregate-pattern/#keep-the-aggregates-small","text":"The rule above and the way programmers think combine in a pretty strong force towards big aggregates. Let's say that someone added a status field to the Order class: public enum OrderStatus { UNPAID, PAID, IN_DELIVERY, COMPLETED } The business asks you to change the status to PAID once 99% of the order value is paid, to IN_DELIVERY once the guy at the warehouse starts to fill the parcels, and to COMPLETED once all parcels ae delivered. Does that mean that the Order class should now take care of payments, parcels, and trackng the guy at the warehouse? The resulting aggregate would be a giant class with lots of responsibilities, just for the sake of maintaining consistency. Also, performance would suffer badly and the risk of a failed transaction would grow significantly. That's a pretty extreme vision, but it shows the limits of the Aggregate pattern's applicability.","title":"Keep The Aggregates Small"},{"location":"knowledge/pattern/aggregate-pattern/#eventual-consistency","text":"Things are usually not that bad. If you go and ask the business whether ist's okay for the order status to be updated a few seconds after the payment has been received or the parcel is delivered, they will most likely answer positively. Programmers often envision temporary lack of consistency as something evil and dangerous. That's a nerd point of view. When the business guys say consistent, they usually mean eventually consistent, and often, it could be even later than a few seconds. There fore, talk to the business guys and embrace eventual consistency, when the domain allows it, to keep your aggregates small and your code simple. Get more information at here .","title":"Eventual Consistency"},{"location":"knowledge/pattern/dependency-injection/","text":"Dependency Injection Dependency Injection is a tachnique that facilitates loosely coupled object-oriented software systems. It is closely related to the Dependency Inversion Principle . In simple systems, references to collaborating objects are made directly within classes that need to refer to them. This results in tight coupling between these classes, making them more difficult to test, refactor, and maintain. Dependency Injection is a technique by which the collaborating objects are passed the class that needs to work with them, and the class itself codes against an interface or base class, rather than a specific implemetation class. There are several ways inject dependencies into a class, via one of these parts of the class: contructor, property, method. Constructor injection is the most common approach, and involves passing an instance of the dependency into the class's constructor. The constructor, in turn, sets the dependency to a local private field, which is then used within the class as needed. This is also an example of the Strategy design pattern . Classes that follow the Explicit Dependencies Principle are easily able to take advantage of constructor dependency injection. Property injection is similar, but instead of providing the instance of the dependency via the constructor, the dependency is instead set via a property of the class. This technique is useful in situations where construction of the class cannot be parameterized. With property injection, the calling code needs to set the dependency properties in order for the class to behave directly, and typically the only way this information is exposed is via documentation, comments. Constructor injection, on the hand, ensures that the object can only be instantiated when all of its dependencies are provided, so it eliminates the possibility of an object being in an invalid state. Method injection simply involves passing collaborating objects as parameters to methods. It is most useful on public methods that have dependencies that are not used any where else in the class. so constructor and/or property injection would be overfill. Dependency injection is related to inversion of control containers, which can be used to automatically and centrally manage which instances dependencies should be provided whenever an object needs to be created. Get more information at here .","title":"Dependency Injection"},{"location":"knowledge/pattern/dependency-injection/#dependency-injection","text":"Dependency Injection is a tachnique that facilitates loosely coupled object-oriented software systems. It is closely related to the Dependency Inversion Principle . In simple systems, references to collaborating objects are made directly within classes that need to refer to them. This results in tight coupling between these classes, making them more difficult to test, refactor, and maintain. Dependency Injection is a technique by which the collaborating objects are passed the class that needs to work with them, and the class itself codes against an interface or base class, rather than a specific implemetation class. There are several ways inject dependencies into a class, via one of these parts of the class: contructor, property, method. Constructor injection is the most common approach, and involves passing an instance of the dependency into the class's constructor. The constructor, in turn, sets the dependency to a local private field, which is then used within the class as needed. This is also an example of the Strategy design pattern . Classes that follow the Explicit Dependencies Principle are easily able to take advantage of constructor dependency injection. Property injection is similar, but instead of providing the instance of the dependency via the constructor, the dependency is instead set via a property of the class. This technique is useful in situations where construction of the class cannot be parameterized. With property injection, the calling code needs to set the dependency properties in order for the class to behave directly, and typically the only way this information is exposed is via documentation, comments. Constructor injection, on the hand, ensures that the object can only be instantiated when all of its dependencies are provided, so it eliminates the possibility of an object being in an invalid state. Method injection simply involves passing collaborating objects as parameters to methods. It is most useful on public methods that have dependencies that are not used any where else in the class. so constructor and/or property injection would be overfill. Dependency injection is related to inversion of control containers, which can be used to automatically and centrally manage which instances dependencies should be provided whenever an object needs to be created. Get more information at here .","title":"Dependency Injection"},{"location":"knowledge/pattern/design_pattern/","text":"Design Pattern Desgin Patter - Abstract Factory Pattern Abstract Factory patterns work around a super-factory which creates other factories. This factory is also called as factory of factories. This type of design pattern omes under creational pattern as this pattern provides one of the best ways to create an object. In Abstract Factory pattern an interface is responsible for creating a factory of related objects without explicitly specifying their classes. Each generated factory can give the objects as per the Factory pattern. Implementation We are going to create a Shape and Color interfaces and concrete classes implementing these interfaces. We create an abstract factory class AbstractFactory as next step. Factory classes ShapeFactory and ColorFactory are defined where each factory extends AbstractFactory . A factory creator/generator class FactoryProducer is created. Singleton Pattern This pattern involves a single class which is responsible to create an object while making sure that only single object gets created. This class provides a way to access its only object which can be accessed directly without need to instantiate the object of the class. Implementation Singleton We're going to create a SingleObject class. SingleObject class have its constructor as private and have a static instance of itself. Builder Pattern Builder pattern builds a complex object using simple objects and using a step by step approach. This type of design pattern comes under creational pattern as this pattern provides one of the best ways to create an object. A Builder class builds the final object step by step. This builder is independent of other objects. We have considered a business case of fast-food restaurant where a typical meal could be a burger and a cold drink. Burger could be either a Veg Burger or Chicken Burger and will be packed by a wrapper. Cold drink could be either a coke or pepsi and will be packed in a bottle. Prototype Pattern Prototype pattern refers to crating duplicate object while keeping performance in mind. This type of design pattern comes under creational pattern as this pattern provides one of the best ways to create an object. This pattern involves implementing a prototype interface which tells to create a clone of the current object. Thist pattern is used when creation of object directly is costly. We're going to create an abstract class Shape and concrete classes extending the Shape class. A class ShapeCache is defined as a next step which stores shape objects in a Hashtable and returns their clone when requested. Adapter Pattern Adapter pattern works as a bridge between two incompatible interfaces. This type of design pattern comes under strcutural pattern as this pattern combines the capability of two independent interfaces. This pattern involves a single class which is responsible to join functionalities of independent or incompatible interfaces. A real life example could be a case of card reader whihc acts as an adapter between memory card and a laptop. so that memory card can be read via laptop. State Pattern In State pattern a class behavior changes based on its state. This type of design pattern comes under behavaior pattern. In State pattern, we create objects which represent various states and a context object whose behavior varies as its state object changes. Strategy Pattern In Strategy pattern, a class behavior or its algoirithm can be changed at run time. This type of design pattern comes under behavior pattern. In Strategy pattern, we create objects which represent various strategies and a context object whose behavior varies as per its strategy object. The strategy object changes the executing algorithm of the context object. Observer Pattern Observer pattern is used when there is one-to-many relationship between objects such as if one object is modified, its dependent objects are to notified automatically. Observer pattern falls under behavioral pattern category. Service Locator Pattern The service locator design pattern is used when we want to locate various services using JNDI lookup. Considering high cost of looking up JNDI for a service, Service Locator pattern makes use of caching technique. For the first time a service is required, Service Locator looks up in JNDI and caches the service object. Further lookup or same service via Service Locator is done in its cache which improves the performance of application to greate extent. Following are the entities of this type of design pattern. Service : Actual Service which will process the request. Reference of such service is to be looked upon in JNDI server. Context / Initial Context : JNDI Context carries the reference to service used for lookup purpose. Service Locator : Service Locator is a single point f contact to get services by JNDI lookup caching the services. Cache : Chace to store references of services to reuse them. Client : Client is the object that invokes the services via ServiceLocator. Bridge Pattern Bridge is used when we need to decouple an abstraction from its implementation so that the two can vary independently. This type of design pattern comes under structural pattern as this pattern decouples implementation class and abstract class by providing a bridge structure between them. This pattern involves an interface which acts as a bridge which makes the functionality of concrete classes independent from interface implementr classes. Both types of classes can be altered structurally without affecting each other. We are demonstrating use of Bridge pattern via following example in which a circle can be drawn in different colors using same abstract class method but different bridge implementer classes. Filter Pattern Filter pattern or Criteria pattern is a design pattern that enables developers to filter a set of objects using different criteria and chaining them in a decoupled way through logical operations. This type of design pattern comes under strcutural pattern as this pattern combines multiple criteria to obtain single criteria. Get more information at here .","title":"Design Pattern"},{"location":"knowledge/pattern/design_pattern/#design-pattern","text":"","title":"Design Pattern"},{"location":"knowledge/pattern/design_pattern/#desgin-patter-abstract-factory-pattern","text":"Abstract Factory patterns work around a super-factory which creates other factories. This factory is also called as factory of factories. This type of design pattern omes under creational pattern as this pattern provides one of the best ways to create an object. In Abstract Factory pattern an interface is responsible for creating a factory of related objects without explicitly specifying their classes. Each generated factory can give the objects as per the Factory pattern.","title":"Desgin Patter - Abstract Factory Pattern"},{"location":"knowledge/pattern/design_pattern/#implementation","text":"We are going to create a Shape and Color interfaces and concrete classes implementing these interfaces. We create an abstract factory class AbstractFactory as next step. Factory classes ShapeFactory and ColorFactory are defined where each factory extends AbstractFactory . A factory creator/generator class FactoryProducer is created.","title":"Implementation"},{"location":"knowledge/pattern/design_pattern/#singleton-pattern","text":"This pattern involves a single class which is responsible to create an object while making sure that only single object gets created. This class provides a way to access its only object which can be accessed directly without need to instantiate the object of the class.","title":"Singleton Pattern"},{"location":"knowledge/pattern/design_pattern/#implementation-singleton","text":"We're going to create a SingleObject class. SingleObject class have its constructor as private and have a static instance of itself.","title":"Implementation Singleton"},{"location":"knowledge/pattern/design_pattern/#builder-pattern","text":"Builder pattern builds a complex object using simple objects and using a step by step approach. This type of design pattern comes under creational pattern as this pattern provides one of the best ways to create an object. A Builder class builds the final object step by step. This builder is independent of other objects. We have considered a business case of fast-food restaurant where a typical meal could be a burger and a cold drink. Burger could be either a Veg Burger or Chicken Burger and will be packed by a wrapper. Cold drink could be either a coke or pepsi and will be packed in a bottle.","title":"Builder Pattern"},{"location":"knowledge/pattern/design_pattern/#prototype-pattern","text":"Prototype pattern refers to crating duplicate object while keeping performance in mind. This type of design pattern comes under creational pattern as this pattern provides one of the best ways to create an object. This pattern involves implementing a prototype interface which tells to create a clone of the current object. Thist pattern is used when creation of object directly is costly. We're going to create an abstract class Shape and concrete classes extending the Shape class. A class ShapeCache is defined as a next step which stores shape objects in a Hashtable and returns their clone when requested.","title":"Prototype Pattern"},{"location":"knowledge/pattern/design_pattern/#adapter-pattern","text":"Adapter pattern works as a bridge between two incompatible interfaces. This type of design pattern comes under strcutural pattern as this pattern combines the capability of two independent interfaces. This pattern involves a single class which is responsible to join functionalities of independent or incompatible interfaces. A real life example could be a case of card reader whihc acts as an adapter between memory card and a laptop. so that memory card can be read via laptop.","title":"Adapter Pattern"},{"location":"knowledge/pattern/design_pattern/#state-pattern","text":"In State pattern a class behavior changes based on its state. This type of design pattern comes under behavaior pattern. In State pattern, we create objects which represent various states and a context object whose behavior varies as its state object changes.","title":"State Pattern"},{"location":"knowledge/pattern/design_pattern/#strategy-pattern","text":"In Strategy pattern, a class behavior or its algoirithm can be changed at run time. This type of design pattern comes under behavior pattern. In Strategy pattern, we create objects which represent various strategies and a context object whose behavior varies as per its strategy object. The strategy object changes the executing algorithm of the context object.","title":"Strategy Pattern"},{"location":"knowledge/pattern/design_pattern/#observer-pattern","text":"Observer pattern is used when there is one-to-many relationship between objects such as if one object is modified, its dependent objects are to notified automatically. Observer pattern falls under behavioral pattern category.","title":"Observer Pattern"},{"location":"knowledge/pattern/design_pattern/#service-locator-pattern","text":"The service locator design pattern is used when we want to locate various services using JNDI lookup. Considering high cost of looking up JNDI for a service, Service Locator pattern makes use of caching technique. For the first time a service is required, Service Locator looks up in JNDI and caches the service object. Further lookup or same service via Service Locator is done in its cache which improves the performance of application to greate extent. Following are the entities of this type of design pattern. Service : Actual Service which will process the request. Reference of such service is to be looked upon in JNDI server. Context / Initial Context : JNDI Context carries the reference to service used for lookup purpose. Service Locator : Service Locator is a single point f contact to get services by JNDI lookup caching the services. Cache : Chace to store references of services to reuse them. Client : Client is the object that invokes the services via ServiceLocator.","title":"Service Locator Pattern"},{"location":"knowledge/pattern/design_pattern/#bridge-pattern","text":"Bridge is used when we need to decouple an abstraction from its implementation so that the two can vary independently. This type of design pattern comes under structural pattern as this pattern decouples implementation class and abstract class by providing a bridge structure between them. This pattern involves an interface which acts as a bridge which makes the functionality of concrete classes independent from interface implementr classes. Both types of classes can be altered structurally without affecting each other. We are demonstrating use of Bridge pattern via following example in which a circle can be drawn in different colors using same abstract class method but different bridge implementer classes.","title":"Bridge Pattern"},{"location":"knowledge/pattern/design_pattern/#filter-pattern","text":"Filter pattern or Criteria pattern is a design pattern that enables developers to filter a set of objects using different criteria and chaining them in a decoupled way through logical operations. This type of design pattern comes under strcutural pattern as this pattern combines multiple criteria to obtain single criteria. Get more information at here .","title":"Filter Pattern"},{"location":"knowledge/pattern/injection/","text":"Inversion of Control Containers and the Dependency Injection pattern Components and Services The topic of wiring elements together drags me almost immediately into the knotty terminology problems that surround the terms service and component. You find long and contradictory articles on the definition of these things with ease. For my purposes here are my current uses of these overloaded terms. I use component to mean a glob of software that's intended to be used, without change, by an application that is out of the control of the writers of the component. By 'without change' I mean that the using application doesn't change the source code of the components, although they may alter the components's behavior by extending it in ways allowed by the component writers. A service is similar to a component in that it's used by foreign applications. The main difference is that I expect a component to be usedlocally(think jar file, assembly, dll, or a source import). A service will be used remotely through some remote interface, either synchronous or asynchronous (eg web service, messaging system, RPC, or socket.) Forms of Dependency Injection The basic idea of the Dependency Injection is to have a separate object, an assembler, that populates a field in the lister class with an appropriate implementation for the finder interface. There are three main styles of dependency injection. The names I'm using for them Constructor Injection, Setter Injection and Interface Injection. If you read about this stuff in the current discussions about Inversion of Control you'll hear these referred to as type 1 IoC(Interface injection), type 2 IoC (setter injection) and type 3 IoC(Constructor injection). I find numeric names rather hard to remember, which is why I've used the names I have here. Using a Service Locator The key benefit of a Dependency Injector is that it removes the dependency that the MovieLister class has on the concrete MoviFinder implementation. This allows me to give listers to friends and for them to plig in a suitatble implementation for their own environment. Injection isn't the only way to break this dependency, another is to use a service locator. The basice idea behind a service locator is to have an object that knows how to get hold of all of the services that an application might need. So a service locator for this application would have a method that returns a movie finder when one is needed. Of course this just shifts the burden a tad, we still have to get the locator into the lister, resulting in the dependencies. Using a Segregated Interface for the Locator One of the issues with the simple approach above, is that the MovieLister is dependent on the full service locator class, even though it only uses one service. We can reduce this b using a role interface. That way, instead of using the full service locator interface, the lister can declare just the bit of interface it needs. Get more information at here .","title":"Inversion of Control Containers and the Dependency Injection pattern"},{"location":"knowledge/pattern/injection/#inversion-of-control-containers-and-the-dependency-injection-pattern","text":"","title":"Inversion of Control Containers and the Dependency Injection pattern"},{"location":"knowledge/pattern/injection/#components-and-services","text":"The topic of wiring elements together drags me almost immediately into the knotty terminology problems that surround the terms service and component. You find long and contradictory articles on the definition of these things with ease. For my purposes here are my current uses of these overloaded terms. I use component to mean a glob of software that's intended to be used, without change, by an application that is out of the control of the writers of the component. By 'without change' I mean that the using application doesn't change the source code of the components, although they may alter the components's behavior by extending it in ways allowed by the component writers. A service is similar to a component in that it's used by foreign applications. The main difference is that I expect a component to be usedlocally(think jar file, assembly, dll, or a source import). A service will be used remotely through some remote interface, either synchronous or asynchronous (eg web service, messaging system, RPC, or socket.)","title":"Components and Services"},{"location":"knowledge/pattern/injection/#forms-of-dependency-injection","text":"The basic idea of the Dependency Injection is to have a separate object, an assembler, that populates a field in the lister class with an appropriate implementation for the finder interface. There are three main styles of dependency injection. The names I'm using for them Constructor Injection, Setter Injection and Interface Injection. If you read about this stuff in the current discussions about Inversion of Control you'll hear these referred to as type 1 IoC(Interface injection), type 2 IoC (setter injection) and type 3 IoC(Constructor injection). I find numeric names rather hard to remember, which is why I've used the names I have here.","title":"Forms of Dependency Injection"},{"location":"knowledge/pattern/injection/#using-a-service-locator","text":"The key benefit of a Dependency Injector is that it removes the dependency that the MovieLister class has on the concrete MoviFinder implementation. This allows me to give listers to friends and for them to plig in a suitatble implementation for their own environment. Injection isn't the only way to break this dependency, another is to use a service locator. The basice idea behind a service locator is to have an object that knows how to get hold of all of the services that an application might need. So a service locator for this application would have a method that returns a movie finder when one is needed. Of course this just shifts the burden a tad, we still have to get the locator into the lister, resulting in the dependencies.","title":"Using a Service Locator"},{"location":"knowledge/pattern/injection/#using-a-segregated-interface-for-the-locator","text":"One of the issues with the simple approach above, is that the MovieLister is dependent on the full service locator class, even though it only uses one service. We can reduce this b using a role interface. That way, instead of using the full service locator interface, the lister can declare just the bit of interface it needs. Get more information at here .","title":"Using a Segregated Interface for the Locator"},{"location":"knowledge/pattern/repository-pattern/","text":"Repository Pattern: A data persistence abstraction The Repository Pattern has gained quite a bit of popularity since it was first introduced as a part of Domain-Driven-Design in 2004. Essentially, it provides an abstraction of data, so that application can work with a simple abstraction that has an interface approximating that of a collection. Adding, removing, updating, and selecting items from this collection is done through a series of straightforward methods, without the need to deal with database concerns like connections, commands, cursors, or readers. Using this pattern can help achieve loose coupling and can keep domain objects persistence ignorant . Althogh the pattern is very popular (or perhaps because of this), it is also frewuently misunderstood and misused. Ther are many different ways to implement the Repository patter. Let's consider a few of them, and their merits and drawbacks. Repository Per Entity or Business Object The simplest approach, especially with an existing system, is to create a new Repository implementation for each business object you need to store to or retreve from your persistence layer. Further, you should only implement the specifc methds you are calling in your application. Avoid the trap of creating a \"standard\" repository class, base class, or default interface that you must implement for all repositories. Yes, if you need to have an Update or a Delete method, you should strive to make its interface consisten, but don't implement a Delete method on your LokkupTableRepository that your're only ever going to be calling List() on. The biggest benefit of this approach is YAGNI - you won't waste any time implementing methods that never get called. Generic Repository Interface public interface IRepository<T> where T : EntityBase { T GetById(int id); IEnumerable<T> List(); IEnumerable<T> List(Expression<Func<T, bool>> predicate); void Add(T entity); void Delete(T entity); void Edit(T entity); } public abstract class EntityBase { public int Id { get; protected set; } } Note that taking in a predicate elimicates the need to return an IQueryable, since any filter details can be passed into the repository. This can still lead to leaking of data access details into calling code through. Consider using the Specification pattern to alleviate this issue if you encounter it. Generic Repository Implementation public class Repository<T> : IRepository<T> where T : EntityBase { private readonly ApplicationDbContext _dbContext; public Repository(ApplicationDbContext dbContext) { _dbContext = dbContext; } public virtual T GetById(int id) { return _dbContext.Set<T>().Find(id); } public virtual IEnumerable<T> List() { return _dbContext.Set<T>().AsEnumerable(); } public virtual IEnumerable<T> List(System.Linq.Expressions.Expression<Func<T, bool>> predicate) { return _dbContext.Set<T>() .Where(predicate) .AsEnumerable(); } public void Insert(T entity) { _dbContext.Set<T>().Add(entity); _dbContext.SaveChanges(); } public void Update(T entity) { _dbContext.Entry(entity).State = EntityState.Modified; _dbContext.SaveChanges(); } public void Delete(T entity) { _dbContext.Set<T>().Remove(entity); _dbContext.SaveChanges(); } } Note that in this implementation, all operations are saved as they are performed; there is no Unit of Work being applied. There are a variety of ways in which Unit of Work behavior can be added to this implementation, the simplest of which being to add an explicit Save() method to the IRepository method, and to only call the underlying SaveChanges() method from this method. IQueryable? Another common question with Repositories has to do with what they return. SHould they return data, or should they return quries that can be further refined before execution (IQueryable)? The former is safer, but the latter offers a great deal of flexibility. In fact, you can simplify your interface to only offer a single method for reading data if you go the IQueryable route, since from there any number of items can be reutrned. Specification Repositories that follow the advice of not exposing IQueryable can often become bloated with many custom query methods. The solution to this is to separate queries into their own types, using the Specification Design Pattern . The Specification can include the expression used to filter the query, any parameters associated with this expression, as well as how much data the query should return. Combining the Repository and Specification patterns can be a great way to ensure you follow the Single Repository Principle in your data access code. Get more information at here .","title":"Repository Pattern"},{"location":"knowledge/pattern/repository-pattern/#repository-pattern-a-data-persistence-abstraction","text":"The Repository Pattern has gained quite a bit of popularity since it was first introduced as a part of Domain-Driven-Design in 2004. Essentially, it provides an abstraction of data, so that application can work with a simple abstraction that has an interface approximating that of a collection. Adding, removing, updating, and selecting items from this collection is done through a series of straightforward methods, without the need to deal with database concerns like connections, commands, cursors, or readers. Using this pattern can help achieve loose coupling and can keep domain objects persistence ignorant . Althogh the pattern is very popular (or perhaps because of this), it is also frewuently misunderstood and misused. Ther are many different ways to implement the Repository patter. Let's consider a few of them, and their merits and drawbacks.","title":"Repository Pattern: A data persistence abstraction"},{"location":"knowledge/pattern/repository-pattern/#repository-per-entity-or-business-object","text":"The simplest approach, especially with an existing system, is to create a new Repository implementation for each business object you need to store to or retreve from your persistence layer. Further, you should only implement the specifc methds you are calling in your application. Avoid the trap of creating a \"standard\" repository class, base class, or default interface that you must implement for all repositories. Yes, if you need to have an Update or a Delete method, you should strive to make its interface consisten, but don't implement a Delete method on your LokkupTableRepository that your're only ever going to be calling List() on. The biggest benefit of this approach is YAGNI - you won't waste any time implementing methods that never get called.","title":"Repository Per Entity or Business Object"},{"location":"knowledge/pattern/repository-pattern/#generic-repository-interface","text":"public interface IRepository<T> where T : EntityBase { T GetById(int id); IEnumerable<T> List(); IEnumerable<T> List(Expression<Func<T, bool>> predicate); void Add(T entity); void Delete(T entity); void Edit(T entity); } public abstract class EntityBase { public int Id { get; protected set; } } Note that taking in a predicate elimicates the need to return an IQueryable, since any filter details can be passed into the repository. This can still lead to leaking of data access details into calling code through. Consider using the Specification pattern to alleviate this issue if you encounter it.","title":"Generic Repository Interface"},{"location":"knowledge/pattern/repository-pattern/#generic-repository-implementation","text":"public class Repository<T> : IRepository<T> where T : EntityBase { private readonly ApplicationDbContext _dbContext; public Repository(ApplicationDbContext dbContext) { _dbContext = dbContext; } public virtual T GetById(int id) { return _dbContext.Set<T>().Find(id); } public virtual IEnumerable<T> List() { return _dbContext.Set<T>().AsEnumerable(); } public virtual IEnumerable<T> List(System.Linq.Expressions.Expression<Func<T, bool>> predicate) { return _dbContext.Set<T>() .Where(predicate) .AsEnumerable(); } public void Insert(T entity) { _dbContext.Set<T>().Add(entity); _dbContext.SaveChanges(); } public void Update(T entity) { _dbContext.Entry(entity).State = EntityState.Modified; _dbContext.SaveChanges(); } public void Delete(T entity) { _dbContext.Set<T>().Remove(entity); _dbContext.SaveChanges(); } } Note that in this implementation, all operations are saved as they are performed; there is no Unit of Work being applied. There are a variety of ways in which Unit of Work behavior can be added to this implementation, the simplest of which being to add an explicit Save() method to the IRepository method, and to only call the underlying SaveChanges() method from this method.","title":"Generic Repository Implementation"},{"location":"knowledge/pattern/repository-pattern/#iqueryable","text":"Another common question with Repositories has to do with what they return. SHould they return data, or should they return quries that can be further refined before execution (IQueryable)? The former is safer, but the latter offers a great deal of flexibility. In fact, you can simplify your interface to only offer a single method for reading data if you go the IQueryable route, since from there any number of items can be reutrned.","title":"IQueryable?"},{"location":"knowledge/pattern/repository-pattern/#specification","text":"Repositories that follow the advice of not exposing IQueryable can often become bloated with many custom query methods. The solution to this is to separate queries into their own types, using the Specification Design Pattern . The Specification can include the expression used to filter the query, any parameters associated with this expression, as well as how much data the query should return. Combining the Repository and Specification patterns can be a great way to ensure you follow the Single Repository Principle in your data access code. Get more information at here .","title":"Specification"},{"location":"knowledge/pattern/specification-pattern/","text":"Specification Pattern One Domain-Deriven-Design solution to the problem of where to place querying, sorting, and paging logic is to use a Specification . The Specification design pattern describes a query in an object. So to encapsulate a paged query that searches for some products, one might create a PagedProduct specification which would take in any necessary parameters (PageSize, pageNumber, filter). Then one of your repository methods (usually a List() overload) would accept an ISpecification and would be able to produce the expected result given the specification. There are several benefits to this approach. The spcification has a name (as opposed to just a bunch of LINQ expressions) that you can reason about and discuss. It can be unit tested in isolution to ensure correctness. And it can easily be resued if you need the same behavior (say on a MVC View action and a Web API action, as well as in varyous services). Further, a specificaition can also be used to describe the shape of the data to be returned, so that queries can return just the data they required. This eliminates the need for lazy loading in web applicaiton and help keep repository implementations from becoming clutteredwith these details. Generic Specification Interface // https://github.com/dotnet-architecture/eShopOnWeb public interface ISpecification<T> { Expression<Func<T, bool>> Criteria { get; } List<Expression<Func<T, object>>> Includes { get; } List<string> IncludeStrings { get; } } Generic Specification Implementation (Base Class) // https://github.com/dotnet-architecture/eShopOnWeb public abstract class BaseSpecification<T> : ISpecification<T> { public BaseSpecification(Expression<Func<T, bool>> criteria) { Criteria = criteria; } public Expression<Func<T, bool>> Criteria { get; } public List<Expression<Func<T, object>>> Includes { get; } = new List<Expression<Func<T, object>>>(); public List<string> IncludeStrings { get; } = new List<string>(); protected virtual void AddInclude(Expression<Func<T, object>> includeExpression) { Includes.Add(includeExpression); } // string-based includes allow for including children of children, e.g. Basket.Items.Product protected virtual void AddInclude(string includeString) { IncludeStrings.Add(includeString); } } A Simple Specification The following specification will load a signle basket entity given either the basket's ID or the ID of the buyer to whom the basket belogs. It will eager load the basket's Items collection. public class BasketWithItemsSpecification : BaseSpecification<Basket> { public BasketWithItemsSpecification(int basketId) : base(b => b.Id == basketId) { AddInclude(b => b.Items); } public BasketWithItemsSpecification(string buyerId) : base(b => b.BuyerId == buyerId) { AddInclude(b => b.Items); } } Generic EF Repository with Specification // https://github.com/dotnet-architecture/eShopOnWeb public IEnumerable<T> List(ISpecification<T> spec) { // fetch a Queryable that includes all expression-based includes var queryableResultWithIncludes = spec.Includes .Aggregate(_dbContext.Set<T>().AsQueryable(), (current, include) => current.Include(include)); // modify the IQueryable to include any string-based include statements var secondaryResult = spec.IncludeStrings .Aggregate(queryableResultWithIncludes, (current, include) => current.Include(include)); // return the result of the query using the specification's criteria expression return secondaryResult .Where(spec.Criteria) .AsEnumerable(); } Although it's not recommended to return IQueryable from a repository, it's perfectly fine to use them within the repository to build up a set of results. You can see this approach used in the List method above, which uses intermediate IQueryable expressions to build up the query's list of includes before executing the query with the specification's criteria on the last line. Get more information at here .","title":"Specification Pattern"},{"location":"knowledge/pattern/specification-pattern/#specification-pattern","text":"One Domain-Deriven-Design solution to the problem of where to place querying, sorting, and paging logic is to use a Specification . The Specification design pattern describes a query in an object. So to encapsulate a paged query that searches for some products, one might create a PagedProduct specification which would take in any necessary parameters (PageSize, pageNumber, filter). Then one of your repository methods (usually a List() overload) would accept an ISpecification and would be able to produce the expected result given the specification. There are several benefits to this approach. The spcification has a name (as opposed to just a bunch of LINQ expressions) that you can reason about and discuss. It can be unit tested in isolution to ensure correctness. And it can easily be resued if you need the same behavior (say on a MVC View action and a Web API action, as well as in varyous services). Further, a specificaition can also be used to describe the shape of the data to be returned, so that queries can return just the data they required. This eliminates the need for lazy loading in web applicaiton and help keep repository implementations from becoming clutteredwith these details.","title":"Specification Pattern"},{"location":"knowledge/pattern/specification-pattern/#generic-specification-interface","text":"// https://github.com/dotnet-architecture/eShopOnWeb public interface ISpecification<T> { Expression<Func<T, bool>> Criteria { get; } List<Expression<Func<T, object>>> Includes { get; } List<string> IncludeStrings { get; } }","title":"Generic Specification Interface"},{"location":"knowledge/pattern/specification-pattern/#generic-specification-implementation-base-class","text":"// https://github.com/dotnet-architecture/eShopOnWeb public abstract class BaseSpecification<T> : ISpecification<T> { public BaseSpecification(Expression<Func<T, bool>> criteria) { Criteria = criteria; } public Expression<Func<T, bool>> Criteria { get; } public List<Expression<Func<T, object>>> Includes { get; } = new List<Expression<Func<T, object>>>(); public List<string> IncludeStrings { get; } = new List<string>(); protected virtual void AddInclude(Expression<Func<T, object>> includeExpression) { Includes.Add(includeExpression); } // string-based includes allow for including children of children, e.g. Basket.Items.Product protected virtual void AddInclude(string includeString) { IncludeStrings.Add(includeString); } }","title":"Generic Specification Implementation (Base Class)"},{"location":"knowledge/pattern/specification-pattern/#a-simple-specification","text":"The following specification will load a signle basket entity given either the basket's ID or the ID of the buyer to whom the basket belogs. It will eager load the basket's Items collection. public class BasketWithItemsSpecification : BaseSpecification<Basket> { public BasketWithItemsSpecification(int basketId) : base(b => b.Id == basketId) { AddInclude(b => b.Items); } public BasketWithItemsSpecification(string buyerId) : base(b => b.BuyerId == buyerId) { AddInclude(b => b.Items); } }","title":"A Simple Specification"},{"location":"knowledge/pattern/specification-pattern/#generic-ef-repository-with-specification","text":"// https://github.com/dotnet-architecture/eShopOnWeb public IEnumerable<T> List(ISpecification<T> spec) { // fetch a Queryable that includes all expression-based includes var queryableResultWithIncludes = spec.Includes .Aggregate(_dbContext.Set<T>().AsQueryable(), (current, include) => current.Include(include)); // modify the IQueryable to include any string-based include statements var secondaryResult = spec.IncludeStrings .Aggregate(queryableResultWithIncludes, (current, include) => current.Include(include)); // return the result of the query using the specification's criteria expression return secondaryResult .Where(spec.Criteria) .AsEnumerable(); } Although it's not recommended to return IQueryable from a repository, it's perfectly fine to use them within the repository to build up a set of results. You can see this approach used in the List method above, which uses intermediate IQueryable expressions to build up the query's list of includes before executing the query with the specification's criteria on the last line. Get more information at here .","title":"Generic EF Repository with Specification"},{"location":"knowledge/pattern/the-7-most-important-software-design-patterns/","text":"The 7 most important software design patterns Singleton The singleton pattern is used to limit creation of a lass to only one object. This is beneficial when one (and only one) object is needed to coordinate actions across the system. There are several examples of where only a single instance of a class should exist, includeing caches, thread pools, and registries. Important consideration: It's possible to subclass a singleton by making the constructor protected instead of private. This might be suitable under some circumstances. One approach taken in these scenarios is to create a register of singletons of the subclasses and the getInstance method can take in a parameter or use an environment variable to return the desired singleton. The registry then maintains a mapping of string names to singleton objects, which can be accessed as needed. Factory Method A normal factory produces goods; a software factory produces objects. And not just that -- it does so without specifying the exact class of the object to be created. To accomplish this, objects are created by calling a factory method instead of calling a constructor Strategy The strategy pattern allows grouping related algorithms under an abstraction, which allows switching out one algorithm or policy for another without modifying the client. Instead of directly implementing a single algorithm, the code receives runtime instructions specifying which of the group of algoritms to run. Observer this pattern is a one-to-many dependency between objects so that when one object changes state, all its dependents are notified. This is typically done by calling one of their methods. Key consideration: In case of many subjects and few observers, if each subject stores its observers separately, it'll increase the storage costs as some subjects will be storing the same observer multiple times. Builder As the name implies, a builder pattern is used to build objects. Sometimes, the objects we create can be complex, made up of serveral sub-objects or require an elaborate construction process. The exercise of creating complex types can be simplified by using the builder pattern. A composite or an aggregate object is what builder generally builds. Key consideration: The builder pattern might seem similar to the 'abstract factory' pattern but one difference is that the builder pattern creates an object step by step whereas the abstract factory pattern returns the object in one go. Adpapter This allows incompatible classes to work together by converting the interface of one class into another. Think of it as a sort of translator: when two heads of states who don't speak a common language meet, usually an interpreter sets between the two and translators the conversation, thus enabling communication. If you have two applications, with one spitting out output as XML with the other requiring JSON input, then you'll need an adapter between the two to make them work seamlessly. State The state pattern encapsulates the various states a machine can be in, and allows an object ot alter its behavior when its interal state changes. The machine or the context, as it is called in pattern-speak, can have actions taken on it that propel it into different states. Without the use of the pattern, the code becomes inflexible and littered with if-else conditionals. Get more information at here .","title":"The 7 mose important software design pattern"},{"location":"knowledge/pattern/the-7-most-important-software-design-patterns/#the-7-most-important-software-design-patterns","text":"","title":"The 7 most important software design patterns"},{"location":"knowledge/pattern/the-7-most-important-software-design-patterns/#singleton","text":"The singleton pattern is used to limit creation of a lass to only one object. This is beneficial when one (and only one) object is needed to coordinate actions across the system. There are several examples of where only a single instance of a class should exist, includeing caches, thread pools, and registries. Important consideration: It's possible to subclass a singleton by making the constructor protected instead of private. This might be suitable under some circumstances. One approach taken in these scenarios is to create a register of singletons of the subclasses and the getInstance method can take in a parameter or use an environment variable to return the desired singleton. The registry then maintains a mapping of string names to singleton objects, which can be accessed as needed.","title":"Singleton"},{"location":"knowledge/pattern/the-7-most-important-software-design-patterns/#factory-method","text":"A normal factory produces goods; a software factory produces objects. And not just that -- it does so without specifying the exact class of the object to be created. To accomplish this, objects are created by calling a factory method instead of calling a constructor","title":"Factory Method"},{"location":"knowledge/pattern/the-7-most-important-software-design-patterns/#strategy","text":"The strategy pattern allows grouping related algorithms under an abstraction, which allows switching out one algorithm or policy for another without modifying the client. Instead of directly implementing a single algorithm, the code receives runtime instructions specifying which of the group of algoritms to run.","title":"Strategy"},{"location":"knowledge/pattern/the-7-most-important-software-design-patterns/#observer","text":"this pattern is a one-to-many dependency between objects so that when one object changes state, all its dependents are notified. This is typically done by calling one of their methods. Key consideration: In case of many subjects and few observers, if each subject stores its observers separately, it'll increase the storage costs as some subjects will be storing the same observer multiple times.","title":"Observer"},{"location":"knowledge/pattern/the-7-most-important-software-design-patterns/#builder","text":"As the name implies, a builder pattern is used to build objects. Sometimes, the objects we create can be complex, made up of serveral sub-objects or require an elaborate construction process. The exercise of creating complex types can be simplified by using the builder pattern. A composite or an aggregate object is what builder generally builds. Key consideration: The builder pattern might seem similar to the 'abstract factory' pattern but one difference is that the builder pattern creates an object step by step whereas the abstract factory pattern returns the object in one go.","title":"Builder"},{"location":"knowledge/pattern/the-7-most-important-software-design-patterns/#adpapter","text":"This allows incompatible classes to work together by converting the interface of one class into another. Think of it as a sort of translator: when two heads of states who don't speak a common language meet, usually an interpreter sets between the two and translators the conversation, thus enabling communication. If you have two applications, with one spitting out output as XML with the other requiring JSON input, then you'll need an adapter between the two to make them work seamlessly.","title":"Adpapter"},{"location":"knowledge/pattern/the-7-most-important-software-design-patterns/#state","text":"The state pattern encapsulates the various states a machine can be in, and allows an object ot alter its behavior when its interal state changes. The machine or the context, as it is called in pattern-speak, can have actions taken on it that propel it into different states. Without the use of the pattern, the code becomes inflexible and littered with if-else conditionals. Get more information at here .","title":"State"},{"location":"knowledge/principles/s-o-l-i-d/","text":"S.O.L.I.D: The First 5 Principles of Object Oriented Design S.O.L.I.D is an acronym for the first five object-oriented design(ODD) These priciples, when combined together, make it easy for a programmer to develop sofware that are easy to maintain and extend. They also make it easy for developers to avoid code smells, easily refactor codem and are also a part of the agile or adaptive software development. When expanded the acronyms might seem complicated, but they are pretty simple to grasp. S - Signle-responsibilty principle A class should have one and only one reason to change meaning that a class should have only one job. O - Open-closed principle Objects or entities should be open for extension, but closed for modification. L - Liskov substitution principle Let q(x) be a property provable about objects of x of type T. Then q(y) should be provable for objects y of type S where S is a subtype of T. I - Interface segregation principle A client should never be forced to implement an interface that it doesn't use or clients shouldn't be forced to depend on methods they do not use. D - Dependency Inversion Principle Entities must depend on abstractions not on concretions. It states that the high level module must not depend on the low level module, but they should depend on abstractions. Conclusion Honestly, S.O.L.I.D might seem to be a handful at first, but with continuous usage and adherence to its guidelines, it becomes a part of you and your code which can easily be extended, modified, tested, and refactored without any problems. Get more information at here .","title":"S.O.L.I.D"},{"location":"knowledge/principles/s-o-l-i-d/#solid-the-first-5-principles-of-object-oriented-design","text":"S.O.L.I.D is an acronym for the first five object-oriented design(ODD) These priciples, when combined together, make it easy for a programmer to develop sofware that are easy to maintain and extend. They also make it easy for developers to avoid code smells, easily refactor codem and are also a part of the agile or adaptive software development. When expanded the acronyms might seem complicated, but they are pretty simple to grasp. S - Signle-responsibilty principle A class should have one and only one reason to change meaning that a class should have only one job. O - Open-closed principle Objects or entities should be open for extension, but closed for modification. L - Liskov substitution principle Let q(x) be a property provable about objects of x of type T. Then q(y) should be provable for objects y of type S where S is a subtype of T. I - Interface segregation principle A client should never be forced to implement an interface that it doesn't use or clients shouldn't be forced to depend on methods they do not use. D - Dependency Inversion Principle Entities must depend on abstractions not on concretions. It states that the high level module must not depend on the low level module, but they should depend on abstractions.","title":"S.O.L.I.D: The First 5 Principles of Object Oriented Design"},{"location":"knowledge/principles/s-o-l-i-d/#conclusion","text":"Honestly, S.O.L.I.D might seem to be a handful at first, but with continuous usage and adherence to its guidelines, it becomes a part of you and your code which can easily be extended, modified, tested, and refactored without any problems. Get more information at here .","title":"Conclusion"},{"location":"knowledge/stories/blockchain-blog/matic-developer-support-program/","text":"Matic Developer Support Program The major hurdles that developers face while developing Blockchain'based applications are: Finacials issues (runway to create at least a basic MVP) Talent sourcing (It is both expensive and hard to come by in the blockchain space) Techniques issues like Scaling , UI/UX Design Develop Tools Reaching out to investor after MVP stage is complete, especial when you are not based out of Blockchain Hubs. Developer Support Program( DSP ) will provide developers/teams building Ethereum/Matic with: Techniques guiden our teams and network Early Support Giants . Security audit support . Helping with investor connections . Marketing & promotional support . Access to Matic Sponsored Hackathon to test product ideas and recuit talent . Get more information at here","title":"Matic Developer Support Program"},{"location":"knowledge/stories/blockchain-blog/matic-developer-support-program/#matic-developer-support-program","text":"The major hurdles that developers face while developing Blockchain'based applications are: Finacials issues (runway to create at least a basic MVP) Talent sourcing (It is both expensive and hard to come by in the blockchain space) Techniques issues like Scaling , UI/UX Design Develop Tools Reaching out to investor after MVP stage is complete, especial when you are not based out of Blockchain Hubs. Developer Support Program( DSP ) will provide developers/teams building Ethereum/Matic with: Techniques guiden our teams and network Early Support Giants . Security audit support . Helping with investor connections . Marketing & promotional support . Access to Matic Sponsored Hackathon to test product ideas and recuit talent . Get more information at here","title":"Matic Developer Support Program"},{"location":"knowledge/stories/blockchain-blog/plasma/","text":"Plasma: Scalable Autonomous Smart Contracts Get more information at here","title":"Plasma"},{"location":"knowledge/stories/blockchain-blog/plasma/#plasma-scalable-autonomous-smart-contracts","text":"Get more information at here","title":"Plasma: Scalable Autonomous Smart Contracts"},{"location":"knowledge/stories/blockchain-blog/what-is-matic-network/","text":"What is Matic Network? Introduction Matric network is layer 2 scaling solution that achieves scale by utility sidechains for off-chain but ensuring asset security using the Plasma Framework and a decentralization network of Proof-of-Stake(PoS). Matric strives to solve scalability and usibility issues while not compromise on decentralization and laveraging the existing developer community and ecosytem. Matic network is an off/side chain scaling solution for existing platforms to provide scalability and superior user experimence to DApps/ user functionality. Key Features and Highlights Scalability : Fast, low-cost and secure transaction on Matic sidechains with finality mainchain and Ethereum as the first compatible Layer 1 basechain. High Throughout : Achived up to 10.000 TPS on a single sidechains on internal testnet and multiple chains to be added for horizontal scaling. User Experience : Smooth UX and developer abstraction from mainchain to Matic Chain, native mobile app and SDK with WalletConnect support. Security : Matic chain operators are themselves stakers in the PoS system. Public Sidechain : Matic mainchains are public in nature(. vs individual DApps chains), permissionless and capable of supporting multiple protocols. Value Propostion Matic is unique in terms of techniqual approach towards Layer 2 as well as its potential support for a variety use cases. Matic Layer 2 is an account-based veriant MoreVP( More Visible Plasma). The Plasma Framework is used to guarentee the security of assets on the main chain(such as ERC-20 and ERC-721 tokens for Ethereums). While generic transaction is secured by the Proof on Stakes, built on top of Tendermint. Matic sidechains is essentially EVM-enabled chain and conducive to the ready deployment of solidity smart contracts, essential to make it an easy tool for Ethereum Developer to use it for scaling their DApps/Protocals. Commercially, Matic sidechains are structurally effective for supporting the many Decentralized Finance(DeFi) Protocols availlable in the Ethereum ecosystem. Matic's core philosophy is to anable DApps to compete with the user experence that is offered by centralized apps today. Ethereum is the first basechain Matic Network supports, but Matic intends to offer for additional basechain based on community sugession and consensus, to enable an interoperable descentralized Layer 2 Blockchain platform. Matic strives to achieve a high degree of decentralizationn with trust -less and decentralized execution while ensuring near instant transfers, low fees and conducive economics for micro-transactions. Matic's open source foundation intends to provide the Matic Wallet, payment APIs and SDKs, products, identity solutions and other enabling solutions that will allow developers to design, implwment and migreate DApps built on base platforms like Ethereum. One of the key pillars of Matic Network's ideology is user experience which is very poor for blockchain applications as of now. The Matic team already built high quality Mobile/Web browser libraries with great developer experience, which will enable business to create real world end user applications at large scale. Matic roadmap also includes supporting cross-chain transfers and third party Decentralized exchanges, liquidity pools etc. Problems Decentralized Apps are making huge progress but the current blockchain ecosystem is not prepared to scale as per the demand. Slow block confirmations, block size limitations and computations- in smart contract based blockchains - need to be solved before we target mass adoption by mainstream users. And most importantly, it needs awesome user experience. Some of the problems associated with the current blockchain platforms are as follows: Slow transactions Blockchain transactions are slow and have variable, sometimes exasperating transaction times. Most blockchain protocols have a limit on the block size and it can take a certain amount of tiem to generate a block. Each transaction also has to wati for multiple block confirmations due to potential chain reorganizations. These limitations are often necessary for a public blockchain as a block need to be validated and must be downloaded bu a certain number of nodes to keep it really decentralized. High transactions fees Day by day, the blockchain market is growing and crypto assets are increasingly being created, transferred, and sold, often involving multiple cryto tokens. Every decentralized applications has it'own token and economy. Paying them for their services requires on-chain transfer. Etherrum charges gas fees on each transaction. Fees are an important factor to reward validators and prevent certain kind of security attacks like DoS. But, the problem is that fees vary depending upon the pending transaction pool size due to the limited block size. Low transaction throughput Public blockchains have to maintain a certain amount of time lag between intermediate block production so as to ensure ample time for block propagation. Also, the eblock size need s to be low, so as to ensure quick propagation of the block through the network. This means that the number of transactions in a particular block needs to be fairly limited. Scalability Each block on a blockchain mlust be validated by multiple nodes and/or compute state in case of a smart contract based blockchain. Each node has to manage a copy of the state and all blocks. While the chain size is increasing day by day, maintaining and validation the whole blockchain is correspondingly setting more difficult. This is a huge risk for decentralization as an idea, overall. Multiple micropayment channels Some payment channel solutions solve the problem of micro-payments. However, opening and managing channels with multiple DApps or users is complex. Additionally, the speed and convenience of mediated payments over channels is still up for debate. Poor usability The current system is inherently bad for normal users. Asthe number of ICO increases, users may want to use DApps with different tokens as payment. Without on-chain trade, the convertibility of one crypto token to another represents a new challenge for both investors alive. It introduces complexity for managing multiple crypto tokens and exchanging tokens to pay on different platforms. Matic Network Matic Network solves these problems bu building a decentralized platform using an adated version of Plasma framework that provises a solution for faster and extremelu low cost transactions with finality on a main chain. Matic Network solves the low transaction throughout problem by using a Block Producer layer to produce the blocks. Block Producers enable the system to produce blocks at a very fast rate. The system ensures decentralization using PoS checkpoints which are pushed to the Ethereum mainchain. This enables Matic to theoretically achieve 2^16 transation on a single side chain. In addition, we are developing a suite of developer tools such as the real-time notification engine for Etherum events - Dagger and a scalable, modular and realtime Ethereum data processor - hermione. Apart from this, the Matic team actively contributes to the WalletConnect protocol implementation. Architecture When a user is transferring ETH or ERC20 tokens, they have to wait for block confirmation times which ranges from 14 seconds to 20 seconds. Also you have to wait for multiple blocks to be sure of the finaltity of the block inclusion in the chain. That's a deterrent for users to use the service. As crytocurrency gains favor, more transactions will jam the Ethereum network and gas fees will increae on an average for each transactionn for faster confirmations by users. Note that gas fees vary as per traffic and confirmation time. We propose Matic as a solution to overcome these problems. Here is how Matic works: User deposits crypto assets in the Matic contract on mainchain (currently implemented with Ethereum blockchain only) Once deposited tokens get confirmed on the main chain, the corresponding tokens will ger reflected on the Matic chain. The user can now transfer tokens to anyone they want instantly with negligible fees . Matic chain has faster blocks (approximately 1 second or less). That way, the transfer will be done almost instantly. Once a user is ready, they can withdraw remaining tokens from the main chain by establishing proof of remaining tokens on Root contract (contract deployed on Ethereum chain) Remember any fungible crypto assets can be rerpresented as ERC20 tokens on Matic chain. That way, the same method will work for any fungible crypto assets. In addition, we have also added support for ERC721/ NFTs (Non Fungible Tokens). Check out on Githuh repos . Consensus and security To provide some context, the Matic ecosystem will have the following actor: End Users. DApp developers: Developers are expected to use the Matic Network to scale their applications and provide a better UI/UX to their end users Stakers Stakes need to deposit/stake tokens to qualify and play a very important role in the Matic Network. They validate the transactions and propose checkpoints on the mainchain using PoS consensus mechaism with a 2/3 majority. They also choose Block Producers amongst themselves, who satisfy a certain criteria, to produce blocks on the sidechains. Block Producers: These are block producers chosen by Stakers who in turn enable faster blockchain generationn times. They have to provide a significant stake to be nominated. The Matic Network uses a dual strategy of Proof of Stake at the checkpointing layer and Block Producers at the block producer layer to achieve faster blocktimes while ensuring a high degree of decentralization by achieveing finality on the main chains using the checkpoints and fraid prood mechanisms . Bassically, anyone can stake their Matic tokens on root contract to become a Staker in the PoS checkpointing layer (contract deployed on Ethereum chain). This provides a high degree decentralized base layer for Matic chain. At the blockchain layer of the Matic Network, there are Block Producers, selected by PoS Stakers on the base layer, who will be creating the Matic Blocks. To achieve faster block generation times, these Block Producers will be low in number. This layer is expexted to achieve ~ second block generation times at extremely low to negligible transaction fees. On Matic Network's chackpointing layer, the basis of Matic Network's PoS mechanism, for every few blocks on the block layer of the Matic Network, a proposer will be chosen among the stakeholders to propose a checkpoint on the main chain. These checkpoints are created by the proposer after validating all the blocks on the block layer of the Matic Network and creating the Merkle tree of the block hashes since the last checkpoint. The Merkle root is the broadcasted to the Staker Network for their signatures. The order stakeholders also verify the proof. They will approve the proposed blocks, if it is valid, by providing their signatures. The system needs the approval of 2/3 of the stakeholders to propose a \"header block\" to the root contract. Once the checkpoint is proposed on the mainchain, anyone on the Ethereum mainchain can challenge the proposed checkpoint within a specified prrod of time. if no one challenges it and the challenge period ends, the checkpoint if formally included as a valid checkpoint on the main chain. Following is a illustration of the \"Header block\": More on header block AKA checkpoint at here . Fraud Proofs To enhance the security of the transactions, Matic Network also provides Fraud Proofs on the mainchain. The mechanism enables any individual on the mainchain to submit the details of the transactions which he/sh thinks is fraudulent. If the challenge is successful, the stakes of the parties involved in the fraud are shared and the challenger receives the slashed funds as an incentive for detecting the fraud. This can be considered as an always-running high reward bounty program for any parties who widh to investigate the veracity of the transactionns on the Matic Network. Multi Chain Support (Horizontal Sharding) The Matic Network public checkpointing layer supports multiple side chains by design. Theoretically, there can be an infinite number of side chains working under the secured and decentrailized layer of checkpoints. Businesses can have their dedicated side chains connected to the public checkpointing layer having full control of their execution environments, while still retaining the immutability, provability and security of transactions via the checkpointing mechanism. Key factors influencing the design of this sharding process are expected to be: Scheduling of checkpointing layer to periodically propose checkpoints for different side chains. Movement of assets across multiple side chains 2.1 User will be able to send assets across side chains using chain ids and receipts. 2.2 Users will be provided with an intuitive wallet interface to perform interchain transactions. 2.3 Developers will be provided with API/SDKs to build programmable interfaces for inter chain transactions. Movement of the assets from one chain to another will be managed at the checkpointing layer and may not require any interaction with the mainchain. Research is currently underway to facilitate faster (possibly instant) inter sidechain transfers. Potential Use Cases Matic Foundation is committed to provide a scalable and user friendly ecosystem for third party Decentralized applications to thrive on. Matic Foundation like Ethereum and other platform foundations will promote various Base chain DApps(like DApps built Ethereum currently, and NEO, EOS in the future) to build and migrate their user facing applications/ transactions on Matic Network. It will also award grants and funding to third party app developers to buld various user cases on top of Matic Network like: Payments The Matic Network will provide an interface for users, payment APIs and SDKs for DApps, merchant and users to instantly accept or pay in crypto assets. Atomic swaps Matic smart contracts will allow users to pay with any crypto token thay prefer and receiver will receive payment in assets thay prefer. Matic will handle conversion through atomic swap between cross-chain crypto assets. Liquidity providers Third parties can use the Matic Network to exchange any tokens for other tokens by leveraging 0x liquidity pool or other liquidity providers while transferring crypto assets. In the case of fiat, the Matic Development Team is planning to collaborate with fiat liquidity providers in currencies of major countries. Decentralized Exchange (DEX) and Marketplace support The Matic Network is expected to have all characteristics which an exchange platform should have - faster and cheapers trades. The Matic Network is capable of supporting decentralized exchanges and enabling trust-less, reliable and easy crypto trades. The decentralized exchange is the future for digital assets and provides better security or solvency than the centralized exchanges. Lending platform The Matic Network will enable platforms for merchants to assess the creditworthiness of connected users via their transaction history. This enables merchants to len tokens to users on the network when transacting with users that fon not have sufficient funds. The Matic Network expects to use the Dharma protocol to provide tokenized debt to users. Identity Users need utilitarian yet user-friendly interface where MetaMask or web3 enabled browsers are not required. Thay do not need to understand how Ethereum works under the hood. Decentralized apps need a way to sign transactions, but that must happen without submitting private keys on each DApp on wen browser or mobile apps. The Matic Development Team believes that users must have control over their private keys withour worrying about the security. The Matic Network will solve that with Open-Identity system and will deliver a seamless experience to users. This system will also provide a way to auto-approve certain kind of transactions depending upon the criteria chosen by the users. This will drive the recurring payments on the Matic Network. Games We expect games to be a big part of the Matic Network. In-game assets represented as NFTs (ERC721) are expected to be bought, sold and traded in huge numbers on our sidechains. Developers will also be able to save game state on the sidechains, if they choose to. Along with the NFT marketplace that we will enable,developers and users will truly have a fast, efficient and secure sidechain to build and play games on. Infrastructure The Matic Development Team will act on the simple mantra - make it simple and seamless. For that, the team will provide new infrastructure around the Matic Network including user-friendly wallets for individual users and merchants, payroll dashboards, payment SDKs and other open source tools. Dagger Dagger is a tool or engine to track Ethereum accounts and events in real-time. We can learn more about Dagger here and check how it works at here . Developers can use Dagger to track their own smart contracts, accounts and transactions. They can create custom service or integrate with third-party services through IFTTT or Zapier. Partners (partial list) Decentraland : Decentraland is a virtual reality platform powered by the Ethereum blockchain. Users can create, experience, and monetize content and applications using a developer toolkit that works on any platform (all VR headsets and web browsers) and is designed to create virtual environments an applications. Quarkchain : QuarkChain is an innovative permissionless blockchain architecture that aims to provide a secure, decentralized and scalable blockchain solution. Ankr Network : Ankr is a distributed computing platform that aims to leverage idle computing resources in data centres and edge devices. Portis : Portis is a technology company that wants to make sure any person in the world can use decentralized application, just as easily as they use \"regular\" apps. Maker DAO : Maker is the organization behind the DAI stablecoin. Ripio Credit Network : Ripio Credit Network is a peer to peer credit network that connects lenders and borrowsers located anywhere in the world. Get more information at here .","title":"What Is Matic Network"},{"location":"knowledge/stories/blockchain-blog/what-is-matic-network/#what-is-matic-network","text":"","title":"What is Matic Network?"},{"location":"knowledge/stories/blockchain-blog/what-is-matic-network/#introduction","text":"Matric network is layer 2 scaling solution that achieves scale by utility sidechains for off-chain but ensuring asset security using the Plasma Framework and a decentralization network of Proof-of-Stake(PoS). Matric strives to solve scalability and usibility issues while not compromise on decentralization and laveraging the existing developer community and ecosytem. Matic network is an off/side chain scaling solution for existing platforms to provide scalability and superior user experimence to DApps/ user functionality.","title":"Introduction"},{"location":"knowledge/stories/blockchain-blog/what-is-matic-network/#key-features-and-highlights","text":"Scalability : Fast, low-cost and secure transaction on Matic sidechains with finality mainchain and Ethereum as the first compatible Layer 1 basechain. High Throughout : Achived up to 10.000 TPS on a single sidechains on internal testnet and multiple chains to be added for horizontal scaling. User Experience : Smooth UX and developer abstraction from mainchain to Matic Chain, native mobile app and SDK with WalletConnect support. Security : Matic chain operators are themselves stakers in the PoS system. Public Sidechain : Matic mainchains are public in nature(. vs individual DApps chains), permissionless and capable of supporting multiple protocols.","title":"Key Features and Highlights"},{"location":"knowledge/stories/blockchain-blog/what-is-matic-network/#value-propostion","text":"Matic is unique in terms of techniqual approach towards Layer 2 as well as its potential support for a variety use cases. Matic Layer 2 is an account-based veriant MoreVP( More Visible Plasma). The Plasma Framework is used to guarentee the security of assets on the main chain(such as ERC-20 and ERC-721 tokens for Ethereums). While generic transaction is secured by the Proof on Stakes, built on top of Tendermint. Matic sidechains is essentially EVM-enabled chain and conducive to the ready deployment of solidity smart contracts, essential to make it an easy tool for Ethereum Developer to use it for scaling their DApps/Protocals. Commercially, Matic sidechains are structurally effective for supporting the many Decentralized Finance(DeFi) Protocols availlable in the Ethereum ecosystem. Matic's core philosophy is to anable DApps to compete with the user experence that is offered by centralized apps today. Ethereum is the first basechain Matic Network supports, but Matic intends to offer for additional basechain based on community sugession and consensus, to enable an interoperable descentralized Layer 2 Blockchain platform. Matic strives to achieve a high degree of decentralizationn with trust -less and decentralized execution while ensuring near instant transfers, low fees and conducive economics for micro-transactions. Matic's open source foundation intends to provide the Matic Wallet, payment APIs and SDKs, products, identity solutions and other enabling solutions that will allow developers to design, implwment and migreate DApps built on base platforms like Ethereum. One of the key pillars of Matic Network's ideology is user experience which is very poor for blockchain applications as of now. The Matic team already built high quality Mobile/Web browser libraries with great developer experience, which will enable business to create real world end user applications at large scale. Matic roadmap also includes supporting cross-chain transfers and third party Decentralized exchanges, liquidity pools etc.","title":"Value Propostion"},{"location":"knowledge/stories/blockchain-blog/what-is-matic-network/#problems","text":"Decentralized Apps are making huge progress but the current blockchain ecosystem is not prepared to scale as per the demand. Slow block confirmations, block size limitations and computations- in smart contract based blockchains - need to be solved before we target mass adoption by mainstream users. And most importantly, it needs awesome user experience. Some of the problems associated with the current blockchain platforms are as follows:","title":"Problems"},{"location":"knowledge/stories/blockchain-blog/what-is-matic-network/#slow-transactions","text":"Blockchain transactions are slow and have variable, sometimes exasperating transaction times. Most blockchain protocols have a limit on the block size and it can take a certain amount of tiem to generate a block. Each transaction also has to wati for multiple block confirmations due to potential chain reorganizations. These limitations are often necessary for a public blockchain as a block need to be validated and must be downloaded bu a certain number of nodes to keep it really decentralized.","title":"Slow transactions"},{"location":"knowledge/stories/blockchain-blog/what-is-matic-network/#high-transactions-fees","text":"Day by day, the blockchain market is growing and crypto assets are increasingly being created, transferred, and sold, often involving multiple cryto tokens. Every decentralized applications has it'own token and economy. Paying them for their services requires on-chain transfer. Etherrum charges gas fees on each transaction. Fees are an important factor to reward validators and prevent certain kind of security attacks like DoS. But, the problem is that fees vary depending upon the pending transaction pool size due to the limited block size.","title":"High transactions fees"},{"location":"knowledge/stories/blockchain-blog/what-is-matic-network/#low-transaction-throughput","text":"Public blockchains have to maintain a certain amount of time lag between intermediate block production so as to ensure ample time for block propagation. Also, the eblock size need s to be low, so as to ensure quick propagation of the block through the network. This means that the number of transactions in a particular block needs to be fairly limited.","title":"Low transaction throughput"},{"location":"knowledge/stories/blockchain-blog/what-is-matic-network/#scalability","text":"Each block on a blockchain mlust be validated by multiple nodes and/or compute state in case of a smart contract based blockchain. Each node has to manage a copy of the state and all blocks. While the chain size is increasing day by day, maintaining and validation the whole blockchain is correspondingly setting more difficult. This is a huge risk for decentralization as an idea, overall.","title":"Scalability"},{"location":"knowledge/stories/blockchain-blog/what-is-matic-network/#multiple-micropayment-channels","text":"Some payment channel solutions solve the problem of micro-payments. However, opening and managing channels with multiple DApps or users is complex. Additionally, the speed and convenience of mediated payments over channels is still up for debate.","title":"Multiple micropayment channels"},{"location":"knowledge/stories/blockchain-blog/what-is-matic-network/#poor-usability","text":"The current system is inherently bad for normal users. Asthe number of ICO increases, users may want to use DApps with different tokens as payment. Without on-chain trade, the convertibility of one crypto token to another represents a new challenge for both investors alive. It introduces complexity for managing multiple crypto tokens and exchanging tokens to pay on different platforms.","title":"Poor usability"},{"location":"knowledge/stories/blockchain-blog/what-is-matic-network/#matic-network","text":"Matic Network solves these problems bu building a decentralized platform using an adated version of Plasma framework that provises a solution for faster and extremelu low cost transactions with finality on a main chain. Matic Network solves the low transaction throughout problem by using a Block Producer layer to produce the blocks. Block Producers enable the system to produce blocks at a very fast rate. The system ensures decentralization using PoS checkpoints which are pushed to the Ethereum mainchain. This enables Matic to theoretically achieve 2^16 transation on a single side chain. In addition, we are developing a suite of developer tools such as the real-time notification engine for Etherum events - Dagger and a scalable, modular and realtime Ethereum data processor - hermione. Apart from this, the Matic team actively contributes to the WalletConnect protocol implementation.","title":"Matic Network"},{"location":"knowledge/stories/blockchain-blog/what-is-matic-network/#architecture","text":"When a user is transferring ETH or ERC20 tokens, they have to wait for block confirmation times which ranges from 14 seconds to 20 seconds. Also you have to wait for multiple blocks to be sure of the finaltity of the block inclusion in the chain. That's a deterrent for users to use the service. As crytocurrency gains favor, more transactions will jam the Ethereum network and gas fees will increae on an average for each transactionn for faster confirmations by users. Note that gas fees vary as per traffic and confirmation time. We propose Matic as a solution to overcome these problems.","title":"Architecture"},{"location":"knowledge/stories/blockchain-blog/what-is-matic-network/#here-is-how-matic-works","text":"User deposits crypto assets in the Matic contract on mainchain (currently implemented with Ethereum blockchain only) Once deposited tokens get confirmed on the main chain, the corresponding tokens will ger reflected on the Matic chain. The user can now transfer tokens to anyone they want instantly with negligible fees . Matic chain has faster blocks (approximately 1 second or less). That way, the transfer will be done almost instantly. Once a user is ready, they can withdraw remaining tokens from the main chain by establishing proof of remaining tokens on Root contract (contract deployed on Ethereum chain) Remember any fungible crypto assets can be rerpresented as ERC20 tokens on Matic chain. That way, the same method will work for any fungible crypto assets. In addition, we have also added support for ERC721/ NFTs (Non Fungible Tokens). Check out on Githuh repos .","title":"Here is how Matic works:"},{"location":"knowledge/stories/blockchain-blog/what-is-matic-network/#consensus-and-security","text":"To provide some context, the Matic ecosystem will have the following actor: End Users. DApp developers: Developers are expected to use the Matic Network to scale their applications and provide a better UI/UX to their end users Stakers Stakes need to deposit/stake tokens to qualify and play a very important role in the Matic Network. They validate the transactions and propose checkpoints on the mainchain using PoS consensus mechaism with a 2/3 majority. They also choose Block Producers amongst themselves, who satisfy a certain criteria, to produce blocks on the sidechains. Block Producers: These are block producers chosen by Stakers who in turn enable faster blockchain generationn times. They have to provide a significant stake to be nominated. The Matic Network uses a dual strategy of Proof of Stake at the checkpointing layer and Block Producers at the block producer layer to achieve faster blocktimes while ensuring a high degree of decentralization by achieveing finality on the main chains using the checkpoints and fraid prood mechanisms . Bassically, anyone can stake their Matic tokens on root contract to become a Staker in the PoS checkpointing layer (contract deployed on Ethereum chain). This provides a high degree decentralized base layer for Matic chain. At the blockchain layer of the Matic Network, there are Block Producers, selected by PoS Stakers on the base layer, who will be creating the Matic Blocks. To achieve faster block generation times, these Block Producers will be low in number. This layer is expexted to achieve ~ second block generation times at extremely low to negligible transaction fees. On Matic Network's chackpointing layer, the basis of Matic Network's PoS mechanism, for every few blocks on the block layer of the Matic Network, a proposer will be chosen among the stakeholders to propose a checkpoint on the main chain. These checkpoints are created by the proposer after validating all the blocks on the block layer of the Matic Network and creating the Merkle tree of the block hashes since the last checkpoint. The Merkle root is the broadcasted to the Staker Network for their signatures. The order stakeholders also verify the proof. They will approve the proposed blocks, if it is valid, by providing their signatures. The system needs the approval of 2/3 of the stakeholders to propose a \"header block\" to the root contract. Once the checkpoint is proposed on the mainchain, anyone on the Ethereum mainchain can challenge the proposed checkpoint within a specified prrod of time. if no one challenges it and the challenge period ends, the checkpoint if formally included as a valid checkpoint on the main chain. Following is a illustration of the \"Header block\": More on header block AKA checkpoint at here .","title":"Consensus and security"},{"location":"knowledge/stories/blockchain-blog/what-is-matic-network/#fraud-proofs","text":"To enhance the security of the transactions, Matic Network also provides Fraud Proofs on the mainchain. The mechanism enables any individual on the mainchain to submit the details of the transactions which he/sh thinks is fraudulent. If the challenge is successful, the stakes of the parties involved in the fraud are shared and the challenger receives the slashed funds as an incentive for detecting the fraud. This can be considered as an always-running high reward bounty program for any parties who widh to investigate the veracity of the transactionns on the Matic Network.","title":"Fraud Proofs"},{"location":"knowledge/stories/blockchain-blog/what-is-matic-network/#multi-chain-support-horizontal-sharding","text":"The Matic Network public checkpointing layer supports multiple side chains by design. Theoretically, there can be an infinite number of side chains working under the secured and decentrailized layer of checkpoints. Businesses can have their dedicated side chains connected to the public checkpointing layer having full control of their execution environments, while still retaining the immutability, provability and security of transactions via the checkpointing mechanism. Key factors influencing the design of this sharding process are expected to be: Scheduling of checkpointing layer to periodically propose checkpoints for different side chains. Movement of assets across multiple side chains 2.1 User will be able to send assets across side chains using chain ids and receipts. 2.2 Users will be provided with an intuitive wallet interface to perform interchain transactions. 2.3 Developers will be provided with API/SDKs to build programmable interfaces for inter chain transactions. Movement of the assets from one chain to another will be managed at the checkpointing layer and may not require any interaction with the mainchain. Research is currently underway to facilitate faster (possibly instant) inter sidechain transfers.","title":"Multi Chain Support (Horizontal Sharding)"},{"location":"knowledge/stories/blockchain-blog/what-is-matic-network/#potential-use-cases","text":"Matic Foundation is committed to provide a scalable and user friendly ecosystem for third party Decentralized applications to thrive on. Matic Foundation like Ethereum and other platform foundations will promote various Base chain DApps(like DApps built Ethereum currently, and NEO, EOS in the future) to build and migrate their user facing applications/ transactions on Matic Network. It will also award grants and funding to third party app developers to buld various user cases on top of Matic Network like:","title":"Potential Use Cases"},{"location":"knowledge/stories/blockchain-blog/what-is-matic-network/#payments","text":"The Matic Network will provide an interface for users, payment APIs and SDKs for DApps, merchant and users to instantly accept or pay in crypto assets.","title":"Payments"},{"location":"knowledge/stories/blockchain-blog/what-is-matic-network/#atomic-swaps","text":"Matic smart contracts will allow users to pay with any crypto token thay prefer and receiver will receive payment in assets thay prefer. Matic will handle conversion through atomic swap between cross-chain crypto assets.","title":"Atomic swaps"},{"location":"knowledge/stories/blockchain-blog/what-is-matic-network/#liquidity-providers","text":"Third parties can use the Matic Network to exchange any tokens for other tokens by leveraging 0x liquidity pool or other liquidity providers while transferring crypto assets. In the case of fiat, the Matic Development Team is planning to collaborate with fiat liquidity providers in currencies of major countries.","title":"Liquidity providers"},{"location":"knowledge/stories/blockchain-blog/what-is-matic-network/#decentralized-exchange-dex-and-marketplace-support","text":"The Matic Network is expected to have all characteristics which an exchange platform should have - faster and cheapers trades. The Matic Network is capable of supporting decentralized exchanges and enabling trust-less, reliable and easy crypto trades. The decentralized exchange is the future for digital assets and provides better security or solvency than the centralized exchanges.","title":"Decentralized Exchange (DEX) and Marketplace support"},{"location":"knowledge/stories/blockchain-blog/what-is-matic-network/#lending-platform","text":"The Matic Network will enable platforms for merchants to assess the creditworthiness of connected users via their transaction history. This enables merchants to len tokens to users on the network when transacting with users that fon not have sufficient funds. The Matic Network expects to use the Dharma protocol to provide tokenized debt to users.","title":"Lending platform"},{"location":"knowledge/stories/blockchain-blog/what-is-matic-network/#identity","text":"Users need utilitarian yet user-friendly interface where MetaMask or web3 enabled browsers are not required. Thay do not need to understand how Ethereum works under the hood. Decentralized apps need a way to sign transactions, but that must happen without submitting private keys on each DApp on wen browser or mobile apps. The Matic Development Team believes that users must have control over their private keys withour worrying about the security. The Matic Network will solve that with Open-Identity system and will deliver a seamless experience to users. This system will also provide a way to auto-approve certain kind of transactions depending upon the criteria chosen by the users. This will drive the recurring payments on the Matic Network.","title":"Identity"},{"location":"knowledge/stories/blockchain-blog/what-is-matic-network/#games","text":"We expect games to be a big part of the Matic Network. In-game assets represented as NFTs (ERC721) are expected to be bought, sold and traded in huge numbers on our sidechains. Developers will also be able to save game state on the sidechains, if they choose to. Along with the NFT marketplace that we will enable,developers and users will truly have a fast, efficient and secure sidechain to build and play games on.","title":"Games"},{"location":"knowledge/stories/blockchain-blog/what-is-matic-network/#infrastructure","text":"The Matic Development Team will act on the simple mantra - make it simple and seamless. For that, the team will provide new infrastructure around the Matic Network including user-friendly wallets for individual users and merchants, payroll dashboards, payment SDKs and other open source tools.","title":"Infrastructure"},{"location":"knowledge/stories/blockchain-blog/what-is-matic-network/#dagger","text":"Dagger is a tool or engine to track Ethereum accounts and events in real-time. We can learn more about Dagger here and check how it works at here . Developers can use Dagger to track their own smart contracts, accounts and transactions. They can create custom service or integrate with third-party services through IFTTT or Zapier.","title":"Dagger"},{"location":"knowledge/stories/blockchain-blog/what-is-matic-network/#partners-partial-list","text":"Decentraland : Decentraland is a virtual reality platform powered by the Ethereum blockchain. Users can create, experience, and monetize content and applications using a developer toolkit that works on any platform (all VR headsets and web browsers) and is designed to create virtual environments an applications. Quarkchain : QuarkChain is an innovative permissionless blockchain architecture that aims to provide a secure, decentralized and scalable blockchain solution. Ankr Network : Ankr is a distributed computing platform that aims to leverage idle computing resources in data centres and edge devices. Portis : Portis is a technology company that wants to make sure any person in the world can use decentralized application, just as easily as they use \"regular\" apps. Maker DAO : Maker is the organization behind the DAI stablecoin. Ripio Credit Network : Ripio Credit Network is a peer to peer credit network that connects lenders and borrowsers located anywhere in the world. Get more information at here .","title":"Partners (partial list)"},{"location":"knowledge/stories/computer-science/what-is-different-between-machine-learning-and-ai/","text":"What is different between machine learning and AI Get more informarion at here .","title":"What is between machine learning and AI"},{"location":"knowledge/stories/computer-science/what-is-different-between-machine-learning-and-ai/#what-is-different-between-machine-learning-and-ai","text":"Get more informarion at here .","title":"What is different between machine learning and AI"},{"location":"knowledge/stories/devops-blog/what-is-devops-and-why-do-we-need-it/","text":"What is DevOps and Why Do We Need It? The DevOps methodology has been recognised by many experts as the best way to promote coopertation between development and opertations teams. However DevOpes is much more than a methodology. It is moree of a culture that makes both the tecnical and business sides of software development much more efficient thus reducing time to market and increasing the overall quality of the product. DevOps Definition Shared responsibility - In many compnanies, the development team is focused solely on creating the product and can become disintierest or even estranged operating and managing a system if it is somebody else's job. If it s also part of the develppments teams;s reponsibiliy to monitor the system over the course of its existence, they are likely to feel the same pains operations teams are dealing with. This will elad to finding new ways of simpligying deployments and the maintenance bu automatingthe deployments and better logging. Autonomous teams - Effective collaboration is only possible when development and opertations teams can make decisions and changes independently without a complicated decision-making process. This includes trusting your teams, adjusting the way risks are managed and getting rid of an enviromet where employees are afraid of teking risks or failing. Removing all silos - Some companies document the process and hand this documentation over to another team and condifer this to be regulae cooperation. Such an arrangement is doomed from the start because it promotes a culture of finger pointing and blamling. You must adjust resourcing structure to let operations teams start collaborating with other teams early on. One of the goals of DevOps is to blur the line between development and operations team to the point where there may not be a difference between them at all. The DevOps Lifecycle There are any stages that make up the DevOps lifecycle, but the process will look like the following: Plan - This stage includes initial plainning about how you envision the development process. Code - Coding the applicaiton according to the requirements of the customer. Build - Integrate all of the various codes you have written Releases- If the testing phase was successful, the applicationn could go live Deploy- The code is deployed to a cloud enviroment for additional usage Operate - Conduct the operations on the code. Monitoe - Keep an eye on how well the app is performing and make any changes necessary tostisfy the client. Why Do We Need DevOps? DevOps offers a lot of benefits for software programming companies such as increased innovation speed which will allow you to serve your customers better. You will also be more flexible to adjust to changing market conditions and drive higher business results. Since the speed and tempo of the releases will be frequent, you will be able to improve your product faster. This way you can release new features sooner, notice and fix bugs quicker and perhas most importantly, respond to customer demands better to give your business a competitive advanage. Continuous integrateion and delivery (CI/CD) are practices that automate the software release procss starting from the build and all the way to deployment. Even though the pace of productionn will increase, it hs to be done so reliably. You will lbe able to use CI/CD to test all of the changes and better monitoring logging will help you continuously stay on top of how well your application is doing in real-time. Security process will bot be compromised since the complance policies will be automeated by using fine-grained controls and configuration management techniques. Conclusion You need to have a more structured approach with the understanding that the release schedule might slow down at first as your teams become acquainted the DevOps culture and methodologies. This will require a lot f education, training and som time to get used to all of the changes. As the transition is going on, it is important to set and monitor business critical metrics such as overall revenue and customer satisfaction. There is no point in having multiple releases just for the sake of having various releases. It must make an impact on the bottom line. Given all of the benefits that DevOps offers it is safe to say that companies who fail to implement DevOps process risk falling behind. One off the most common misconceptions is that DevOps is jus for large companies when in fact small and medium-sized business can take advantage of everything DevOps has to offer. In fact, one of the reasons that companies such Facebook, Netflix, and offer adopters of DevOps and were able to deliver new features to their user faster than the competition. Therefore, if you have not yet begun implementing DevOps, you are falling behind your competitors. Get more information at here","title":"What is DevOps? Why do we need it?"},{"location":"knowledge/stories/devops-blog/what-is-devops-and-why-do-we-need-it/#what-is-devops-and-why-do-we-need-it","text":"The DevOps methodology has been recognised by many experts as the best way to promote coopertation between development and opertations teams. However DevOpes is much more than a methodology. It is moree of a culture that makes both the tecnical and business sides of software development much more efficient thus reducing time to market and increasing the overall quality of the product.","title":"What is DevOps and Why Do We Need It?"},{"location":"knowledge/stories/devops-blog/what-is-devops-and-why-do-we-need-it/#devops-definition","text":"Shared responsibility - In many compnanies, the development team is focused solely on creating the product and can become disintierest or even estranged operating and managing a system if it is somebody else's job. If it s also part of the develppments teams;s reponsibiliy to monitor the system over the course of its existence, they are likely to feel the same pains operations teams are dealing with. This will elad to finding new ways of simpligying deployments and the maintenance bu automatingthe deployments and better logging. Autonomous teams - Effective collaboration is only possible when development and opertations teams can make decisions and changes independently without a complicated decision-making process. This includes trusting your teams, adjusting the way risks are managed and getting rid of an enviromet where employees are afraid of teking risks or failing. Removing all silos - Some companies document the process and hand this documentation over to another team and condifer this to be regulae cooperation. Such an arrangement is doomed from the start because it promotes a culture of finger pointing and blamling. You must adjust resourcing structure to let operations teams start collaborating with other teams early on. One of the goals of DevOps is to blur the line between development and operations team to the point where there may not be a difference between them at all.","title":"DevOps Definition"},{"location":"knowledge/stories/devops-blog/what-is-devops-and-why-do-we-need-it/#the-devops-lifecycle","text":"There are any stages that make up the DevOps lifecycle, but the process will look like the following: Plan - This stage includes initial plainning about how you envision the development process. Code - Coding the applicaiton according to the requirements of the customer. Build - Integrate all of the various codes you have written Releases- If the testing phase was successful, the applicationn could go live Deploy- The code is deployed to a cloud enviroment for additional usage Operate - Conduct the operations on the code. Monitoe - Keep an eye on how well the app is performing and make any changes necessary tostisfy the client.","title":"The DevOps Lifecycle"},{"location":"knowledge/stories/devops-blog/what-is-devops-and-why-do-we-need-it/#why-do-we-need-devops","text":"DevOps offers a lot of benefits for software programming companies such as increased innovation speed which will allow you to serve your customers better. You will also be more flexible to adjust to changing market conditions and drive higher business results. Since the speed and tempo of the releases will be frequent, you will be able to improve your product faster. This way you can release new features sooner, notice and fix bugs quicker and perhas most importantly, respond to customer demands better to give your business a competitive advanage. Continuous integrateion and delivery (CI/CD) are practices that automate the software release procss starting from the build and all the way to deployment. Even though the pace of productionn will increase, it hs to be done so reliably. You will lbe able to use CI/CD to test all of the changes and better monitoring logging will help you continuously stay on top of how well your application is doing in real-time. Security process will bot be compromised since the complance policies will be automeated by using fine-grained controls and configuration management techniques.","title":"Why Do We Need DevOps?"},{"location":"knowledge/stories/devops-blog/what-is-devops-and-why-do-we-need-it/#conclusion","text":"You need to have a more structured approach with the understanding that the release schedule might slow down at first as your teams become acquainted the DevOps culture and methodologies. This will require a lot f education, training and som time to get used to all of the changes. As the transition is going on, it is important to set and monitor business critical metrics such as overall revenue and customer satisfaction. There is no point in having multiple releases just for the sake of having various releases. It must make an impact on the bottom line. Given all of the benefits that DevOps offers it is safe to say that companies who fail to implement DevOps process risk falling behind. One off the most common misconceptions is that DevOps is jus for large companies when in fact small and medium-sized business can take advantage of everything DevOps has to offer. In fact, one of the reasons that companies such Facebook, Netflix, and offer adopters of DevOps and were able to deliver new features to their user faster than the competition. Therefore, if you have not yet begun implementing DevOps, you are falling behind your competitors. Get more information at here","title":"Conclusion"},{"location":"knowledge/stories/docker-labs/overview/","text":"A Docker Captain's Blog Welcome to DockerLabs Are you new to Docker? Looking out for building Your Career in DevOps & Containers Technology? Welcome! You are at the right place. DockerLabs brings you tutorials that help you get hands-on experience using Docker & Kubernetes. Here you woll find complete documentation of labs and tutorials that will help you, no matter if you are a beginner, SysAdmin, IT Pro or Developer. Yes, you read it right! Its \\$0 learning platform. You don't need any infrastructure. Most of the tutorials runs on Play with Docker Platform & Play with Kubernetes Platform . This is a free browser based learning platform for you. Docker tools like Docker Engine, Docker Compose & Docker Machine are already installed. Hence, we have everything ready for you to get started with. Get more information at here .","title":"A Docker Captain's Blog"},{"location":"knowledge/stories/docker-labs/overview/#a-docker-captains-blog","text":"","title":"A Docker Captain's Blog"},{"location":"knowledge/stories/docker-labs/overview/#welcome-to-dockerlabs","text":"Are you new to Docker? Looking out for building Your Career in DevOps & Containers Technology? Welcome! You are at the right place. DockerLabs brings you tutorials that help you get hands-on experience using Docker & Kubernetes. Here you woll find complete documentation of labs and tutorials that will help you, no matter if you are a beginner, SysAdmin, IT Pro or Developer. Yes, you read it right! Its \\$0 learning platform. You don't need any infrastructure. Most of the tutorials runs on Play with Docker Platform & Play with Kubernetes Platform . This is a free browser based learning platform for you. Docker tools like Docker Engine, Docker Compose & Docker Machine are already installed. Hence, we have everything ready for you to get started with. Get more information at here .","title":"Welcome to DockerLabs"},{"location":"knowledge/stories/skill-blog/3-things-to-do-when-you-dont-have-computers-science-degree/","text":"3 things to do when you have a computer science degree If you are looking for a software engineering job, but you don't have a degree in computer science, you should: Find other ways to demonstrate your expertise. Network,network,network. Practice for white board interviews(or other types of tecnical interviews) as if your life depended on it. Find other ways to demonstrate your expertise. Complete a relevent program on freeCodeCamp . Take courses on SoloLearn . If the curriculum at a coding bootcamp aligns with your goals, and if you can pull it off financialy, consider it. Understand, though, that while you might learn to code, you won't get a comprehensive education in computer science. Develop a well-rounded portfolio. Contribute to open-source projects on Github. Develop an online presence. Network, network, network Paricipate in challenges and show off your projects. Connect with recruiters. Connect with people who are doing exactly what you wish to be doing. Get feedback on your resume and portfolio. Treat your profile like a portfolio. Keep it updated. Go to meetups and hackathons. You'll meet prople who can guide you in the right direction. You'll develop portfolio pieces that you can talk about at interviews. Write. Writing is another underrated form of networking. When you publish somthing, your work out there for recruiters and industry leaders to see. You'll connect with others in your industry. Talk with recruters, even if they don't have position for your right now. Recruiters are able to get your resume and portfolio into the right hands, especially if a company's system is filtering our resumes from people without degrees. They can and will call you later on if a job matching your qualifications pops up. They can and will advocate on behalf of string candidates. Recruiters can give you feedback on your resume, your portfolio, and your other qualifications before they share it with hiring managers. Practice for interviews as if your life depended on it. If white board interviews are in your future, get on LeetCode and start practicing. If another form of technical interview is in your future, find our exactly what you'll be asked to do, and prepare. Get more information at here .","title":"3 Things to do when you don't have computer science degree"},{"location":"knowledge/stories/skill-blog/3-things-to-do-when-you-dont-have-computers-science-degree/#3-things-to-do-when-you-have-a-computer-science-degree","text":"","title":"3 things to do when you have a computer science degree"},{"location":"knowledge/stories/skill-blog/3-things-to-do-when-you-dont-have-computers-science-degree/#if-you-are-looking-for-a-software-engineering-job-but-you-dont-have-a-degree-in-computer-science-you-should","text":"Find other ways to demonstrate your expertise. Network,network,network. Practice for white board interviews(or other types of tecnical interviews) as if your life depended on it.","title":"If you are looking for a software engineering job, but you don't have a degree in computer science, you should:"},{"location":"knowledge/stories/skill-blog/3-things-to-do-when-you-dont-have-computers-science-degree/#find-other-ways-to-demonstrate-your-expertise","text":"Complete a relevent program on freeCodeCamp . Take courses on SoloLearn . If the curriculum at a coding bootcamp aligns with your goals, and if you can pull it off financialy, consider it. Understand, though, that while you might learn to code, you won't get a comprehensive education in computer science. Develop a well-rounded portfolio. Contribute to open-source projects on Github. Develop an online presence.","title":"Find other ways to demonstrate your expertise."},{"location":"knowledge/stories/skill-blog/3-things-to-do-when-you-dont-have-computers-science-degree/#network-network-network","text":"Paricipate in challenges and show off your projects. Connect with recruiters. Connect with people who are doing exactly what you wish to be doing. Get feedback on your resume and portfolio. Treat your profile like a portfolio. Keep it updated. Go to meetups and hackathons. You'll meet prople who can guide you in the right direction. You'll develop portfolio pieces that you can talk about at interviews. Write. Writing is another underrated form of networking. When you publish somthing, your work out there for recruiters and industry leaders to see. You'll connect with others in your industry. Talk with recruters, even if they don't have position for your right now. Recruiters are able to get your resume and portfolio into the right hands, especially if a company's system is filtering our resumes from people without degrees. They can and will call you later on if a job matching your qualifications pops up. They can and will advocate on behalf of string candidates. Recruiters can give you feedback on your resume, your portfolio, and your other qualifications before they share it with hiring managers.","title":"Network, network, network"},{"location":"knowledge/stories/skill-blog/3-things-to-do-when-you-dont-have-computers-science-degree/#practice-for-interviews-as-if-your-life-depended-on-it","text":"If white board interviews are in your future, get on LeetCode and start practicing. If another form of technical interview is in your future, find our exactly what you'll be asked to do, and prepare. Get more information at here .","title":"Practice for interviews as if your life depended on it."},{"location":"knowledge/stories/skill-blog/beyond-coding-soft-skill-to-avoid-projects-fails/","text":"Beyond coding- Soft skill to avoid projects failures. Get more information at here .","title":"Beyond Coding - Soft skill to avoid projects failures"},{"location":"knowledge/stories/skill-blog/beyond-coding-soft-skill-to-avoid-projects-fails/#beyond-coding-soft-skill-to-avoid-projects-failures","text":"Get more information at here .","title":"Beyond coding- Soft skill to avoid projects failures."},{"location":"knowledge/stories/skill-blog/code-and-life/","text":"Code and Life Get more information at here .","title":"Code and Life"},{"location":"knowledge/stories/skill-blog/code-and-life/#code-and-life","text":"Get more information at here .","title":"Code and Life"},{"location":"knowledge/stories/skill-blog/how-code-reviews-work-at-microsoft/","text":"How code reviews work at Microsoft Get more information at here .","title":"How code reviews work at Microsoft"},{"location":"knowledge/stories/skill-blog/how-code-reviews-work-at-microsoft/#how-code-reviews-work-at-microsoft","text":"Get more information at here .","title":"How code reviews work at Microsoft"},{"location":"knowledge/stories/skill-blog/how-to-be-a-great-programmer/","text":"How to be a great programmer Problem Solver Extraordinaire Although therre are many ways to solve a problem, there are a few parts of the process that stand out to me. Programmers who are also great problem solvers distell a problem to its essence, in order to identify their overall aim and begin a problem with purpose. Then, they break each problem into small, manageable parts- attacking each part in turn, and sometimes in visual terms by drawing a picture to mke it \"real world\". Things began to change when I began learning about the problem solving process, and how to problem solve effectively. I now begin a problem with intent. I have Geoge Polya's book, How to Solve It , to thank for that bit of advice. I've adapted some of Polya's ideas to programming, like understanding the problem. \"The problem must be understood,\" Polya writes. This includes being able to \"point out the principal parts of the problem, the unknown, the data and the condition.\" For each problem, I pull out a sheet of paper and weite answers to these questions: what am I solving for or trying to find? (unknown); what am I given?(data); and what constraints or details do I need to be aware of? (condition). Writing out problem details slows me doen metall, and helps me think through exactly what I need to do, which is half of the battle. There are many ways to go about this. Sometimes I outline the steps I need to take in numerical order: first do this , second do that. Other times I make the problem visual. I'll also draw pictures or diagrams. For a recursive problem, I'll draw a diagram of what's happending on each recursive call until I hit the base case. Almost always, however, I find a way to simplify the problem to make ti more manageabl and to help me spot a pattern. Above all, the aim for me is to enter a problem with purpose, and maintain that sense of purpose throughout. What about your Computer? Learning computer science is the second programming funcdametal. I recently started learning computer science and love it because I'm moving beyond surface level. I'm going \"behind the scenes\" to learn what happens when I use a built-in funcrion, for example. I'm also learning about memory and run time, among many other topics. in short, I'm learning why a computer does the things it does. Knowing the \"why\" enhances my contextual knowledge and makes me a more informed programmer. It's also enriching my understanding of how core programming concepts work. Programmers who master the fundamentals seem to code with confidence: thay know the \"how\" and \"why\" of their programming choices, which improve their work and builds their credibility with others. Get more information at here","title":"How to be a great programmer"},{"location":"knowledge/stories/skill-blog/how-to-be-a-great-programmer/#how-to-be-a-great-programmer","text":"","title":"How to be a great programmer"},{"location":"knowledge/stories/skill-blog/how-to-be-a-great-programmer/#problem-solver-extraordinaire","text":"Although therre are many ways to solve a problem, there are a few parts of the process that stand out to me. Programmers who are also great problem solvers distell a problem to its essence, in order to identify their overall aim and begin a problem with purpose. Then, they break each problem into small, manageable parts- attacking each part in turn, and sometimes in visual terms by drawing a picture to mke it \"real world\". Things began to change when I began learning about the problem solving process, and how to problem solve effectively. I now begin a problem with intent. I have Geoge Polya's book, How to Solve It , to thank for that bit of advice. I've adapted some of Polya's ideas to programming, like understanding the problem. \"The problem must be understood,\" Polya writes. This includes being able to \"point out the principal parts of the problem, the unknown, the data and the condition.\" For each problem, I pull out a sheet of paper and weite answers to these questions: what am I solving for or trying to find? (unknown); what am I given?(data); and what constraints or details do I need to be aware of? (condition). Writing out problem details slows me doen metall, and helps me think through exactly what I need to do, which is half of the battle. There are many ways to go about this. Sometimes I outline the steps I need to take in numerical order: first do this , second do that. Other times I make the problem visual. I'll also draw pictures or diagrams. For a recursive problem, I'll draw a diagram of what's happending on each recursive call until I hit the base case. Almost always, however, I find a way to simplify the problem to make ti more manageabl and to help me spot a pattern. Above all, the aim for me is to enter a problem with purpose, and maintain that sense of purpose throughout.","title":"Problem Solver Extraordinaire"},{"location":"knowledge/stories/skill-blog/how-to-be-a-great-programmer/#what-about-your-computer","text":"Learning computer science is the second programming funcdametal. I recently started learning computer science and love it because I'm moving beyond surface level. I'm going \"behind the scenes\" to learn what happens when I use a built-in funcrion, for example. I'm also learning about memory and run time, among many other topics. in short, I'm learning why a computer does the things it does. Knowing the \"why\" enhances my contextual knowledge and makes me a more informed programmer. It's also enriching my understanding of how core programming concepts work. Programmers who master the fundamentals seem to code with confidence: thay know the \"how\" and \"why\" of their programming choices, which improve their work and builds their credibility with others. Get more information at here","title":"What about your Computer?"},{"location":"knowledge/stories/skill-blog/how-to-debug-with-firefox/","text":"How to debug web apps with firefox developer tools By knowing how to debug your web app, you can leverage developer tools to hack your productivity. After reading this article, you'll be able to: debug mobile and tablet version of your web application on your computer. connect the mobile Firefox application to your computer and debug web applications there. find the fonts and styles that best fit your website withour having to build your CSS and JS code again and again. fin problems in your CSS layout. Finally, at the end - I am going to tell yu how to install the powerful Firefox Developer Edition . Get more information at here .","title":"How to debug web apps with Firefox developer tools"},{"location":"knowledge/stories/skill-blog/how-to-debug-with-firefox/#how-to-debug-web-apps-with-firefox-developer-tools","text":"By knowing how to debug your web app, you can leverage developer tools to hack your productivity. After reading this article, you'll be able to: debug mobile and tablet version of your web application on your computer. connect the mobile Firefox application to your computer and debug web applications there. find the fonts and styles that best fit your website withour having to build your CSS and JS code again and again. fin problems in your CSS layout. Finally, at the end - I am going to tell yu how to install the powerful Firefox Developer Edition . Get more information at here .","title":"How to debug web apps with firefox developer tools"},{"location":"knowledge/stories/skill-blog/introducing-net-5/","text":"Introducing .NET 5 Get more information at here .","title":"Introducing .NET 5"},{"location":"knowledge/stories/skill-blog/introducing-net-5/#introducing-net-5","text":"Get more information at here .","title":"Introducing .NET 5"},{"location":"knowledge/stories/skill-blog/net-core-is-future-of-net/","text":".NET Core is future of .NET Get more information at here .","title":".NET Core is Future of .NET"},{"location":"knowledge/stories/skill-blog/net-core-is-future-of-net/#net-core-is-future-of-net","text":"Get more information at here .","title":".NET Core is future of .NET"},{"location":"knowledge/stories/skill-blog/net-stacks/","text":".NET Technology Stacks for Windows Desktop Development In this article, we're going to take a look at the following technologies: Win32 and COM WinForms Windows Presentation Framework (WPF) Universal Windows Platform (UWP) Electron(.NET) Avalonia Win32 and COM Win32 has been introduced in late 1995 with the release of Windows 95 and is available in all following Windows releases including Windows 10. Modernizing an applixation written using C++ and the Win32 APIs is a hard task. I do not know about anything alse than rewriting the entire application using another technology stack. Most newer technologies require either Visual Basic .NET or more often C# as the programming language. Windows Forms(WinForms) WinForms is the Windows desktip technology stack introduced with the .NET Framework in 2002 to make desktop development much simpler. After about 15 yearsof closed sorce development, Microsoft announced to open-source WinForms on December 4th, 2018. WinForms allows the developer to get results quickly. Visual Studio has a built-in editor which enables us to drag and drop controls from the toolbar onto the dialogs. By double-clicking onto the controls on the dialog, Visual Studio generated click handlers in the code which allow us to write the application logic. The advantages of WinForms are obvious: Rapid prototying, gaining fast results and the opportunity to work with a graphical editor in Visual Studio to crete the use interfaces. WinForms apps run on all Windows computers with an installed .NET Framework. The downsides of WinForms are the limiteddesign options, the close relationship with Visual Studio which makes application development outside Visual Studio nearly impossible, and generated code which can be a pain using version control systems when multiple developers collaborate on the smae project. Windows Presentation Framework (WPF) With the release of .NET Framework 3.0 in 2006 Microsoft introduced the WPF as an alternative to WinForms for Windows desktop development. The main difference between Windows and WPF is the design language. For WinForms apps, we use the graphicla designer, and for WPF applixations we use XAML as a markup language to describe our user interfaces. The advantages are obvious. Using an XML-like languages developer have full control over the structure of their user interfaces. Custom designs were enabled by WPF and implementing custom controls to build richer user exeriences became possible. Using the data-binding mechanism and utilizing the MVVM design pattern WPF applications allow develipers to split their code between user interface design code and business logic. Application logic can and should be written in class libraries with no dependency on any user interface framework. It allows for more maintainable and reusable code. On the other side, development becomes much more demanding and requires more knowledge. Gaining results can be hard in the beginning, and there are always multiple ways to solve a prblem. WinForms is often limited by forces you to a more straightforward solution on the other side. Universal Windows Platform UWP applications aer a modern alternative to WPF and WinForms applixations. UWP replaces the Windows Runtime (WinRT) and requires Windows 10. UWP aps can be developed either in C# using XAML which feels a lot like writing a WPF application (with a few restrictions) or using JS and HTML. Electron (.NET) Electron allows developing desktop applixations with web technologies (HTML, CSS, and JS). Electron runs your applixation on Chromium as a node application with means that electron opens a browser window and lets you run your web application similar to a native desktop application. The advantage is theat Electron applications are cross-platform and also run on iOS and Linux. On the downside, we have heavy CPU and RAM usage because the behind the scene running Chromium process consumes a lot of resources. Electron.NET allows us to develop Electron applications using ASP.NET Core. We can write C# and have the same APIs as Electron offers to JavaScript developers. In general, Electron.NET coms with the same advantages and disadvantages compared to Electron. Avalonia Avalonia is a cross-platform XAML Framework for .NET Framework, .NET Core and Mono. Avalonia has a similar look like WPF or UWP interface definitions because it utilizes a XAML dialect for the view definitions. It also supports MVVM, data-binding and much more. Avalonia is new which means that they do not have a lot of legacy code in their framework. Unlike WPF where there is code older than a decade that cannot be removed because of compatibility issues. Get more information at here .","title":".NET stacks"},{"location":"knowledge/stories/skill-blog/net-stacks/#net-technology-stacks-for-windows-desktop-development","text":"In this article, we're going to take a look at the following technologies: Win32 and COM WinForms Windows Presentation Framework (WPF) Universal Windows Platform (UWP) Electron(.NET) Avalonia","title":".NET Technology Stacks for Windows Desktop Development"},{"location":"knowledge/stories/skill-blog/net-stacks/#win32-and-com","text":"Win32 has been introduced in late 1995 with the release of Windows 95 and is available in all following Windows releases including Windows 10. Modernizing an applixation written using C++ and the Win32 APIs is a hard task. I do not know about anything alse than rewriting the entire application using another technology stack. Most newer technologies require either Visual Basic .NET or more often C# as the programming language.","title":"Win32 and COM"},{"location":"knowledge/stories/skill-blog/net-stacks/#windows-formswinforms","text":"WinForms is the Windows desktip technology stack introduced with the .NET Framework in 2002 to make desktop development much simpler. After about 15 yearsof closed sorce development, Microsoft announced to open-source WinForms on December 4th, 2018. WinForms allows the developer to get results quickly. Visual Studio has a built-in editor which enables us to drag and drop controls from the toolbar onto the dialogs. By double-clicking onto the controls on the dialog, Visual Studio generated click handlers in the code which allow us to write the application logic. The advantages of WinForms are obvious: Rapid prototying, gaining fast results and the opportunity to work with a graphical editor in Visual Studio to crete the use interfaces. WinForms apps run on all Windows computers with an installed .NET Framework. The downsides of WinForms are the limiteddesign options, the close relationship with Visual Studio which makes application development outside Visual Studio nearly impossible, and generated code which can be a pain using version control systems when multiple developers collaborate on the smae project.","title":"Windows Forms(WinForms)"},{"location":"knowledge/stories/skill-blog/net-stacks/#windows-presentation-framework-wpf","text":"With the release of .NET Framework 3.0 in 2006 Microsoft introduced the WPF as an alternative to WinForms for Windows desktop development. The main difference between Windows and WPF is the design language. For WinForms apps, we use the graphicla designer, and for WPF applixations we use XAML as a markup language to describe our user interfaces. The advantages are obvious. Using an XML-like languages developer have full control over the structure of their user interfaces. Custom designs were enabled by WPF and implementing custom controls to build richer user exeriences became possible. Using the data-binding mechanism and utilizing the MVVM design pattern WPF applications allow develipers to split their code between user interface design code and business logic. Application logic can and should be written in class libraries with no dependency on any user interface framework. It allows for more maintainable and reusable code. On the other side, development becomes much more demanding and requires more knowledge. Gaining results can be hard in the beginning, and there are always multiple ways to solve a prblem. WinForms is often limited by forces you to a more straightforward solution on the other side.","title":"Windows Presentation Framework (WPF)"},{"location":"knowledge/stories/skill-blog/net-stacks/#universal-windows-platform","text":"UWP applications aer a modern alternative to WPF and WinForms applixations. UWP replaces the Windows Runtime (WinRT) and requires Windows 10. UWP aps can be developed either in C# using XAML which feels a lot like writing a WPF application (with a few restrictions) or using JS and HTML.","title":"Universal Windows Platform"},{"location":"knowledge/stories/skill-blog/net-stacks/#electron-net","text":"Electron allows developing desktop applixations with web technologies (HTML, CSS, and JS). Electron runs your applixation on Chromium as a node application with means that electron opens a browser window and lets you run your web application similar to a native desktop application. The advantage is theat Electron applications are cross-platform and also run on iOS and Linux. On the downside, we have heavy CPU and RAM usage because the behind the scene running Chromium process consumes a lot of resources. Electron.NET allows us to develop Electron applications using ASP.NET Core. We can write C# and have the same APIs as Electron offers to JavaScript developers. In general, Electron.NET coms with the same advantages and disadvantages compared to Electron.","title":"Electron (.NET)"},{"location":"knowledge/stories/skill-blog/net-stacks/#avalonia","text":"Avalonia is a cross-platform XAML Framework for .NET Framework, .NET Core and Mono. Avalonia has a similar look like WPF or UWP interface definitions because it utilizes a XAML dialect for the view definitions. It also supports MVVM, data-binding and much more. Avalonia is new which means that they do not have a lot of legacy code in their framework. Unlike WPF where there is code older than a decade that cannot be removed because of compatibility issues. Get more information at here .","title":"Avalonia"},{"location":"knowledge/stories/skill-blog/roadmap-web-developer-2019/","text":"Roadmap web developer in 2019 The purpose of these roadmap is to give you an idea about the landscape and to guide you if you are confused about what to learn nexet and not to encourage you to pick what is hip and trendy. You should grow some understanding of why one tool would better be suited for some cases than the other and remember hip and trendy never means beast suited for the job. FRONT-END Roadmap BACK-END Roadmap DEV-OPS Roadmap Get more information at here .","title":"Roadmap Web Developer 2019"},{"location":"knowledge/stories/skill-blog/roadmap-web-developer-2019/#roadmap-web-developer-in-2019","text":"The purpose of these roadmap is to give you an idea about the landscape and to guide you if you are confused about what to learn nexet and not to encourage you to pick what is hip and trendy. You should grow some understanding of why one tool would better be suited for some cases than the other and remember hip and trendy never means beast suited for the job.","title":"Roadmap web developer in 2019"},{"location":"knowledge/stories/skill-blog/roadmap-web-developer-2019/#front-end-roadmap","text":"","title":"FRONT-END Roadmap"},{"location":"knowledge/stories/skill-blog/roadmap-web-developer-2019/#back-end-roadmap","text":"","title":"BACK-END Roadmap"},{"location":"knowledge/stories/skill-blog/roadmap-web-developer-2019/#dev-ops-roadmap","text":"Get more information at here .","title":"DEV-OPS Roadmap"},{"location":"knowledge/stories/skill-blog/scraping-the-web-with-nodejs/","text":"Scraping the web with nodejs Get more information at here .","title":"Scraping Web With Nodejs"},{"location":"knowledge/stories/skill-blog/scraping-the-web-with-nodejs/#scraping-the-web-with-nodejs","text":"Get more information at here .","title":"Scraping the web with nodejs"},{"location":"knowledge/stories/skill-blog/tinygo/","text":"TinyGo - A go compiler for small places TinyGo is a proect ro bring the Go programming language to microcontrollers and modern web browsers by creating a new compiler based on LLVM . You can compile and run TinyGo programs on several different microcontrller boards such as the BBC micro:bit and the Arduino Uno. TinyGo can also be used to produce WebAssembly (WASM) code which is very compact in size. Just want to see the code? Go to the Github repository at https://github.com/tinygo-org/tinygo TinyGo also has support for several different devices such as accelerometers and magnetometers. Check out the Github repository at https:// github.com/tingo-org/drivers for more information. Get more information at here .","title":"Tiny Go"},{"location":"knowledge/stories/skill-blog/tinygo/#tinygo-a-go-compiler-for-small-places","text":"TinyGo is a proect ro bring the Go programming language to microcontrollers and modern web browsers by creating a new compiler based on LLVM . You can compile and run TinyGo programs on several different microcontrller boards such as the BBC micro:bit and the Arduino Uno. TinyGo can also be used to produce WebAssembly (WASM) code which is very compact in size. Just want to see the code? Go to the Github repository at https://github.com/tinygo-org/tinygo TinyGo also has support for several different devices such as accelerometers and magnetometers. Check out the Github repository at https:// github.com/tingo-org/drivers for more information. Get more information at here .","title":"TinyGo - A go compiler for small places"},{"location":"knowledge/stories/skill-blog/what-i-learn-from-lauching-a-freelance-writing-bussiness/","text":"What I learn from lauching a freelance writing business Get more information at here .","title":"What I learn from lauching a freelance writing business"},{"location":"knowledge/stories/skill-blog/what-i-learn-from-lauching-a-freelance-writing-bussiness/#what-i-learn-from-lauching-a-freelance-writing-business","text":"Get more information at here .","title":"What I learn from lauching a freelance writing business"},{"location":"knowledge/stories/skill-blog/what-javascript-developers-learn-from-c++/","text":"What JavaScript Developer can learn from C++ Get more information at here .","title":"What Javascript Developers can learn from C++"},{"location":"knowledge/stories/skill-blog/what-javascript-developers-learn-from-c++/#what-javascript-developer-can-learn-from-c","text":"Get more information at here .","title":"What JavaScript Developer can learn from C++"},{"location":"knowledge/stories/thinking-in-art/4-practical-things-i-do-to-stay-positive/","text":"4 Practical things I do to stay positive Not blaming others nor yourself is one of the most important things if you want to stay positive. But there are also 4 other things I do to stay positive. Here they are: 1. Practice garatitude Taking a few moments every day to write down a few things your grateful for changes your perspactive. The funny thing about this practice is that when you read about it, you think \"It can't be THAT effective!\". Remmber that you can also be gratefull for things that you didn't ask for Be gratefull for everything - even the bed things 2. Don't take it personally Well, maybe it's time to lighten up a bit. We overanalyze every single thing people say and do. Sometimes, prople just to do dumb things. Not everything people do is personal. If you want to live a good lige, you can't take everytthing peronally. Understand that there are more important things to life - things like your health, family and career. 3. Let it out We all deal with fifficult thins in our lives. Life is basically one challenge after the other. We can't let that get the best of us. That's whay we need to let it all out. All your anger, frustration, anxiety, insecurity - it needs to get our of your system. Whether you do that by journaling, tailking to a friend, or going to therapy is your choice. Just make sure you don't keep all those things inside yourself. 4. Focus on improvement Don't send one second on feeling sorry for yourseld and focus on what you can do NOW to improve something about your life. It doesn't even matter what it is. got for a walk. take a shower, Shave, Start a journal . Fix something in your house. Write down some ideas. Set a goal . You can even do those things while you're feeling bad. The author of the famous self-help book. Think And Grow Rich , put it: If you cannot do great things, do small things in a great way. It's unquestionable: Doing small things will make you feel better. But you don't need to believe me. Just go and do small thing now and see it for yourself. Get more information at here .","title":"4 Practical Things I do to stay positive"},{"location":"knowledge/stories/thinking-in-art/4-practical-things-i-do-to-stay-positive/#4-practical-things-i-do-to-stay-positive","text":"Not blaming others nor yourself is one of the most important things if you want to stay positive. But there are also 4 other things I do to stay positive. Here they are:","title":"4 Practical things I do to stay positive"},{"location":"knowledge/stories/thinking-in-art/4-practical-things-i-do-to-stay-positive/#1-practice-garatitude","text":"Taking a few moments every day to write down a few things your grateful for changes your perspactive. The funny thing about this practice is that when you read about it, you think \"It can't be THAT effective!\". Remmber that you can also be gratefull for things that you didn't ask for Be gratefull for everything - even the bed things","title":"1. Practice garatitude"},{"location":"knowledge/stories/thinking-in-art/4-practical-things-i-do-to-stay-positive/#2-dont-take-it-personally","text":"Well, maybe it's time to lighten up a bit. We overanalyze every single thing people say and do. Sometimes, prople just to do dumb things. Not everything people do is personal. If you want to live a good lige, you can't take everytthing peronally. Understand that there are more important things to life - things like your health, family and career.","title":"2. Don't take it personally"},{"location":"knowledge/stories/thinking-in-art/4-practical-things-i-do-to-stay-positive/#3-let-it-out","text":"We all deal with fifficult thins in our lives. Life is basically one challenge after the other. We can't let that get the best of us. That's whay we need to let it all out. All your anger, frustration, anxiety, insecurity - it needs to get our of your system. Whether you do that by journaling, tailking to a friend, or going to therapy is your choice. Just make sure you don't keep all those things inside yourself.","title":"3. Let it out"},{"location":"knowledge/stories/thinking-in-art/4-practical-things-i-do-to-stay-positive/#4-focus-on-improvement","text":"Don't send one second on feeling sorry for yourseld and focus on what you can do NOW to improve something about your life. It doesn't even matter what it is. got for a walk. take a shower, Shave, Start a journal . Fix something in your house. Write down some ideas. Set a goal . You can even do those things while you're feeling bad. The author of the famous self-help book. Think And Grow Rich , put it: If you cannot do great things, do small things in a great way. It's unquestionable: Doing small things will make you feel better. But you don't need to believe me. Just go and do small thing now and see it for yourself. Get more information at here .","title":"4. Focus on improvement"},{"location":"knowledge/stories/thinking-in-art/how-to-get-back-on-track/","text":"How to get back on track Get more information at here .","title":"How to get back on track"},{"location":"knowledge/stories/thinking-in-art/how-to-get-back-on-track/#how-to-get-back-on-track","text":"Get more information at here .","title":"How to get back on track"},{"location":"knowledge/stories/thinking-in-art/how-to-quit-your-job-and-live-your-startup-dream/","text":"How to quit your job and live your startup dream There is. And it is a 4-step process called Q.U.I.T. Q - Question your Passion U - Understand Your Reputation I - Investigate your skills T - Test the waters Q - Question Your Passion The 1st step is to firm your passion and confirm whether it is \" true \" or \" false \". These are some of the ways in which you can do it. Is your Passion your own or imposed one? If you are just thinking of \" copying \", then you are on the wrong path here. You need to know \" what you want to do \" and \" what is your potential \". That i the true test of your passion. Is your Passion a stop-gap arrangement or the End Story? You believe that once your point is \"proven\", you can again go back to the cushy comforts for your job. You can never be more wrong than this A passion has to be the end goal. It can ever be a \"stop-gap\" arrangement. If you treat it like that, your startup is already \" finished \" before it hs started. Is your Passionn only a Passing Fad? Technology Trends are like Fashion. They come and go every day and there is constant disruption. But amidst all this disruption, the one thing that remains constant is innovation. Startups which have the mettle to constantly innovate and stay ahead of the disruption curve survive the game. Others just wither and die. You need to ask yourself an important question here: Are you a chaser or a changer? A casher just cashes on the current market trend, makes some money and exits. There is no real passion involved here. A changer on the other hand, contantly \" reworks \" his product and stays ahead in the game. Your passionn is a \" true \" if it is that of a changer. U - Undrstand your Reputation Once you have confirmed your passion. The next step is to understand the power of your reputation. Your reputation needs to be Steller enough to shoulder the burden of your unknown startup. Some of the ways you can do it can be as below. Is your reputation self-made or company made? You fell your outstanding reputation is the key to help you find customers for your startup. Now just go back a step and think abour your \"reputation\" your personal reptationn should always be based on your unique abilities and talents independent of \"company provided\" facilities or processes. Your \"potential\" customer should be able to recognize this ability in you to proceed further. Their \"belief\" in you should be absolute whether you are working for a big company or your own startup. How Trustworthy are your contacts? Analyze and categorie your contacts into 3 groups. High peofile contact: approachable for \"business\" High profile contact: not approachable for \"business\" Casual contacts You should have at least 40% of your contacts approachable for \"business\" to make a good start. This will not only help you to give that initial tractiono but will also build up an early stellar reputation of your startup. Always remember making a great product is not enough. You also nees \"customers\" who can vouch for the product based on your \"reputation\". I - Investigate your Skills One your passion and reputation are proved beyond doubt, we come to the meat of the problem-the skills you have to take your startup forward. We require three types of Skills here. Core Skills Core Skills are your \" bread and butter \" of your start-up. They can range from anything between AI, robotics to our boring PHP. These tecnical skills are the foundation for the great product that you are building and it is important that you and your co-founders have \" near-perfect \" expertise in these skills. Do a through audit of your core skills and map them on a scale between 1 to 10 Majority of the skills going above 8 would mean, you are good to go. Management Skills These are soft skills like leadership, team building, resourcing, and negotiations. But these are very very important. So if you have worked all your life in building great technical stuff and never had the \"experience\" to manage a project or a team, you might have a serious handicap here. In that case, you need to get these complementary skills from your co-founder. Everything from ideas to finances will be hazy during the initial days of the startup and it is very important to have strong management skills to optimally use the meager resources and get maximum out of them. So never under estimate its importance. Peripheral Skills You need to take care of these fictions also and it is important for you to get a deep-dive understanding of these functions to perform effectively. These \" supporting \" functions have the potential to make or break your startup so you need to get yourseld trained in these areas also. It will be great if you can find these skills as complementary in your co-founders. T- Test the waters Now if you have reached so far successfully, it is time to test the waters. We need to utilize all \" approachable \" contacts whom we had identified earlier. What do the \"Feelers\" say? This is where \"feelers\" can help you. A feeler is just a brief description of the idea along with the top 2-3 potential benefits. This can be either an email, a short presentation of even a casual discussionn at a coffee shop. The basic purpose to get a \"feel\" of the customer's mind before proceeding forward. Care should be taken here only to reveal the \" what you are doing \" part keeping the \" how \" part in vague details. if 70% or more of your contacs show interest in your idea, you are on the right path. Show a Demo It is worth investing time and effort if the \"potential\" contract is going to become a \"confirmed\" customer. Such customers will be visibly exvited by your idea and cannot wait till they see it \"working\". you can plan a prototype of \"limited version\" for these customers which just showcases the meat of your product. This will not only build the customer confidence but will also build the confidence of your own team and give them \"something\" tangible to start working with. These demos are highly useful for negotiations with angel investors also. Bringing it all together Many of us were raised with a belief that we can be anything we want to be and fed with the idea that if we pursue our passion, money will follow. Most of us buy this mantra, but in real life, things are never as simple as they seem. Real life requires retionality, logic and perennial inspection and improvement of our own self. And one of the more irresponsible interpretatios of \" following your dreams \" is the assumptionn that your happiness depends on it. Never equate happiness with passion. You are entering into an unending loop of frustraion and despair by doing that. Get more information at here","title":"How to quit your job and live your startup dream"},{"location":"knowledge/stories/thinking-in-art/how-to-quit-your-job-and-live-your-startup-dream/#how-to-quit-your-job-and-live-your-startup-dream","text":"There is. And it is a 4-step process called Q.U.I.T. Q - Question your Passion U - Understand Your Reputation I - Investigate your skills T - Test the waters","title":"How to quit your job and live your startup dream"},{"location":"knowledge/stories/thinking-in-art/how-to-quit-your-job-and-live-your-startup-dream/#q-question-your-passion","text":"The 1st step is to firm your passion and confirm whether it is \" true \" or \" false \". These are some of the ways in which you can do it.","title":"Q - Question Your Passion"},{"location":"knowledge/stories/thinking-in-art/how-to-quit-your-job-and-live-your-startup-dream/#is-your-passion-your-own-or-imposed-one","text":"If you are just thinking of \" copying \", then you are on the wrong path here. You need to know \" what you want to do \" and \" what is your potential \". That i the true test of your passion.","title":"Is your Passion your own or imposed one?"},{"location":"knowledge/stories/thinking-in-art/how-to-quit-your-job-and-live-your-startup-dream/#is-your-passion-a-stop-gap-arrangement-or-the-end-story","text":"You believe that once your point is \"proven\", you can again go back to the cushy comforts for your job.","title":"Is your Passion a stop-gap arrangement or the End Story?"},{"location":"knowledge/stories/thinking-in-art/how-to-quit-your-job-and-live-your-startup-dream/#you-can-never-be-more-wrong-than-this","text":"A passion has to be the end goal. It can ever be a \"stop-gap\" arrangement. If you treat it like that, your startup is already \" finished \" before it hs started.","title":"You can never be more wrong than this"},{"location":"knowledge/stories/thinking-in-art/how-to-quit-your-job-and-live-your-startup-dream/#is-your-passionn-only-a-passing-fad","text":"Technology Trends are like Fashion. They come and go every day and there is constant disruption. But amidst all this disruption, the one thing that remains constant is innovation. Startups which have the mettle to constantly innovate and stay ahead of the disruption curve survive the game. Others just wither and die. You need to ask yourself an important question here: Are you a chaser or a changer? A casher just cashes on the current market trend, makes some money and exits. There is no real passion involved here. A changer on the other hand, contantly \" reworks \" his product and stays ahead in the game. Your passionn is a \" true \" if it is that of a changer.","title":"Is your Passionn only a Passing Fad?"},{"location":"knowledge/stories/thinking-in-art/how-to-quit-your-job-and-live-your-startup-dream/#u-undrstand-your-reputation","text":"Once you have confirmed your passion. The next step is to understand the power of your reputation. Your reputation needs to be Steller enough to shoulder the burden of your unknown startup. Some of the ways you can do it can be as below.","title":"U - Undrstand your Reputation"},{"location":"knowledge/stories/thinking-in-art/how-to-quit-your-job-and-live-your-startup-dream/#is-your-reputation-self-made-or-company-made","text":"You fell your outstanding reputation is the key to help you find customers for your startup.","title":"Is your reputation self-made or company made?"},{"location":"knowledge/stories/thinking-in-art/how-to-quit-your-job-and-live-your-startup-dream/#now-just-go-back-a-step-and-think-abour-your-reputation","text":"your personal reptationn should always be based on your unique abilities and talents independent of \"company provided\" facilities or processes. Your \"potential\" customer should be able to recognize this ability in you to proceed further. Their \"belief\" in you should be absolute whether you are working for a big company or your own startup.","title":"Now just go back a step and think abour your \"reputation\""},{"location":"knowledge/stories/thinking-in-art/how-to-quit-your-job-and-live-your-startup-dream/#how-trustworthy-are-your-contacts","text":"Analyze and categorie your contacts into 3 groups. High peofile contact: approachable for \"business\" High profile contact: not approachable for \"business\" Casual contacts You should have at least 40% of your contacts approachable for \"business\" to make a good start. This will not only help you to give that initial tractiono but will also build up an early stellar reputation of your startup. Always remember making a great product is not enough. You also nees \"customers\" who can vouch for the product based on your \"reputation\".","title":"How Trustworthy are your contacts?"},{"location":"knowledge/stories/thinking-in-art/how-to-quit-your-job-and-live-your-startup-dream/#i-investigate-your-skills","text":"One your passion and reputation are proved beyond doubt, we come to the meat of the problem-the skills you have to take your startup forward. We require three types of Skills here.","title":"I - Investigate your Skills"},{"location":"knowledge/stories/thinking-in-art/how-to-quit-your-job-and-live-your-startup-dream/#core-skills","text":"Core Skills are your \" bread and butter \" of your start-up. They can range from anything between AI, robotics to our boring PHP. These tecnical skills are the foundation for the great product that you are building and it is important that you and your co-founders have \" near-perfect \" expertise in these skills. Do a through audit of your core skills and map them on a scale between 1 to 10 Majority of the skills going above 8 would mean, you are good to go.","title":"Core Skills"},{"location":"knowledge/stories/thinking-in-art/how-to-quit-your-job-and-live-your-startup-dream/#management-skills","text":"These are soft skills like leadership, team building, resourcing, and negotiations. But these are very very important. So if you have worked all your life in building great technical stuff and never had the \"experience\" to manage a project or a team, you might have a serious handicap here. In that case, you need to get these complementary skills from your co-founder. Everything from ideas to finances will be hazy during the initial days of the startup and it is very important to have strong management skills to optimally use the meager resources and get maximum out of them. So never under estimate its importance.","title":"Management Skills"},{"location":"knowledge/stories/thinking-in-art/how-to-quit-your-job-and-live-your-startup-dream/#peripheral-skills","text":"You need to take care of these fictions also and it is important for you to get a deep-dive understanding of these functions to perform effectively. These \" supporting \" functions have the potential to make or break your startup so you need to get yourseld trained in these areas also. It will be great if you can find these skills as complementary in your co-founders.","title":"Peripheral Skills"},{"location":"knowledge/stories/thinking-in-art/how-to-quit-your-job-and-live-your-startup-dream/#t-test-the-waters","text":"Now if you have reached so far successfully, it is time to test the waters. We need to utilize all \" approachable \" contacts whom we had identified earlier.","title":"T- Test the waters"},{"location":"knowledge/stories/thinking-in-art/how-to-quit-your-job-and-live-your-startup-dream/#what-do-the-feelers-say","text":"This is where \"feelers\" can help you. A feeler is just a brief description of the idea along with the top 2-3 potential benefits. This can be either an email, a short presentation of even a casual discussionn at a coffee shop. The basic purpose to get a \"feel\" of the customer's mind before proceeding forward. Care should be taken here only to reveal the \" what you are doing \" part keeping the \" how \" part in vague details. if 70% or more of your contacs show interest in your idea, you are on the right path.","title":"What do the \"Feelers\" say?"},{"location":"knowledge/stories/thinking-in-art/how-to-quit-your-job-and-live-your-startup-dream/#show-a-demo","text":"It is worth investing time and effort if the \"potential\" contract is going to become a \"confirmed\" customer. Such customers will be visibly exvited by your idea and cannot wait till they see it \"working\". you can plan a prototype of \"limited version\" for these customers which just showcases the meat of your product. This will not only build the customer confidence but will also build the confidence of your own team and give them \"something\" tangible to start working with. These demos are highly useful for negotiations with angel investors also.","title":"Show a Demo"},{"location":"knowledge/stories/thinking-in-art/how-to-quit-your-job-and-live-your-startup-dream/#bringing-it-all-together","text":"Many of us were raised with a belief that we can be anything we want to be and fed with the idea that if we pursue our passion, money will follow. Most of us buy this mantra, but in real life, things are never as simple as they seem. Real life requires retionality, logic and perennial inspection and improvement of our own self. And one of the more irresponsible interpretatios of \" following your dreams \" is the assumptionn that your happiness depends on it. Never equate happiness with passion. You are entering into an unending loop of frustraion and despair by doing that. Get more information at here","title":"Bringing it all together"},{"location":"knowledge/stories/thinking-in-art/how-two-words-can-change-your-life/","text":"How Tow words can change your life Get more information at here .","title":"How two words can change your life"},{"location":"knowledge/stories/thinking-in-art/how-two-words-can-change-your-life/#how-tow-words-can-change-your-life","text":"Get more information at here .","title":"How Tow words can change your life"},{"location":"knowledge/stories/thinking-in-art/make-is-pretty/","text":"Make is pretty Get more information at here .","title":"Make is pretty"},{"location":"knowledge/stories/thinking-in-art/make-is-pretty/#make-is-pretty","text":"Get more information at here .","title":"Make is pretty"},{"location":"knowledge/stories/thinking-in-art/you-could-have-today-instead-you-choose-tomorrow/","text":"You Could Have Today Instead You Choose Tomorrow You don't have to do a lot every day, but you have to do something. Something. Every day. Some what is that something? When you know what that something is, suddenly you have power and clarity and control. You know what to say yes to. What to say no to. You know who you are and what your life needs to be built around. One can't design a life around what it's like to be on vacation. Vacations are not real. They cost money. They happend somewhere far away from where you live. Life can't be filled with the day of your greatest, most impressive accomplishment either. To be Tom Brady every day, coming back from 28-3 in the Super Bowl to pull off a surprise victory in overtime- that would be exhausting. That's great once. What we need is something sustainable. Something balances . Something deliberate without being forced. Purposefull without being obsessed with productivity. We need something like a great Saturday - or one of those Mondays where you're not sure if it's part of a three-day weekend, resulting in just enough work that it's productive, but not so much that it's a chore. The funny thing is, as much as I enjoy these days, they are fleeting and rate. Why is it that I allow Webnesday to suck? Why do I choose for Tuesday to be filled with meetings that I don't remember agreeing to attend? Or phone calls that I answer? Part of the answer is that yes, I must make a living, but the truth is , my best work never comes on those crappy days. In fact, the idea for the book project I am selling now came to me on one of those long walks. And that's what pays for my house, not the emails I spend so much time responding to. 'You could be good today,' the Roman emperor Marcus Aurelius wrotr. 'But instead you choose tomorrow.' Early I said that those Satuedays were the kinds of days to build a life around. I think the mistake is that a lot of people try to build a life toward them instead. What's that line from the famous Loverboy song? Everybody's working fr the weekend. Today could be that amazing day for you. Today could be how you want life to be. You just have to choose for it to be. Or rather, stop choosing for it not to be. Get More Information at Here .","title":"You Could Have Today Intead You Choose Tomorrow"},{"location":"knowledge/stories/thinking-in-art/you-could-have-today-instead-you-choose-tomorrow/#you-could-have-today-instead-you-choose-tomorrow","text":"You don't have to do a lot every day, but you have to do something. Something. Every day. Some what is that something? When you know what that something is, suddenly you have power and clarity and control. You know what to say yes to. What to say no to. You know who you are and what your life needs to be built around. One can't design a life around what it's like to be on vacation. Vacations are not real. They cost money. They happend somewhere far away from where you live. Life can't be filled with the day of your greatest, most impressive accomplishment either. To be Tom Brady every day, coming back from 28-3 in the Super Bowl to pull off a surprise victory in overtime- that would be exhausting. That's great once. What we need is something sustainable. Something balances . Something deliberate without being forced. Purposefull without being obsessed with productivity. We need something like a great Saturday - or one of those Mondays where you're not sure if it's part of a three-day weekend, resulting in just enough work that it's productive, but not so much that it's a chore. The funny thing is, as much as I enjoy these days, they are fleeting and rate. Why is it that I allow Webnesday to suck? Why do I choose for Tuesday to be filled with meetings that I don't remember agreeing to attend? Or phone calls that I answer? Part of the answer is that yes, I must make a living, but the truth is , my best work never comes on those crappy days. In fact, the idea for the book project I am selling now came to me on one of those long walks. And that's what pays for my house, not the emails I spend so much time responding to. 'You could be good today,' the Roman emperor Marcus Aurelius wrotr. 'But instead you choose tomorrow.' Early I said that those Satuedays were the kinds of days to build a life around. I think the mistake is that a lot of people try to build a life toward them instead. What's that line from the famous Loverboy song? Everybody's working fr the weekend. Today could be that amazing day for you. Today could be how you want life to be. You just have to choose for it to be. Or rather, stop choosing for it not to be. Get More Information at Here .","title":"You Could Have Today Instead You Choose Tomorrow"},{"location":"knowledge/stories/xamarin-blog/add-a-backend-to-your-app-in-10-minutes/","text":"Add a Backend to your app in 10 minutes Get more information at here .","title":"Add a Backend to Your App in 10 minutes"},{"location":"knowledge/stories/xamarin-blog/add-a-backend-to-your-app-in-10-minutes/#add-a-backend-to-your-app-in-10-minutes","text":"Get more information at here .","title":"Add a Backend to your app in 10 minutes"},{"location":"knowledge/stories/xamarin-blog/behavior-xamarin/","text":"Behavior in Xamarin Forms Get more information at here .","title":"Behavior In Xamarin Forms"},{"location":"knowledge/stories/xamarin-blog/behavior-xamarin/#behavior-in-xamarin-forms","text":"Get more information at here .","title":"Behavior in Xamarin Forms"},{"location":"knowledge/stories/xamarin-blog/better-resource-organization-xamarin-forms/","text":"Better Resource Organization in Xamarin.Forms Get more information at here .","title":"Better Resource Organization in Xamarin.Forms"},{"location":"knowledge/stories/xamarin-blog/better-resource-organization-xamarin-forms/#better-resource-organization-in-xamarinforms","text":"Get more information at here .","title":"Better Resource Organization in Xamarin.Forms"},{"location":"knowledge/stories/xamarin-blog/getting-started-workmanager/","text":"Getting Started With WorkManager If you need to schedule a background task on Android, you're probably familiar with all of the various ways to accomplish this such as: Google Cloud Messaging Firebase Cloud Messaging DownloadManager Foreground Service Alarm Manager etc WorkManager Give a warm welcome to WorkManager. WorkManager is a library that makes it easy to schedule deferrable, asynchronous tasks even if the app exits ot the device restarts. It was designed to be backwards compatible to API 14 and does so by wrapping JobScheduler, AlarmManager, and BroadcastReceivers all in one. Using JobScheduler your app will be running on na device that is API 23+. Anything below, you'll be using combination of AlarmManager + BroadcastReceivers. How is work executed? Work is fed to an executor to guarantee the work is done. Thee executor will complete the work so long as it meets the constraints that you set up when you enqueue the work. Async by Default Every operation is asynchronous. Thus you won't have to worry about threading at all. The operations are saved in a WorkManager database that is the source of truth for any enqueued, successful, or cancelled operations. When does work end? Upon the work finishing. In the event that the constraints are no longer met(Network could be lost, Phone is no longer plugged,etc) The OS decied to kill your enqueued work. you cancelled your work. Lifetime of work One Time Work This type of work's final state is Successed when completed. Otherwise it will go into a Failed or Cancelled state. Periodic Work This type of work does not have a dinal state because it has either a finite or infinite amount of iterations. Thus it will continuously enqueue and run. Get more imformation at here .","title":"Getting Started WorkManager"},{"location":"knowledge/stories/xamarin-blog/getting-started-workmanager/#getting-started-with-workmanager","text":"If you need to schedule a background task on Android, you're probably familiar with all of the various ways to accomplish this such as: Google Cloud Messaging Firebase Cloud Messaging DownloadManager Foreground Service Alarm Manager etc","title":"Getting Started With WorkManager"},{"location":"knowledge/stories/xamarin-blog/getting-started-workmanager/#workmanager","text":"Give a warm welcome to WorkManager. WorkManager is a library that makes it easy to schedule deferrable, asynchronous tasks even if the app exits ot the device restarts. It was designed to be backwards compatible to API 14 and does so by wrapping JobScheduler, AlarmManager, and BroadcastReceivers all in one. Using JobScheduler your app will be running on na device that is API 23+. Anything below, you'll be using combination of AlarmManager + BroadcastReceivers.","title":"WorkManager"},{"location":"knowledge/stories/xamarin-blog/getting-started-workmanager/#how-is-work-executed","text":"Work is fed to an executor to guarantee the work is done. Thee executor will complete the work so long as it meets the constraints that you set up when you enqueue the work.","title":"How is work executed?"},{"location":"knowledge/stories/xamarin-blog/getting-started-workmanager/#async-by-default","text":"Every operation is asynchronous. Thus you won't have to worry about threading at all. The operations are saved in a WorkManager database that is the source of truth for any enqueued, successful, or cancelled operations.","title":"Async by Default"},{"location":"knowledge/stories/xamarin-blog/getting-started-workmanager/#when-does-work-end","text":"Upon the work finishing. In the event that the constraints are no longer met(Network could be lost, Phone is no longer plugged,etc) The OS decied to kill your enqueued work. you cancelled your work.","title":"When does work end?"},{"location":"knowledge/stories/xamarin-blog/getting-started-workmanager/#lifetime-of-work","text":"","title":"Lifetime of work"},{"location":"knowledge/stories/xamarin-blog/getting-started-workmanager/#one-time-work","text":"This type of work's final state is Successed when completed. Otherwise it will go into a Failed or Cancelled state.","title":"One Time Work"},{"location":"knowledge/stories/xamarin-blog/getting-started-workmanager/#periodic-work","text":"This type of work does not have a dinal state because it has either a finite or infinite amount of iterations. Thus it will continuously enqueue and run. Get more imformation at here .","title":"Periodic Work"},{"location":"knowledge/stories/xamarin-blog/mobile-tools-for-designers/","text":"Mobile tools for designers Get more information at here .","title":"Mobile Tools for Designer"},{"location":"knowledge/stories/xamarin-blog/mobile-tools-for-designers/#mobile-tools-for-designers","text":"Get more information at here .","title":"Mobile tools for designers"},{"location":"knowledge/stories/xamarin-blog/sharing-files-and-email-attachments/","text":"Sharing Files & Email Attachments with Xamrin.Essentials Preview Features Enabling Previews The Xamarin.Essentials team has adopted one of their favorite features from Xamarin.Forms. The \" feature flag \" approach that enables developers to try out new features before they are officially released. This is a great addition for developers who want early access because special build installation is not needed. Above all, it allows the team to repidly work on the API and make feedback-based changes if needed. To enable preview features in Xamarin.Essentials there is a new ExperimentalFeature class with a single method on it, Enable . This method takes in string arguments that represent the feature to enable. Here we can enable both email attachments and sharing files: ExperimentalFeatures.Enable(ExperimentalFeatures.EmailAttachments, ExperimentalFeatures.ShareFileRequest); This can be done at any point in the applicationn lifecycle but must be called before using the new APIs. Email Attachments This feature anables an app to emails files in email clients on the device. After the feature enabled any file can be emailed. Xamarin.Essentials will automatically detect the file type(MIME), then request the file to be added as an attachment. Every email client is different a may only support specific file extensions or none at all. here is sample of writing text to disk and adding it as an email attachment: var message = new EmailMessage { Subject = \"Hello\", Body = \"World\", } var fn = \"Attachment.txt\"; var file = Path.Combine(FileSystem.CacheDirectory, fn); File.WriteAllText(file, \"Hello World\"); message.Attachments.Add(new EmailAttachment(file)) await Email.ComposeAsync(message); Sharing a File This feature also enables an app to share files with other applications on the device. Xamarin.Essential will automatically detect the file type (MIME) while requesting a share. Each platform may only support specific file extensions. Here is a sample of writing text to disk and sharing it to other apps: var fn = \"Attachment.txt\"; var file = Path.Combine(File.System.CacheDirectory, fn); File.WriteAllText(file, \"Hello World\"); await Share.RequestAsync(new ShareFileRequest { Title = Title; File = new ShareFile(file) }) As you can see that the Xamarin.Essentials APIs work really well together for the ability to get file directories, save file to disk, and share the with the world! Get more information at here .","title":"Sharing Files and Email Attachment"},{"location":"knowledge/stories/xamarin-blog/sharing-files-and-email-attachments/#sharing-files-email-attachments-with-xamrinessentials-preview-features","text":"","title":"Sharing Files &amp; Email Attachments with Xamrin.Essentials Preview Features"},{"location":"knowledge/stories/xamarin-blog/sharing-files-and-email-attachments/#enabling-previews","text":"The Xamarin.Essentials team has adopted one of their favorite features from Xamarin.Forms. The \" feature flag \" approach that enables developers to try out new features before they are officially released. This is a great addition for developers who want early access because special build installation is not needed. Above all, it allows the team to repidly work on the API and make feedback-based changes if needed. To enable preview features in Xamarin.Essentials there is a new ExperimentalFeature class with a single method on it, Enable . This method takes in string arguments that represent the feature to enable. Here we can enable both email attachments and sharing files: ExperimentalFeatures.Enable(ExperimentalFeatures.EmailAttachments, ExperimentalFeatures.ShareFileRequest); This can be done at any point in the applicationn lifecycle but must be called before using the new APIs.","title":"Enabling Previews"},{"location":"knowledge/stories/xamarin-blog/sharing-files-and-email-attachments/#email-attachments","text":"This feature anables an app to emails files in email clients on the device. After the feature enabled any file can be emailed. Xamarin.Essentials will automatically detect the file type(MIME), then request the file to be added as an attachment. Every email client is different a may only support specific file extensions or none at all. here is sample of writing text to disk and adding it as an email attachment: var message = new EmailMessage { Subject = \"Hello\", Body = \"World\", } var fn = \"Attachment.txt\"; var file = Path.Combine(FileSystem.CacheDirectory, fn); File.WriteAllText(file, \"Hello World\"); message.Attachments.Add(new EmailAttachment(file)) await Email.ComposeAsync(message);","title":"Email Attachments"},{"location":"knowledge/stories/xamarin-blog/sharing-files-and-email-attachments/#sharing-a-file","text":"This feature also enables an app to share files with other applications on the device. Xamarin.Essential will automatically detect the file type (MIME) while requesting a share. Each platform may only support specific file extensions. Here is a sample of writing text to disk and sharing it to other apps: var fn = \"Attachment.txt\"; var file = Path.Combine(File.System.CacheDirectory, fn); File.WriteAllText(file, \"Hello World\"); await Share.RequestAsync(new ShareFileRequest { Title = Title; File = new ShareFile(file) }) As you can see that the Xamarin.Essentials APIs work really well together for the ability to get file directories, save file to disk, and share the with the world! Get more information at here .","title":"Sharing a File"},{"location":"knowledge/stories/xamarin-blog/shrinking-your-android-app-size/","text":"Shrinking Your Android App Size Get more information at here .","title":"Shrinking Your Android App Size"},{"location":"knowledge/stories/xamarin-blog/shrinking-your-android-app-size/#shrinking-your-android-app-size","text":"Get more information at here .","title":"Shrinking Your Android App Size"},{"location":"knowledge/web-development/asp.net/introduction/","text":"Introduction to ASP.NET Core ASP.NET COre is a cross-platform, high performance, open-source framework for building modern, cloud-based, internet-connected applications, With ASP.NET Core, you can: Build web apps and services, IoT apps,and mobile backends. Use your favorite development tools on Windows, macOS, and Linux. Deploy to the cloud or on-premises. Run on .Net Core or .NET Framework . Why choose ASP.NET Core? Millons of developers have used (and continue to use) ASP.NET 4.x to create web apps. ASP.NET Core is a redesign of ASP.NET 4.x, with architectural changes that result in a leaner, more modular framework. ASP.NET Core provides the following benefits: A unified story for building web UI and web APIs. Architected for testability. Razor Pages makes coding page-focused screnarios easier and more productive. Ability to develop and run on Windows, macOS, and Linux. Open-source and community-focused. Integration of modern, client-side framework and development workflows. A cloud-ready, environment-based configuration system. A lightweight, high-performance, and modular HTTP request pipeline. Ability to host on IIS, Nginx, Apache, Docker, or self-host in your own process. Side-by-side app versioning when targeting .NET Core. Tooling that simplified modern web development. Build web APOs and web UI using ASP.NET Core MVC ASP.NET Core MVC provides features to build web APIs and web apps. The Model-View Controller pattern helps make your web APIs and web apps testable. Razor Pages is a page-based progamming model that makes building web UI easier and more productive. Razor markup provides a productive syntax for Razor Pages adn MVC views. Tag Helpers enable server0side code to participate in creating and rendering HTML elements in Razor files. Built-in support for multiple data formats and content negotiation lets your web APIs reach a broad range of clients, including browsers and mobile devices. Model binding automatically maps data from HTTP requests to action method parameters. Model validation automatically prtforms client-side and server-side validation. Client-side development ASP.NET Core integrates seamlessly with popular client-side frameworks and libraries, including Blazor, Angular, React, and Bootstrap. For more information, see Introduction to Blazor in ASP.NET Core and related topic under Client-side development. Get more information at here .","title":"Introduction"},{"location":"knowledge/web-development/asp.net/introduction/#introduction-to-aspnet-core","text":"ASP.NET COre is a cross-platform, high performance, open-source framework for building modern, cloud-based, internet-connected applications, With ASP.NET Core, you can: Build web apps and services, IoT apps,and mobile backends. Use your favorite development tools on Windows, macOS, and Linux. Deploy to the cloud or on-premises. Run on .Net Core or .NET Framework .","title":"Introduction to ASP.NET Core"},{"location":"knowledge/web-development/asp.net/introduction/#why-choose-aspnet-core","text":"Millons of developers have used (and continue to use) ASP.NET 4.x to create web apps. ASP.NET Core is a redesign of ASP.NET 4.x, with architectural changes that result in a leaner, more modular framework. ASP.NET Core provides the following benefits: A unified story for building web UI and web APIs. Architected for testability. Razor Pages makes coding page-focused screnarios easier and more productive. Ability to develop and run on Windows, macOS, and Linux. Open-source and community-focused. Integration of modern, client-side framework and development workflows. A cloud-ready, environment-based configuration system. A lightweight, high-performance, and modular HTTP request pipeline. Ability to host on IIS, Nginx, Apache, Docker, or self-host in your own process. Side-by-side app versioning when targeting .NET Core. Tooling that simplified modern web development.","title":"Why choose ASP.NET Core?"},{"location":"knowledge/web-development/asp.net/introduction/#build-web-apos-and-web-ui-using-aspnet-core-mvc","text":"ASP.NET Core MVC provides features to build web APIs and web apps. The Model-View Controller pattern helps make your web APIs and web apps testable. Razor Pages is a page-based progamming model that makes building web UI easier and more productive. Razor markup provides a productive syntax for Razor Pages adn MVC views. Tag Helpers enable server0side code to participate in creating and rendering HTML elements in Razor files. Built-in support for multiple data formats and content negotiation lets your web APIs reach a broad range of clients, including browsers and mobile devices. Model binding automatically maps data from HTTP requests to action method parameters. Model validation automatically prtforms client-side and server-side validation.","title":"Build web APOs and web UI using ASP.NET Core MVC"},{"location":"knowledge/web-development/asp.net/introduction/#client-side-development","text":"ASP.NET Core integrates seamlessly with popular client-side frameworks and libraries, including Blazor, Angular, React, and Bootstrap. For more information, see Introduction to Blazor in ASP.NET Core and related topic under Client-side development. Get more information at here .","title":"Client-side development"},{"location":"knowledge/web-development/asp.net/nswag/","text":"NSwag: The Swagger/OpenAPI toolchain for .NET, ASP.NET Core and TypeScript Nwag is a Swagger/OpenAPI 2.9 and 3.9 toolchain for .NET, .NET Core, Web API, ASP.NET Core, TypeScript (jQuery, Angularjs,Angular 2+, Aurelia, KnockoutJS and more) and other platforms, written in C#. The Swagger specification JSON and JSON Schema to descrine a RESTful web API. The NSwag project provide tools to generate Swagger specification from existing ASP.NET Web API controllers and client code from these Swagger specifications. The project combines the functionality of Swashbuckle (Swagger generation) and AutoRest (client generation) in one toolchain. This way a lot of incompatibilities can be avoided and features which are not well described by the Swagger specification or JSON Schema are better supported. Features: Generate Swagger 2.0 and OpenAPI 3.0 specification from C# ASP.NET (Core) controllers . Serve the specs via ASP.NET (Core) middleware, optionally with Swagger UI or ReDoc Generate C# or Type Script clients/proxies from these specs. Everything can be automated via CLI (distributed via NuGet tool or build target; or NPM) CLI configured via JSON file or NSwagStudio Windows UI Ways to use the toolchain: Simple to use Windows GUI, NSwagStudio By using the Swagger or Swagger UI OWIN and ASP.NET Core Middlewares (also serves the Swagger UI )(recommended) Via command line (Windows, Mac and Linux suport through Mono or .NET Core console binary, also via NPM package ) In your C# code, via NuGet In your MSBuild targets . With ServiceProjectReference tags in your .csproj (preview) In your Azure V2 Functions (external project, might not use latest NSwag version) Tutorials Video Tutorial: How to integrate NSwag into your ASP.NET COre Web API project (5 mins) Integrate the NSwag toolchain into your ASP.NET Web API project Generate an Angular TypeScrip client from an existing ASP.NET Web API web assembly Get more information at here .","title":"NSwag"},{"location":"knowledge/web-development/asp.net/nswag/#nswag-the-swaggeropenapi-toolchain-for-net-aspnet-core-and-typescript","text":"Nwag is a Swagger/OpenAPI 2.9 and 3.9 toolchain for .NET, .NET Core, Web API, ASP.NET Core, TypeScript (jQuery, Angularjs,Angular 2+, Aurelia, KnockoutJS and more) and other platforms, written in C#. The Swagger specification JSON and JSON Schema to descrine a RESTful web API. The NSwag project provide tools to generate Swagger specification from existing ASP.NET Web API controllers and client code from these Swagger specifications. The project combines the functionality of Swashbuckle (Swagger generation) and AutoRest (client generation) in one toolchain. This way a lot of incompatibilities can be avoided and features which are not well described by the Swagger specification or JSON Schema are better supported.","title":"NSwag: The Swagger/OpenAPI toolchain for .NET, ASP.NET Core and TypeScript"},{"location":"knowledge/web-development/asp.net/nswag/#features","text":"Generate Swagger 2.0 and OpenAPI 3.0 specification from C# ASP.NET (Core) controllers . Serve the specs via ASP.NET (Core) middleware, optionally with Swagger UI or ReDoc Generate C# or Type Script clients/proxies from these specs. Everything can be automated via CLI (distributed via NuGet tool or build target; or NPM) CLI configured via JSON file or NSwagStudio Windows UI","title":"Features:"},{"location":"knowledge/web-development/asp.net/nswag/#ways-to-use-the-toolchain","text":"Simple to use Windows GUI, NSwagStudio By using the Swagger or Swagger UI OWIN and ASP.NET Core Middlewares (also serves the Swagger UI )(recommended) Via command line (Windows, Mac and Linux suport through Mono or .NET Core console binary, also via NPM package ) In your C# code, via NuGet In your MSBuild targets . With ServiceProjectReference tags in your .csproj (preview) In your Azure V2 Functions (external project, might not use latest NSwag version)","title":"Ways to use the toolchain:"},{"location":"knowledge/web-development/asp.net/nswag/#tutorials","text":"Video Tutorial: How to integrate NSwag into your ASP.NET COre Web API project (5 mins) Integrate the NSwag toolchain into your ASP.NET Web API project Generate an Angular TypeScrip client from an existing ASP.NET Web API web assembly Get more information at here .","title":"Tutorials"},{"location":"knowledge/web-development/basics/code-coverage/","text":"Code coverage What is Code coverage Code coverage is a measure which describes the degree of which the source code o the program has been tested. It one form of white box testing which finds the areas of the set of test cases. It also creates some test cases to increase coverage and determining a quantitative measure of code coverage. Why use Code Coverage Here, are some prime reasons for using code coverage: It helps you to measure the efficiency of test implementation. It offer a quantitative measurement. It defines the degree to which the source code has been tested. Code Coverage Methods Statement Coverage Statement coverage is a white box test design technique which involves execution of all the executable statements in the source code at least once. It is used to calculate and measure the number of statements in the source code which can be executed given the requirements. Statement coverage is used to derive scenario based upon the structure of the code under test. Statement Coverage = (Number of excuted statements / Total number of statements) * 100 What is covered by Statement Coverage? Unused Statements Dead Code Unused Branches Decision Coverage Decision coverage reports the true or false outcomes of each Boolean expression. In this coverage, expressions can sometimes get complicated. Therefore, it is very hard to achieve 100% coverage. That's why there are many different methods of reporting this metric. All these methods focus on covering the most important combinations. It is very much similar to decision coverage, but it offers better sensitivity to control flow. Decision Coverage = (Number of Decision Outcomes Exercised/Tootal Number of Decision Outcomes) * 100 Branch Coverage In the branch coverage, every outcome from a code module is tested. if the outcomes are binary, you need to test both True and False outcomes. By using Branch coverage method, you can also measure the fraction of independent code segments. it also helps you to find out which is sections of code don't have any branches. Branch Coverage = (Number of Executed Branched/Total Number of Branches) * 100 Branch coverage Testing offers the following advantages: Allows you to validate-all the branches in the code. Helps you to ensure that no branched lead to any abnormality of the program's operation. Branch coverage method removes issues which happen because of statement coverage testing. Allows you to find those areas which are not tested by other testing methods. It allows you to find a quantitative measure of code coverage. Branch coverage ignores branches inside the Boolean expressions. Condition Coverage Conditional coverage or expression coverage will reveal how the variables or subexpressions in the coditional statement are evaluated. In this coverage expressions with logical operands are only considered. Conditional coverage offers better sensitivity to the control flow thatn decision coverage. Condition coverage does not give a guarantee about full decision coverage. Condition Coverage = (Number of Executed Operands/ Total Number of Operands) * 100 Finite State Machine Coverage Finite state machine coverage is certainly the most complex type of code coverage method. This is because it works on the behavior of the design. In this coverage method, you need to look for how many time-specific states are visited, transited. It also checks how many sequences are included in a finite state machine. Which Type of Code Coverage to Choose This is certainly the most difficult answer to give. In order to select a coverage method, the tester needs to check that the code under test has single or multiple undiscovered defects. cost of the potential penalty. cost of lost reputation. cost of lost sale, etc. Advantages of Using Code Coverage Helpful to evaluate a quantitative measure of code coverage. It allows you to create extra test cased to increase coverage. It allows you to find the areas of a program which is not exercised by a set of test cases. Disadvantages of Using Code Coverage Even when any specifc feature is not implemented in design, code coverage still report 100% coverage. It is not possible to determine whetger we tested all possible values of a feature with the help of code coverage. Code coverage is also not telling how much and how well you have covered your logic. In the case when the specified function hasn't implemented, or a not incluced from the specification, then structure-based techniques cannot find that issue. Summary Code coverage is a measure which describes the degree of which the source code of the program has been tested. It helps you to measure the efficiency of test implementation. Five Code Coverage methods are 1.) Statement Coverage 2.) Condition Coverage 3.) Branch Coverage 4) Toggle Coverage 5) FSM Coverage. Statement coverage involves execution of all the executable statements in the source code at least once. Decision coverage reports the true or false outcomes of each Boolean expression. In the branch coverage, every outcome from a code module is tested. Conditional will reveal how the variables or subexpressions in the conditional statement are evaluated. Finite state machine coverage is certainly the most complext type of code coverage method. In order to select a coverage method, the tester needs to check the cost of the potential penalty, lost reputation, lost sale, etc. Code coverage tells you how well the source code has been exercised by your test bench while Functional coverage measures how well the functionality of the design has been covered. Covertura, JTest, Clover, Emma, Kalistick are few important code coverage tools. Code Coverage allows you to create extra test cases to increase coverage. Code Coverage does help you to determine whether we tested all possible value of a feature. Get more information at here .","title":"Code Coverage"},{"location":"knowledge/web-development/basics/code-coverage/#code-coverage","text":"","title":"Code coverage"},{"location":"knowledge/web-development/basics/code-coverage/#what-is-code-coverage","text":"Code coverage is a measure which describes the degree of which the source code o the program has been tested. It one form of white box testing which finds the areas of the set of test cases. It also creates some test cases to increase coverage and determining a quantitative measure of code coverage.","title":"What is Code coverage"},{"location":"knowledge/web-development/basics/code-coverage/#why-use-code-coverage","text":"Here, are some prime reasons for using code coverage: It helps you to measure the efficiency of test implementation. It offer a quantitative measurement. It defines the degree to which the source code has been tested.","title":"Why use Code Coverage"},{"location":"knowledge/web-development/basics/code-coverage/#code-coverage-methods","text":"","title":"Code Coverage Methods"},{"location":"knowledge/web-development/basics/code-coverage/#statement-coverage","text":"Statement coverage is a white box test design technique which involves execution of all the executable statements in the source code at least once. It is used to calculate and measure the number of statements in the source code which can be executed given the requirements. Statement coverage is used to derive scenario based upon the structure of the code under test. Statement Coverage = (Number of excuted statements / Total number of statements) * 100 What is covered by Statement Coverage? Unused Statements Dead Code Unused Branches","title":"Statement Coverage"},{"location":"knowledge/web-development/basics/code-coverage/#decision-coverage","text":"Decision coverage reports the true or false outcomes of each Boolean expression. In this coverage, expressions can sometimes get complicated. Therefore, it is very hard to achieve 100% coverage. That's why there are many different methods of reporting this metric. All these methods focus on covering the most important combinations. It is very much similar to decision coverage, but it offers better sensitivity to control flow. Decision Coverage = (Number of Decision Outcomes Exercised/Tootal Number of Decision Outcomes) * 100","title":"Decision Coverage"},{"location":"knowledge/web-development/basics/code-coverage/#branch-coverage","text":"In the branch coverage, every outcome from a code module is tested. if the outcomes are binary, you need to test both True and False outcomes. By using Branch coverage method, you can also measure the fraction of independent code segments. it also helps you to find out which is sections of code don't have any branches. Branch Coverage = (Number of Executed Branched/Total Number of Branches) * 100 Branch coverage Testing offers the following advantages: Allows you to validate-all the branches in the code. Helps you to ensure that no branched lead to any abnormality of the program's operation. Branch coverage method removes issues which happen because of statement coverage testing. Allows you to find those areas which are not tested by other testing methods. It allows you to find a quantitative measure of code coverage. Branch coverage ignores branches inside the Boolean expressions.","title":"Branch Coverage"},{"location":"knowledge/web-development/basics/code-coverage/#condition-coverage","text":"Conditional coverage or expression coverage will reveal how the variables or subexpressions in the coditional statement are evaluated. In this coverage expressions with logical operands are only considered. Conditional coverage offers better sensitivity to the control flow thatn decision coverage. Condition coverage does not give a guarantee about full decision coverage. Condition Coverage = (Number of Executed Operands/ Total Number of Operands) * 100","title":"Condition Coverage"},{"location":"knowledge/web-development/basics/code-coverage/#finite-state-machine-coverage","text":"Finite state machine coverage is certainly the most complex type of code coverage method. This is because it works on the behavior of the design. In this coverage method, you need to look for how many time-specific states are visited, transited. It also checks how many sequences are included in a finite state machine.","title":"Finite State Machine Coverage"},{"location":"knowledge/web-development/basics/code-coverage/#which-type-of-code-coverage-to-choose","text":"This is certainly the most difficult answer to give. In order to select a coverage method, the tester needs to check that the code under test has single or multiple undiscovered defects. cost of the potential penalty. cost of lost reputation. cost of lost sale, etc.","title":"Which Type of Code Coverage to Choose"},{"location":"knowledge/web-development/basics/code-coverage/#advantages-of-using-code-coverage","text":"Helpful to evaluate a quantitative measure of code coverage. It allows you to create extra test cased to increase coverage. It allows you to find the areas of a program which is not exercised by a set of test cases.","title":"Advantages of Using Code Coverage"},{"location":"knowledge/web-development/basics/code-coverage/#disadvantages-of-using-code-coverage","text":"Even when any specifc feature is not implemented in design, code coverage still report 100% coverage. It is not possible to determine whetger we tested all possible values of a feature with the help of code coverage. Code coverage is also not telling how much and how well you have covered your logic. In the case when the specified function hasn't implemented, or a not incluced from the specification, then structure-based techniques cannot find that issue.","title":"Disadvantages of Using Code Coverage"},{"location":"knowledge/web-development/basics/code-coverage/#summary","text":"Code coverage is a measure which describes the degree of which the source code of the program has been tested. It helps you to measure the efficiency of test implementation. Five Code Coverage methods are 1.) Statement Coverage 2.) Condition Coverage 3.) Branch Coverage 4) Toggle Coverage 5) FSM Coverage. Statement coverage involves execution of all the executable statements in the source code at least once. Decision coverage reports the true or false outcomes of each Boolean expression. In the branch coverage, every outcome from a code module is tested. Conditional will reveal how the variables or subexpressions in the conditional statement are evaluated. Finite state machine coverage is certainly the most complext type of code coverage method. In order to select a coverage method, the tester needs to check the cost of the potential penalty, lost reputation, lost sale, etc. Code coverage tells you how well the source code has been exercised by your test bench while Functional coverage measures how well the functionality of the design has been covered. Covertura, JTest, Clover, Emma, Kalistick are few important code coverage tools. Code Coverage allows you to create extra test cases to increase coverage. Code Coverage does help you to determine whether we tested all possible value of a feature. Get more information at here .","title":"Summary"},{"location":"knowledge/web-development/basics/css-handbook/","text":"The CSS Handbook: a handy guide to CSS for developers Adding css to an html page 1:Using the link tag The link tag is the way to include a CSS file. <link rel=\"stylesheet\" type=\"text/css\" href=\"myfile.css\" /> 2:using the style tag Instead of using the link tag to point to separate stylesheet containing our CSS, we can add the CSS directly inside a style tag. THis is the syntax: <style> ...our CSS...; </style> 3:Inline styles Inline styles are the third way to add CSS to a page. We can add a style attribute to an HTML tag, and add CSS into it. <div style=\"\">...</div> SELECTORS we can target that element using this selector p : p { color: yellow; } Classes are identified using the . symbol, while ids using the # symbol. Example using a class: <p class=\"dog-name\"> Roger </p> .dog-name { color: yellow; } Example using an id: <p id=\"dog-name\"> Roger </p> #dog-name { color: yellow; } Cobining selectors So far we've seen how to target an element, aclass or an id. Let's introduce more powerful selectors. Follow the doument tree with selectors If you have a span tag nested inside a p tag, you can target that one without applying the style to a span tag not included in a p tag: <span> Hello! </span> <p> My dog name is: <span class=\"dog-name\"> Roger </span> </p> p span { color: yellow; } To make the dependency strict on the first level, you can use the > symbol between the two tokens: p > span { color: yellow; } Direct children will have the style applied: <p> <span> This is yellow </span> <strong> <span> This is not yellow </span> </strong> </p> Adjacent sibling selectors let us style an element only if preceded by a specific element. We do so using the + operator: p + span { color: yellow; } We have a lot more selectors we can use: attribute selectors pseudo class selectors pseudo element selectors CASCADE Cascade is the process, or algotihm, that determines the properties applied to each element on the page. Trying to converge from a list of CSS rules that are defined in various places. It does so taking in condiseration: specificity importance inheritance order in the file It also takes care of resolving conflicts. Two or more competing CSS rules for the same property applied to the same element need t be elaborated according to the CSS spec, to determine which one needs to be applied. Even if you just have one CSS file loaded by your page, there is other CSS that is going to be part of the process. We have the browser(user agent) CSS. Browsers come with a defaults set of rules, all different between browsers. SPECIFICITY Enter specificity. The more specfific rule will win. If two or more rules have the same specificity, the one that appears last wins. How to calculate specificity Specificity is calculated using a convention. We have 4 slots, and each one of them starts at 0: 0 0 0 0 . The slot at the left is the most important, and the rightmost one is the least important. Slot 1 The first slot, the rightmost one, is the least important. We increase this value when we have an element slector . An element is a tag name. p { } /* 0 0 0 1 */ span { } /* 0 0 0 1 */ p span { } /* 0 0 0 2 */ p > span { } /* 0 0 0 2 */ div p > span { } /* 0 0 0 3 */ Slot 2 The second slot is incremented by 3 things: class selectors pseudo-class selectors attribute selectors Every time a rule meets one of those, we increment the value of the sencond column from the right. .name { } /* 0 0 1 0 */ .users .name { } /* 0 0 2 0 */ [href$=\".pdf\"] { } /* 0 0 1 0 */ :hover { } /* 0 0 1 0 */ Of course slot 2 selectors can be combined with slot 1 selectors: div .name { } /* 0 0 1 1 */ a[href$=\".pdf\"] { } /* 0 0 1 1 */ .pictures img:hover { } /* 0 0 2 1 */ one nice trick with classes is that you can repeat the same class and increse the specificity. .name { } /* 0 0 1 0 */ .name.name { } /* 0 0 2 0 */ .name.name.name { } /* 0 0 3 0 */ Slot 3 Slot 3 holds the most important thing that can affet your CSS specificity in a CSS file: the id . Every element can have an id attribute assigned, and we can use that in our stylesheet to target the element. #name { } /* 0 1 0 0 */ .user #name { } /* 0 1 1 0 */ #name span { } /* 0 1 0 1 */ Slot 4 Slot 3 is affected by inline styles. Any inline style will have precedence over any rule defined in an external CSS file, or inside the style tag in the page header. <p style=\"color: red\">Test</p> /* 1 0 0 0 */ Importance Specificity does not matter if a rule ends with !important : p { font-size: 20px !important; } That rule will take precedence over any rule with more specificity. Tips Generally, !important should have no place in your CSS files. using the id attribute to style CSS is also debated a lot, since it has a very high specificity. A good alternative is to use classes insted, which have less specificity, and so they are easier to work with, and they are more powerful (you can have multiple classes for an element, and a class can be reused multiple times). Tool s to clculate the specificity You can use the site https://specificity.keegan.st/ to perform the specificity calculation for you automatically. It's useful especially if you are trying to figure things out, as it can be a nice feedback tool. INHERITANCE Properties that inherit Here is a list of the properties that do inherit. This list is non-comprehensive, but those rules are just the most populer ones you'll likely use: border-collapse border-spacing caption-side color cursor direction empty-cells font-size font-family font-style font-variant font-weight font-size-adjust font-stretch font letter-spacing line-height list-style-image list-style-position list-style-type list-style orphans quotes tab-size text-align text-algn-last text-decoration-color text-indent text-justify text-shadow text-transform visibility white-space widows word-break word-spacing I got it from this nice Sitepoint article on CSS inheritance. Forcing properties to inherit What if you have a property that's not inherited by default, and you want it to, in a child? In the children, yout set the property value to the special keyword inherit . body { background-color: yellow; } p { background-color: inherit; } Forcing properties to NOT inherit On the contrary, you might have a property inherited and you want to avoid so. You can use the revert keyword to revert it. In this case, the value is reverted to the original value the browser gave it in its default stylesheet. Other special values you can also set any property to: initial : use the default browser stylesheet if available. If not, and if the property inherits by default, inherit the value. Otherwise do nothing. unset : if the property inherits by default, inherit. Otherwise do nothing. IMPORT From any CSS file you can import another CSS file using the @import directive. @import url(myfile.css); url() can manage absolute or relative URLs. One important thing you need to know is that @import directives must be put before any other CSS in the file, or they will be ignored. You can use media descriptors to only load a CSS file on the specific media: @import url(myfile.css) all; @import url(myfile-screen.css) screen; @import url(myfile-print.css) print; To be continued. Get more information at here .","title":"CSS HandBook"},{"location":"knowledge/web-development/basics/css-handbook/#the-css-handbook-a-handy-guide-to-css-for-developers","text":"","title":"The CSS Handbook: a handy guide to CSS for developers"},{"location":"knowledge/web-development/basics/css-handbook/#adding-css-to-an-html-page","text":"","title":"Adding css to an html page"},{"location":"knowledge/web-development/basics/css-handbook/#1using-the-link-tag","text":"The link tag is the way to include a CSS file. <link rel=\"stylesheet\" type=\"text/css\" href=\"myfile.css\" />","title":"1:Using the link tag"},{"location":"knowledge/web-development/basics/css-handbook/#2using-the-style-tag","text":"Instead of using the link tag to point to separate stylesheet containing our CSS, we can add the CSS directly inside a style tag. THis is the syntax: <style> ...our CSS...; </style>","title":"2:using the style tag"},{"location":"knowledge/web-development/basics/css-handbook/#3inline-styles","text":"Inline styles are the third way to add CSS to a page. We can add a style attribute to an HTML tag, and add CSS into it. <div style=\"\">...</div>","title":"3:Inline styles"},{"location":"knowledge/web-development/basics/css-handbook/#selectors","text":"we can target that element using this selector p : p { color: yellow; } Classes are identified using the . symbol, while ids using the # symbol. Example using a class: <p class=\"dog-name\"> Roger </p> .dog-name { color: yellow; } Example using an id: <p id=\"dog-name\"> Roger </p> #dog-name { color: yellow; }","title":"SELECTORS"},{"location":"knowledge/web-development/basics/css-handbook/#cobining-selectors","text":"So far we've seen how to target an element, aclass or an id. Let's introduce more powerful selectors.","title":"Cobining selectors"},{"location":"knowledge/web-development/basics/css-handbook/#follow-the-doument-tree-with-selectors","text":"If you have a span tag nested inside a p tag, you can target that one without applying the style to a span tag not included in a p tag: <span> Hello! </span> <p> My dog name is: <span class=\"dog-name\"> Roger </span> </p> p span { color: yellow; } To make the dependency strict on the first level, you can use the > symbol between the two tokens: p > span { color: yellow; } Direct children will have the style applied: <p> <span> This is yellow </span> <strong> <span> This is not yellow </span> </strong> </p> Adjacent sibling selectors let us style an element only if preceded by a specific element. We do so using the + operator: p + span { color: yellow; } We have a lot more selectors we can use: attribute selectors pseudo class selectors pseudo element selectors","title":"Follow the doument tree with selectors"},{"location":"knowledge/web-development/basics/css-handbook/#cascade","text":"Cascade is the process, or algotihm, that determines the properties applied to each element on the page. Trying to converge from a list of CSS rules that are defined in various places. It does so taking in condiseration: specificity importance inheritance order in the file It also takes care of resolving conflicts. Two or more competing CSS rules for the same property applied to the same element need t be elaborated according to the CSS spec, to determine which one needs to be applied. Even if you just have one CSS file loaded by your page, there is other CSS that is going to be part of the process. We have the browser(user agent) CSS. Browsers come with a defaults set of rules, all different between browsers.","title":"CASCADE"},{"location":"knowledge/web-development/basics/css-handbook/#specificity","text":"Enter specificity. The more specfific rule will win. If two or more rules have the same specificity, the one that appears last wins.","title":"SPECIFICITY"},{"location":"knowledge/web-development/basics/css-handbook/#how-to-calculate-specificity","text":"Specificity is calculated using a convention. We have 4 slots, and each one of them starts at 0: 0 0 0 0 . The slot at the left is the most important, and the rightmost one is the least important.","title":"How to calculate specificity"},{"location":"knowledge/web-development/basics/css-handbook/#slot-1","text":"The first slot, the rightmost one, is the least important. We increase this value when we have an element slector . An element is a tag name. p { } /* 0 0 0 1 */ span { } /* 0 0 0 1 */ p span { } /* 0 0 0 2 */ p > span { } /* 0 0 0 2 */ div p > span { } /* 0 0 0 3 */","title":"Slot 1"},{"location":"knowledge/web-development/basics/css-handbook/#slot-2","text":"The second slot is incremented by 3 things: class selectors pseudo-class selectors attribute selectors Every time a rule meets one of those, we increment the value of the sencond column from the right. .name { } /* 0 0 1 0 */ .users .name { } /* 0 0 2 0 */ [href$=\".pdf\"] { } /* 0 0 1 0 */ :hover { } /* 0 0 1 0 */ Of course slot 2 selectors can be combined with slot 1 selectors: div .name { } /* 0 0 1 1 */ a[href$=\".pdf\"] { } /* 0 0 1 1 */ .pictures img:hover { } /* 0 0 2 1 */ one nice trick with classes is that you can repeat the same class and increse the specificity. .name { } /* 0 0 1 0 */ .name.name { } /* 0 0 2 0 */ .name.name.name { } /* 0 0 3 0 */","title":"Slot 2"},{"location":"knowledge/web-development/basics/css-handbook/#slot-3","text":"Slot 3 holds the most important thing that can affet your CSS specificity in a CSS file: the id . Every element can have an id attribute assigned, and we can use that in our stylesheet to target the element. #name { } /* 0 1 0 0 */ .user #name { } /* 0 1 1 0 */ #name span { } /* 0 1 0 1 */","title":"Slot 3"},{"location":"knowledge/web-development/basics/css-handbook/#slot-4","text":"Slot 3 is affected by inline styles. Any inline style will have precedence over any rule defined in an external CSS file, or inside the style tag in the page header. <p style=\"color: red\">Test</p> /* 1 0 0 0 */","title":"Slot 4"},{"location":"knowledge/web-development/basics/css-handbook/#importance","text":"Specificity does not matter if a rule ends with !important : p { font-size: 20px !important; } That rule will take precedence over any rule with more specificity.","title":"Importance"},{"location":"knowledge/web-development/basics/css-handbook/#tips","text":"Generally, !important should have no place in your CSS files. using the id attribute to style CSS is also debated a lot, since it has a very high specificity. A good alternative is to use classes insted, which have less specificity, and so they are easier to work with, and they are more powerful (you can have multiple classes for an element, and a class can be reused multiple times).","title":"Tips"},{"location":"knowledge/web-development/basics/css-handbook/#tool-s-to-clculate-the-specificity","text":"You can use the site https://specificity.keegan.st/ to perform the specificity calculation for you automatically. It's useful especially if you are trying to figure things out, as it can be a nice feedback tool.","title":"Tool s to clculate the specificity"},{"location":"knowledge/web-development/basics/css-handbook/#inheritance","text":"","title":"INHERITANCE"},{"location":"knowledge/web-development/basics/css-handbook/#properties-that-inherit","text":"Here is a list of the properties that do inherit. This list is non-comprehensive, but those rules are just the most populer ones you'll likely use: border-collapse border-spacing caption-side color cursor direction empty-cells font-size font-family font-style font-variant font-weight font-size-adjust font-stretch font letter-spacing line-height list-style-image list-style-position list-style-type list-style orphans quotes tab-size text-align text-algn-last text-decoration-color text-indent text-justify text-shadow text-transform visibility white-space widows word-break word-spacing I got it from this nice Sitepoint article on CSS inheritance.","title":"Properties that inherit"},{"location":"knowledge/web-development/basics/css-handbook/#forcing-properties-to-inherit","text":"What if you have a property that's not inherited by default, and you want it to, in a child? In the children, yout set the property value to the special keyword inherit . body { background-color: yellow; } p { background-color: inherit; }","title":"Forcing properties to inherit"},{"location":"knowledge/web-development/basics/css-handbook/#forcing-properties-to-not-inherit","text":"On the contrary, you might have a property inherited and you want to avoid so. You can use the revert keyword to revert it. In this case, the value is reverted to the original value the browser gave it in its default stylesheet.","title":"Forcing properties to NOT inherit"},{"location":"knowledge/web-development/basics/css-handbook/#other-special-values","text":"you can also set any property to: initial : use the default browser stylesheet if available. If not, and if the property inherits by default, inherit the value. Otherwise do nothing. unset : if the property inherits by default, inherit. Otherwise do nothing.","title":"Other special values"},{"location":"knowledge/web-development/basics/css-handbook/#import","text":"From any CSS file you can import another CSS file using the @import directive. @import url(myfile.css); url() can manage absolute or relative URLs. One important thing you need to know is that @import directives must be put before any other CSS in the file, or they will be ignored. You can use media descriptors to only load a CSS file on the specific media: @import url(myfile.css) all; @import url(myfile-screen.css) screen; @import url(myfile-print.css) print; To be continued. Get more information at here .","title":"IMPORT"},{"location":"knowledge/web-development/basics/functional-testing/","text":"Functional Testing What is Functional Testing ? Functional Testing is defined as a type of testing which verifies that each function of the software application operates in conformance with the requirement specification. This testing mainly involves black box testing and it is not concerned about the source code of the application. Each and every functionality of the system is tested by providing appropriate input, verifying the ourput and comparing the actual results with the expected results. This testing involves checking of User Interface, APIs, Database, security, client/server applications and functionality of the Application Under Test. The testing can be done either manually or using automation. What do you test in Functional Testing ? The prime objective of Functional testing is checking the functionalities of the software system. It mainly concentrates on: Mainline functions : Testing the main functions of an application. Basic Usability : It involves basic usability testing of the system. It checks whether a user can freely navigate through the screens without any difficulties. Accessibility : Checks the accessibility of the system for the user. Error Conditions : Usage of testing techniques to check for error conditions. It checks whether suitable error messages are displayed. How to perform Functional Testing: Complete Process In order to functionally test an application, the following steps must be observed. Understan the Software Engineering Requirements. Identify test input (test data) Compute th expected outcomes with the selected test input values. Execute test cases. Comparison of actual and computed expected result. Get more information at here .","title":"Functional Testing"},{"location":"knowledge/web-development/basics/functional-testing/#functional-testing","text":"","title":"Functional Testing"},{"location":"knowledge/web-development/basics/functional-testing/#what-is-functional-testing","text":"Functional Testing is defined as a type of testing which verifies that each function of the software application operates in conformance with the requirement specification. This testing mainly involves black box testing and it is not concerned about the source code of the application. Each and every functionality of the system is tested by providing appropriate input, verifying the ourput and comparing the actual results with the expected results. This testing involves checking of User Interface, APIs, Database, security, client/server applications and functionality of the Application Under Test. The testing can be done either manually or using automation.","title":"What is Functional Testing ?"},{"location":"knowledge/web-development/basics/functional-testing/#what-do-you-test-in-functional-testing","text":"The prime objective of Functional testing is checking the functionalities of the software system. It mainly concentrates on: Mainline functions : Testing the main functions of an application. Basic Usability : It involves basic usability testing of the system. It checks whether a user can freely navigate through the screens without any difficulties. Accessibility : Checks the accessibility of the system for the user. Error Conditions : Usage of testing techniques to check for error conditions. It checks whether suitable error messages are displayed.","title":"What do you test in Functional Testing ?"},{"location":"knowledge/web-development/basics/functional-testing/#how-to-perform-functional-testing-complete-process","text":"In order to functionally test an application, the following steps must be observed. Understan the Software Engineering Requirements. Identify test input (test data) Compute th expected outcomes with the selected test input values. Execute test cases. Comparison of actual and computed expected result. Get more information at here .","title":"How to perform Functional Testing: Complete Process"},{"location":"knowledge/web-development/basics/graphql/","text":"GraphQL A query language for your API GraphQL is a query language for APIs and a runtime for fulfilling those queries with your exsting data. GraphQL provides a complete and understandable description of the data in your API, gives clients the power to ask for exactly what they need and nothing more, makes it easier to evolve APIs over time, and enables powerful developer tools. Ask for what you need, get exactly that Send a GraphQL query to your API and get exactly what you need, nothing more and nothing less. GraphQL queries always return predictable results. Apps using GraphQL are fast and stable because they control the data they get, not the server. Get many resources in a single request GraphQL queries access not just the properties of one resource but also smoothly follow references between them. While typical REST APIs require loading your app needs in a single request. Apps using GraphQL can be quick even on slow mobile network connections. Describe what's possible with a type system GraphQL APIs are organizaed in terms of types and fields, not endpoints. Access the full capabilities of your data from a single endpoint. GraphQL uses types to ensure Apps only ask for what's possible and provide clear and helpful errors. Apps can use types to avoid writing manual parsing code. Evolve your API without versions Add new fields and types to your GraphQL API without impacting existing queries. Aging fields can be deprecated and hidden from tools. By using a single evolving version, GraphQL APIs give apps continuous access to new features and encourage cleaner, more maintainable server code. Bring your own data and code GraphQL creates a uniform API across your entire application without being limited by a specific storage engine. Write GraphQL APIs that leverage your existing data and code with GraphQL engines available in many languages. You provide functions for each field in the type system, and GraphQL calls them with optimal concurrency. Get more information at here .","title":"GraphQL"},{"location":"knowledge/web-development/basics/graphql/#graphql","text":"","title":"GraphQL"},{"location":"knowledge/web-development/basics/graphql/#a-query-language-for-your-api","text":"GraphQL is a query language for APIs and a runtime for fulfilling those queries with your exsting data. GraphQL provides a complete and understandable description of the data in your API, gives clients the power to ask for exactly what they need and nothing more, makes it easier to evolve APIs over time, and enables powerful developer tools.","title":"A query language for your API"},{"location":"knowledge/web-development/basics/graphql/#ask-for-what-you-need-get-exactly-that","text":"Send a GraphQL query to your API and get exactly what you need, nothing more and nothing less. GraphQL queries always return predictable results. Apps using GraphQL are fast and stable because they control the data they get, not the server.","title":"Ask for what you need, get exactly that"},{"location":"knowledge/web-development/basics/graphql/#get-many-resources-in-a-single-request","text":"GraphQL queries access not just the properties of one resource but also smoothly follow references between them. While typical REST APIs require loading your app needs in a single request. Apps using GraphQL can be quick even on slow mobile network connections.","title":"Get many resources in a single request"},{"location":"knowledge/web-development/basics/graphql/#describe-whats-possible-with-a-type-system","text":"GraphQL APIs are organizaed in terms of types and fields, not endpoints. Access the full capabilities of your data from a single endpoint. GraphQL uses types to ensure Apps only ask for what's possible and provide clear and helpful errors. Apps can use types to avoid writing manual parsing code.","title":"Describe what's possible with a type system"},{"location":"knowledge/web-development/basics/graphql/#evolve-your-api-without-versions","text":"Add new fields and types to your GraphQL API without impacting existing queries. Aging fields can be deprecated and hidden from tools. By using a single evolving version, GraphQL APIs give apps continuous access to new features and encourage cleaner, more maintainable server code.","title":"Evolve your API without versions"},{"location":"knowledge/web-development/basics/graphql/#bring-your-own-data-and-code","text":"GraphQL creates a uniform API across your entire application without being limited by a specific storage engine. Write GraphQL APIs that leverage your existing data and code with GraphQL engines available in many languages. You provide functions for each field in the type system, and GraphQL calls them with optimal concurrency. Get more information at here .","title":"Bring your own data and code"},{"location":"knowledge/web-development/basics/integration-testing/","text":"Integration testing Integration testing is the phase in software testing in which individual software modules are combined and tested as a group. Integration testing is conducted to evaluate the compliance of a system or component with specified in an integration test plan to those aggregates, and delivers as its output the integrated system ready for system testing. Why do Integration Testing Althoudh each software module is unit tested, defects still exist for various reasons like A Module, in general, is designed by an individual software developer whose undrstanding and programming logic may differ from other programmers. Integration Testing becomes necessary to verify the software modules work in unity. At the time of module development, there are wide chances of change in requirements by the clients. These new requirements may not be unit tested and hence system ntegration Testing becomes necessary. Interfaces of the software modules with the database could be erroneous. External Hardware interfaces, if any, could be erroneous. 0 Inadequate exception handling could cause issues. Approach Some different types of integration testing are big-bing, mixed(sandwich), risky-harded, top-down, and bottom-up. Other Integration Patterns are: collaboration integration, backbone integration layer integration, client-sever integration, distributed services integration and high frequency integration. Software Engineering defines variety of strategies to execute Integration testing: Big Bang Approach. Incremental Approach: which is further divided into the following: Top Down Approach Bottom Up Approach Sandwich Approach - Combination of Top Down and Bottom Up Big Bang Approach Here all component are integrated together at once and then tested. Advantages Convenient for small systems. Disadvantages Fault Localization is difficult. Given the sheer number of interfaces that need to be tested in this approach, some interfaces link to be tested could be missed easily. Since the Integration testing can commence only after \"all\" the modules are designed, the testing team will have less time for execution in the testing phase. Since all modules are tested at once, high-risk critical modules are not isolated and teted on priority. Peripheral modules which deal with user interfaces are also not isolated and tested on priority. Incremental Approach In this approah, testing is done by joining two or more modules that re logically related . Then the other related modules are added and test for the proper functioning. The process continues until all of the modules are joined and tested sucessfully. Incremental Approach, in turn, is carried out by two different Methods: Bottom Up Top Down What is Sub and Driver? Incremental Approach is carried out by using dummy progrms called Stubs and Drivers . Stubs and Drivers do not implement the entire programming logic of the software module but just simulate data communication with the calling module. Stub : Is called by the Module under Test. Driver : Calls the Module to be tested. Bottom-up Integration In the bottom-up strategy, each module at lower levels is tested with higher modules until all modules are tested. It takes help of Drivers for testing. Advantages Bottom Up Fault localization is easier. No time is wasted waiting for all modules to be developed unlike Big-bang approach. Disadvantages Bottom Up Critical modules which control the flow of application are tested last and may be prone to defects. An early prototype is not possible. Top-down Integration In top to down approach, tsting takes place from top to down following the control flow of the software system. Advantages Top Down Fault Localization is easier. Possibility to obtain an early prototype. Critical Modules are tested on priority; major design flaws could be found and fixed first. Disadvantages Top Down Needs many Stubs. Modules at a lower level are tested inadequately. How tio do Integration Testing? The Integration test procedure irrespective of the Software testing strategies: Prepare the Integrtion Tests Plan Design the Test Scenarios, Cases, and Scripts. Executing the test Cases followed by reporting the defects. Tracking & re-testing the defects. Steps 3 and 4 are repeated until the completion of Integration is successful. Best Practice/ Guideline for Integration Testing First, determine the Integration Test Strategy that could be adopted and later prepare the test cases and test data accordingly. Study the Architecture design of the Application and identify the Critical Modules. Obtain the interface designs from the Architectural team and create test cases to verify all of the interfaces in detail. Interface to database/external hardware/ software application must be tested in detail. After the test cases, it's the test data which plays the critical role. Always have the mock data prepared, prior to executing. Get more inforamtion at here .","title":"Integration Testing"},{"location":"knowledge/web-development/basics/integration-testing/#integration-testing","text":"Integration testing is the phase in software testing in which individual software modules are combined and tested as a group. Integration testing is conducted to evaluate the compliance of a system or component with specified in an integration test plan to those aggregates, and delivers as its output the integrated system ready for system testing.","title":"Integration testing"},{"location":"knowledge/web-development/basics/integration-testing/#why-do-integration-testing","text":"Althoudh each software module is unit tested, defects still exist for various reasons like A Module, in general, is designed by an individual software developer whose undrstanding and programming logic may differ from other programmers. Integration Testing becomes necessary to verify the software modules work in unity. At the time of module development, there are wide chances of change in requirements by the clients. These new requirements may not be unit tested and hence system ntegration Testing becomes necessary. Interfaces of the software modules with the database could be erroneous. External Hardware interfaces, if any, could be erroneous. 0 Inadequate exception handling could cause issues.","title":"Why do Integration Testing"},{"location":"knowledge/web-development/basics/integration-testing/#approach","text":"Some different types of integration testing are big-bing, mixed(sandwich), risky-harded, top-down, and bottom-up. Other Integration Patterns are: collaboration integration, backbone integration layer integration, client-sever integration, distributed services integration and high frequency integration. Software Engineering defines variety of strategies to execute Integration testing: Big Bang Approach. Incremental Approach: which is further divided into the following: Top Down Approach Bottom Up Approach Sandwich Approach - Combination of Top Down and Bottom Up","title":"Approach"},{"location":"knowledge/web-development/basics/integration-testing/#big-bang-approach","text":"Here all component are integrated together at once and then tested.","title":"Big Bang Approach"},{"location":"knowledge/web-development/basics/integration-testing/#advantages","text":"Convenient for small systems.","title":"Advantages"},{"location":"knowledge/web-development/basics/integration-testing/#disadvantages","text":"Fault Localization is difficult. Given the sheer number of interfaces that need to be tested in this approach, some interfaces link to be tested could be missed easily. Since the Integration testing can commence only after \"all\" the modules are designed, the testing team will have less time for execution in the testing phase. Since all modules are tested at once, high-risk critical modules are not isolated and teted on priority. Peripheral modules which deal with user interfaces are also not isolated and tested on priority.","title":"Disadvantages"},{"location":"knowledge/web-development/basics/integration-testing/#incremental-approach","text":"In this approah, testing is done by joining two or more modules that re logically related . Then the other related modules are added and test for the proper functioning. The process continues until all of the modules are joined and tested sucessfully. Incremental Approach, in turn, is carried out by two different Methods: Bottom Up Top Down","title":"Incremental Approach"},{"location":"knowledge/web-development/basics/integration-testing/#what-is-sub-and-driver","text":"Incremental Approach is carried out by using dummy progrms called Stubs and Drivers . Stubs and Drivers do not implement the entire programming logic of the software module but just simulate data communication with the calling module. Stub : Is called by the Module under Test. Driver : Calls the Module to be tested.","title":"What is Sub and Driver?"},{"location":"knowledge/web-development/basics/integration-testing/#bottom-up-integration","text":"In the bottom-up strategy, each module at lower levels is tested with higher modules until all modules are tested. It takes help of Drivers for testing.","title":"Bottom-up Integration"},{"location":"knowledge/web-development/basics/integration-testing/#advantages-bottom-up","text":"Fault localization is easier. No time is wasted waiting for all modules to be developed unlike Big-bang approach.","title":"Advantages Bottom Up"},{"location":"knowledge/web-development/basics/integration-testing/#disadvantages-bottom-up","text":"Critical modules which control the flow of application are tested last and may be prone to defects. An early prototype is not possible.","title":"Disadvantages Bottom Up"},{"location":"knowledge/web-development/basics/integration-testing/#top-down-integration","text":"In top to down approach, tsting takes place from top to down following the control flow of the software system.","title":"Top-down Integration"},{"location":"knowledge/web-development/basics/integration-testing/#advantages-top-down","text":"Fault Localization is easier. Possibility to obtain an early prototype. Critical Modules are tested on priority; major design flaws could be found and fixed first.","title":"Advantages Top Down"},{"location":"knowledge/web-development/basics/integration-testing/#disadvantages-top-down","text":"Needs many Stubs. Modules at a lower level are tested inadequately.","title":"Disadvantages Top Down"},{"location":"knowledge/web-development/basics/integration-testing/#how-tio-do-integration-testing","text":"The Integration test procedure irrespective of the Software testing strategies: Prepare the Integrtion Tests Plan Design the Test Scenarios, Cases, and Scripts. Executing the test Cases followed by reporting the defects. Tracking & re-testing the defects. Steps 3 and 4 are repeated until the completion of Integration is successful.","title":"How tio do Integration Testing?"},{"location":"knowledge/web-development/basics/integration-testing/#best-practice-guideline-for-integration-testing","text":"First, determine the Integration Test Strategy that could be adopted and later prepare the test cases and test data accordingly. Study the Architecture design of the Application and identify the Critical Modules. Obtain the interface designs from the Architectural team and create test cases to verify all of the interfaces in detail. Interface to database/external hardware/ software application must be tested in detail. After the test cases, it's the test data which plays the critical role. Always have the mock data prepared, prior to executing. Get more inforamtion at here .","title":"Best Practice/ Guideline for Integration Testing"},{"location":"knowledge/web-development/basics/introducing-websockets/","text":"Introducing WebSockets: Bringing Sockets to the Web The WebSocket specification defines and API establishing \"socket\" connections between a web browser and a server. In plain words: There is an persistent connection between the client and the server and both parties can start sending data at any time. Getting Started You open up a WebSocket connection simply by calling the WebSocket constructor: var connection = new WebSocket('ws://html5rocks.websocket.org/echo', ['soap', 'xmpp']) Attaching some event handlers immediately to the connection allows you to know when the connection is opened, received incomimg messages, or there is an error. The second argument accepts optional subprotocols. It can be a string or array of strings. Each strng should represent a subprotocol name and server accepts only one of passes subprotocols in the array. Accepted subprotocol can be determined by accessing protocol property of WebSocket object. The subprotocol names must be one of registered subprotocol names in IANA registry . There is currently only one subprotocol name (soap) registered as of February 2012. // When the connection is open, send some data to the server connection.onopen = function () { connection.send('Ping'); // Send the message 'Ping' to the server }; // Log errors connection.onerror = function (error) { console.log('WebSocket Error ' + error); }; // Log messages from the server connection.onmessage = function (e) { console.log('Server: ' + e.data); }; Communicating with the Server As soon as we have a connection to the server (when the open event is fired) we can start sending data to the server using the send('your mess') method on the connection object. It used to support only strings, but in the latest spec it now can send binary messages too. To send binary data, you can use either Blob or ArrayBuffer object. // Sending String connection.send('your message'); // Sending canvas ImageData as ArrayBuffer var img = canvas_context.getImageData(0, 0, 400, 320); var binary = new Uint8Array(img.data.length); for (var i = 0; i < img.data.length; i++) { binary[i] = img.data[i]; } connection.send(binary.buffer); // Sending file as Blob var file = document.querySelector('input[type=\"file\"]').files[0]; connection.send(file); Equally the server might send us messages at any time. Whenever this happens the onmessage callback fires. The callback receives an event object and the actual message is accessible via the data property. WebSocket can also receive binary messages in the latest spec. Binary frames can be received in Blob or ArrayBuffer format. To specify the format of the received binary, set the binaryType property of WebSocket object to either 'blob' or 'arraybuffer'. The default format is 'blob'. (You don't have to align binaryType param on sending.) // Setting binaryType to accept received binary as either 'blob' or 'arraybuffer' connection.binaryType = 'arraybuffer'; connection.onmessage = function(e) { console.log(e.data.byteLength); // ArrayBuffer object if binary }; Another newly added feature of WebSocket is extensions. using extensions, it will be possible to send frames compressed , multiplexed , etc. You can find server accepted extensions by examining the extensions property of the WebSocket object after the open event. There is no officially published extentions spec just yet as of February 2012, // Determining accepted extensions console.log(connection.extensions); Cross Origin Communication Being a modern protocol, cross origin comminication is baked right into WebSOcket. While you should still make sure only to comunicate with clients and servers that you trust, WebSocket enables communication between parties on any domain. The server decides whether to make its service available to all clients or only those that reside on a set of well defined domains. Proxy Servers Every new teachnology comes with a new set of problems. In the case of WebSocket it is the compatibility with proxe servers with meditate HTTP connections in most company networks. The WebSocket protocol uses the HTTP upgrade system (twhich is normally used for HTTP/SSL) to \"upgrade\" and HTTP connection to a WebSocket conneciton. Some proxy servers do not like this and will drop the connection. Thus, even if a given client uses the WebSocket protocol, it may not be possible to establist a connection. This makes the next section even more important. Use WebSockets Today WebSocket is still a young technology and not fully implemented in all browsers. However, you can use WebSocket today with libraries that use one of the fallbacks mentioned above whenever WebSocket is not available. A library that has become very popular in this domain is socket.io which comes with a client and a server implementation of the protocol and includes fallbacks (socket.io doesn't support binary messaging yet as of Februrary 2012). There are also commercial solutions such as PusherApp which can be easily integrated into any web enviroment by providing a HTTP API to send WebSocket messages to clients. Due to the extra HTTP request there will always be extra overhead compared to pure WebSocket. The Server Side Using WebSocket creates a whole new usage pattern for server side applications. While traditional server stacks such LAMP are designed around the HTTP request/response cycle they often do not deal well with a large number of open WebSocket connections. Keeping a large number of connections open at the same time requires an architecture that receives high concurrency at a low performance cost. Such architectures are usually designed around either threading or so callled non-blocking IO. Server Side Implementations Node.js Socket.IO WebSocket-Node ws Java Jetty Ruby EventMachine Python pywebsocket Tornado Erlang Shirasu C++ libwebsockets .NET SuperWebSocket Use Cases Use WebSocket whenever you need a truly low latency, near realtime connection between the client and the server. Keep in mind that this might involve rethinking ho you build your server side applications with a new foces on technologies such as event queues. Some example use cases are: Multiplayer online games Chat applications Live sports ticker Realtime updating social streams References The WebSocket API The WebSocket Protocol WebSockets-MDN Get more information at here .","title":"Introducing WebSockets"},{"location":"knowledge/web-development/basics/introducing-websockets/#introducing-websockets-bringing-sockets-to-the-web","text":"The WebSocket specification defines and API establishing \"socket\" connections between a web browser and a server. In plain words: There is an persistent connection between the client and the server and both parties can start sending data at any time.","title":"Introducing WebSockets: Bringing Sockets to the Web"},{"location":"knowledge/web-development/basics/introducing-websockets/#getting-started","text":"You open up a WebSocket connection simply by calling the WebSocket constructor: var connection = new WebSocket('ws://html5rocks.websocket.org/echo', ['soap', 'xmpp']) Attaching some event handlers immediately to the connection allows you to know when the connection is opened, received incomimg messages, or there is an error. The second argument accepts optional subprotocols. It can be a string or array of strings. Each strng should represent a subprotocol name and server accepts only one of passes subprotocols in the array. Accepted subprotocol can be determined by accessing protocol property of WebSocket object. The subprotocol names must be one of registered subprotocol names in IANA registry . There is currently only one subprotocol name (soap) registered as of February 2012. // When the connection is open, send some data to the server connection.onopen = function () { connection.send('Ping'); // Send the message 'Ping' to the server }; // Log errors connection.onerror = function (error) { console.log('WebSocket Error ' + error); }; // Log messages from the server connection.onmessage = function (e) { console.log('Server: ' + e.data); };","title":"Getting Started"},{"location":"knowledge/web-development/basics/introducing-websockets/#communicating-with-the-server","text":"As soon as we have a connection to the server (when the open event is fired) we can start sending data to the server using the send('your mess') method on the connection object. It used to support only strings, but in the latest spec it now can send binary messages too. To send binary data, you can use either Blob or ArrayBuffer object. // Sending String connection.send('your message'); // Sending canvas ImageData as ArrayBuffer var img = canvas_context.getImageData(0, 0, 400, 320); var binary = new Uint8Array(img.data.length); for (var i = 0; i < img.data.length; i++) { binary[i] = img.data[i]; } connection.send(binary.buffer); // Sending file as Blob var file = document.querySelector('input[type=\"file\"]').files[0]; connection.send(file); Equally the server might send us messages at any time. Whenever this happens the onmessage callback fires. The callback receives an event object and the actual message is accessible via the data property. WebSocket can also receive binary messages in the latest spec. Binary frames can be received in Blob or ArrayBuffer format. To specify the format of the received binary, set the binaryType property of WebSocket object to either 'blob' or 'arraybuffer'. The default format is 'blob'. (You don't have to align binaryType param on sending.) // Setting binaryType to accept received binary as either 'blob' or 'arraybuffer' connection.binaryType = 'arraybuffer'; connection.onmessage = function(e) { console.log(e.data.byteLength); // ArrayBuffer object if binary }; Another newly added feature of WebSocket is extensions. using extensions, it will be possible to send frames compressed , multiplexed , etc. You can find server accepted extensions by examining the extensions property of the WebSocket object after the open event. There is no officially published extentions spec just yet as of February 2012, // Determining accepted extensions console.log(connection.extensions);","title":"Communicating with the Server"},{"location":"knowledge/web-development/basics/introducing-websockets/#cross-origin-communication","text":"Being a modern protocol, cross origin comminication is baked right into WebSOcket. While you should still make sure only to comunicate with clients and servers that you trust, WebSocket enables communication between parties on any domain. The server decides whether to make its service available to all clients or only those that reside on a set of well defined domains.","title":"Cross Origin Communication"},{"location":"knowledge/web-development/basics/introducing-websockets/#proxy-servers","text":"Every new teachnology comes with a new set of problems. In the case of WebSocket it is the compatibility with proxe servers with meditate HTTP connections in most company networks. The WebSocket protocol uses the HTTP upgrade system (twhich is normally used for HTTP/SSL) to \"upgrade\" and HTTP connection to a WebSocket conneciton. Some proxy servers do not like this and will drop the connection. Thus, even if a given client uses the WebSocket protocol, it may not be possible to establist a connection. This makes the next section even more important.","title":"Proxy Servers"},{"location":"knowledge/web-development/basics/introducing-websockets/#use-websockets-today","text":"WebSocket is still a young technology and not fully implemented in all browsers. However, you can use WebSocket today with libraries that use one of the fallbacks mentioned above whenever WebSocket is not available. A library that has become very popular in this domain is socket.io which comes with a client and a server implementation of the protocol and includes fallbacks (socket.io doesn't support binary messaging yet as of Februrary 2012). There are also commercial solutions such as PusherApp which can be easily integrated into any web enviroment by providing a HTTP API to send WebSocket messages to clients. Due to the extra HTTP request there will always be extra overhead compared to pure WebSocket.","title":"Use WebSockets Today"},{"location":"knowledge/web-development/basics/introducing-websockets/#the-server-side","text":"Using WebSocket creates a whole new usage pattern for server side applications. While traditional server stacks such LAMP are designed around the HTTP request/response cycle they often do not deal well with a large number of open WebSocket connections. Keeping a large number of connections open at the same time requires an architecture that receives high concurrency at a low performance cost. Such architectures are usually designed around either threading or so callled non-blocking IO.","title":"The Server Side"},{"location":"knowledge/web-development/basics/introducing-websockets/#server-side-implementations","text":"Node.js Socket.IO WebSocket-Node ws Java Jetty Ruby EventMachine Python pywebsocket Tornado Erlang Shirasu C++ libwebsockets .NET SuperWebSocket","title":"Server Side Implementations"},{"location":"knowledge/web-development/basics/introducing-websockets/#use-cases","text":"Use WebSocket whenever you need a truly low latency, near realtime connection between the client and the server. Keep in mind that this might involve rethinking ho you build your server side applications with a new foces on technologies such as event queues. Some example use cases are: Multiplayer online games Chat applications Live sports ticker Realtime updating social streams","title":"Use Cases"},{"location":"knowledge/web-development/basics/introducing-websockets/#references","text":"The WebSocket API The WebSocket Protocol WebSockets-MDN Get more information at here .","title":"References"},{"location":"knowledge/web-development/basics/regression-testing/","text":"Regression Testing What is Regression Testing? Regression Testing is defuned as a type of software testing to confirm that a recent program or code change has not adversely affected existing features. Regression Testing is nothing of already executed test cases which are re-excuted to ensure existing functionalities work find. This testing is done to make sure that new code changes should not have side effects on the existing functinalities. It ensures that the old code still works once the new code changes are done. Need of Regression Testing Regression Testing is requred when there is a Change in requirements and code is modified according to the requirement. New feature is added to the software. Defect fixing. Performance issue fix. How to do Regression Testing Software maintence is an activity which includes enhancements, error corrections, optimization and deletion of existing features. These modifications may cause the system to work incorrectly. Therefore, Regression Testing becomes necessary. Regression Testing can be carried out using the follwoing techniques: Retest All This is one of themethods for Regression Testing in which all the tests in the existing test bucket or suite should be re-executed. This is very expensive as it requires huge time and resources. Regression Test Selection Instead of re-executing the entire test suite, it is better to select part of the test suite to be run. Test cases selected can be categorized as 1) Reusable Test Cases 2) Obsolete Test Cases. Re-usable Test cases can be used in succeeding regression cycles. Obsolete Test Cases can't be used in succeeding cycles. Prioritization of Test Cases Prioritize the test cases depending on business impact, critical & frequently used functionalities. Selection of test cases based on priority will greatly reduce the regression test suite. Selecting test cases for regression testing Effective Regression Tests can be done by selecting the following test cases: Test cases which have frequent defects. Functionalities which are more visible to the users. Test cases which verify core features of the product. Test cases of Functionalities which has undergone more and recent changes. All Integration Test Cases. All Complex Test Cases. Boundary value test cases. A sample of Successful test cases. A sample of Failure test cases. Regression Testing and Configuration Management Configuration Management during Regression Testing becomes imperative in Agile Enviroments where a code is being continuously modified. To ensure effective regression tests, observe the following: Code being regression tested should be under a configuration management tool. No changes must be allowed to coe during the regression test phase. Regression test code must be kept immune to developer changes. The database used for regression testing must be isolated. No databse changes must be allowed. Get more information at here .","title":"Regression Testing"},{"location":"knowledge/web-development/basics/regression-testing/#regression-testing","text":"","title":"Regression Testing"},{"location":"knowledge/web-development/basics/regression-testing/#what-is-regression-testing","text":"Regression Testing is defuned as a type of software testing to confirm that a recent program or code change has not adversely affected existing features. Regression Testing is nothing of already executed test cases which are re-excuted to ensure existing functionalities work find. This testing is done to make sure that new code changes should not have side effects on the existing functinalities. It ensures that the old code still works once the new code changes are done.","title":"What is Regression Testing?"},{"location":"knowledge/web-development/basics/regression-testing/#need-of-regression-testing","text":"Regression Testing is requred when there is a Change in requirements and code is modified according to the requirement. New feature is added to the software. Defect fixing. Performance issue fix.","title":"Need of Regression Testing"},{"location":"knowledge/web-development/basics/regression-testing/#how-to-do-regression-testing","text":"Software maintence is an activity which includes enhancements, error corrections, optimization and deletion of existing features. These modifications may cause the system to work incorrectly. Therefore, Regression Testing becomes necessary. Regression Testing can be carried out using the follwoing techniques:","title":"How to do Regression Testing"},{"location":"knowledge/web-development/basics/regression-testing/#retest-all","text":"This is one of themethods for Regression Testing in which all the tests in the existing test bucket or suite should be re-executed. This is very expensive as it requires huge time and resources.","title":"Retest All"},{"location":"knowledge/web-development/basics/regression-testing/#regression-test-selection","text":"Instead of re-executing the entire test suite, it is better to select part of the test suite to be run. Test cases selected can be categorized as 1) Reusable Test Cases 2) Obsolete Test Cases. Re-usable Test cases can be used in succeeding regression cycles. Obsolete Test Cases can't be used in succeeding cycles.","title":"Regression Test Selection"},{"location":"knowledge/web-development/basics/regression-testing/#prioritization-of-test-cases","text":"Prioritize the test cases depending on business impact, critical & frequently used functionalities. Selection of test cases based on priority will greatly reduce the regression test suite.","title":"Prioritization of Test Cases"},{"location":"knowledge/web-development/basics/regression-testing/#selecting-test-cases-for-regression-testing","text":"Effective Regression Tests can be done by selecting the following test cases: Test cases which have frequent defects. Functionalities which are more visible to the users. Test cases which verify core features of the product. Test cases of Functionalities which has undergone more and recent changes. All Integration Test Cases. All Complex Test Cases. Boundary value test cases. A sample of Successful test cases. A sample of Failure test cases.","title":"Selecting test cases for regression testing"},{"location":"knowledge/web-development/basics/regression-testing/#regression-testing-and-configuration-management","text":"Configuration Management during Regression Testing becomes imperative in Agile Enviroments where a code is being continuously modified. To ensure effective regression tests, observe the following: Code being regression tested should be under a configuration management tool. No changes must be allowed to coe during the regression test phase. Regression test code must be kept immune to developer changes. The database used for regression testing must be isolated. No databse changes must be allowed. Get more information at here .","title":"Regression Testing and Configuration Management"},{"location":"knowledge/web-development/basics/unit-testing/","text":"Unit Testing Description Unit tests are typically autmated tests written and run by software developers to ensure that a section of an application meets its design and behaves as intended. In procedural programming, a unit could be an entire module, but it is more commonly an individual function or procedure. In object-oriented programming, a unit is often an entire interface, such as a class, but could be an individual method. By writing tests first for the smallest testable units, then the compound behaviors between those , one can build up comprehensive tests for complex applications. Advantages The goal of unit testing is to isolate each part of the program and show that the individual parts are correct. A unit provides a strict, written contract that the piece of code must satisfy. As a resul, it affords several benefits. Unit testing finds problems early in the development cycle. his includes both bugs in the programmer's implementation and flaws or missing parts of the specification for the unit. The process of writing a thorough set of tests forces the author to think through inputs, outputs, and error conditions, and thust more cresply define the unit's desired behavior. The cost of finding a bug before coding begins or when the code is first written is conisderably lower than the coset of detecting, identifying, and correcting the bug later. Bugs in released code may also cause costly problems for the end-users of the software. Code can be impossible or difficult to unit test if poorly written, thus unit testing can force developers to structure functions and objects in better ways. Unit testing provides a sort of living documentation of the system. Devleopers looking to learn what funcutionality is provided by a unit, and how to use it, can look at the unit tests to gain a basic understanding of the unit's interface. Unit testing may reduce uncertainty in the units themselves and can be used in a bottom-up testing style approach. By testing the parts of a program first and then testing the sum of its parts, integration testing becomes much easier. Developers looking to learn what functionality is provided by a unit and how to use it can look at the unit tests to gain a basic understanding for the unit API. Unit testing allows the programmer to refactor code at a later date, and make sure the module still works correctly. The procedure is to write test cases for all functions and methods so that whenver a change causes a fault, it can be quickly identified and fixed. Due to the modular nature of the unit testin, we cant test parts of the project without waiting for others to be completed. Limitations and disadvantages Testing will not catch every error in the program , because it cannot evluate every execution path in any but the most trivial programs. This problem is a superset of the halting problem, which is undecidable. The same is tru for unit testing, Additionally, unit testing by definition only tests the functionality of the units themselves. Therefore, it will not catch integration errors or broader system-level errors (such as functions performed across multiple units, o non-functional test areas such as perfromace). TUnt testing should be done in conjunction with other software testin activities, as they only show the presence or absence of partiular errors; they cannot prove a complete absence of errors. to guarantee correct behavior for every execution path and every possible input and ensure the absence of errors, other techniques are required, namely the application of formal methods to proving that a software component has no unexpected behavior. Unit testing can't be expected to catch every error in a program. It is not possible to evaluate all execution paths even in the most trivial programs. Unit testing by its very nature focused on a unit of code. Hence it can't catch integration errors or broad system level errors. Unit Testing Best Practices Unit Test cases should be independent. In case of any enhancements or change in requirements, unit test cases should not be affected. Test only one code at a time. Follow clear and consistent naming conventions for your unit tests. In case of a change in code in any module, eunsure there is a corresponding unit Test Case for the modeul, and the module passes the tests before changing the implementation. Bugs identified during unit testing must be fixed before proceeding to the next phase in SDLC Adopt a \"test as your code\" approach. The more code you write without testing, the more paths you have to check for errors. Unit Teating: Mock Objects Unit testing relies on mock objects being created to est sections of code that are not yet part of a complete application. Mock objects fill in for the missing parts of the program. For example, you might have a function that needs variables or objects that are not created yet. In unit testing, those will be accounted for inthe form of mock objects created solely for the purpose of the unit testing done on that section of code. Unit Testing Tools There are several automated tools available to assist with unit testing. We will provide a few examples below: Jtest : Parasoft Jtest is an IDE plucgin that leverages open-source frameworks(Junit, Mockito, PowerMock, and Spring) with guided and easy one-click actions for creating, scaling, and maintaining unit tests. Junit : Junit is a free to use teing tool used for Java programming language. It provides assertions to identify test method. This tool test data first and then inserted in the piece of code. NUnit : NUnit is widely used unit-testing framework use for all .net languages. JMockit :: JMockit is open source Unit testing tool. It is a code coverage tool with line and path metrics. EMMA : EMMA is an open-souce toolkit for analyzing and reporting code written in Java language. Emma support coverge types like method, line, basic block. Test Driven Development(TDD) & Unit Testing Unit testing in TDD involves an extensive use of testing framewotk. A unit test framework is used in order to create automated unit tests. Unit testing frameworks are not unique to TDD, but thwy are essential to it. Below we look at some of what TDD brings to the world of unit testing: Tests are written before the code. Rely heavily on testing frameworks. All classes in the applications are tested. Quick and easy integration is made possible. Unit Testing Myth Myth: It requires time, and I am always oversheduled My code is rock solid! I do not need unit tests. Myths by their very nature are false assumptions. These assumptions lead to a vicious cycle as follows: Truth is Unit testing increase the speed of development. Once units are integrated, very simple errors which could have very easily found and fixed in unit tested take a very long time to be traced and fixed. Summary There can be alot involved in unit testing. It can be complex or rather simple depending on the application being tested and the testing strategies, tools and philosophies used. Unit testing is always necessary on some level. That is a certainty. Get more information at here .","title":"Unit Testing"},{"location":"knowledge/web-development/basics/unit-testing/#unit-testing","text":"","title":"Unit Testing"},{"location":"knowledge/web-development/basics/unit-testing/#description","text":"Unit tests are typically autmated tests written and run by software developers to ensure that a section of an application meets its design and behaves as intended. In procedural programming, a unit could be an entire module, but it is more commonly an individual function or procedure. In object-oriented programming, a unit is often an entire interface, such as a class, but could be an individual method. By writing tests first for the smallest testable units, then the compound behaviors between those , one can build up comprehensive tests for complex applications.","title":"Description"},{"location":"knowledge/web-development/basics/unit-testing/#advantages","text":"The goal of unit testing is to isolate each part of the program and show that the individual parts are correct. A unit provides a strict, written contract that the piece of code must satisfy. As a resul, it affords several benefits. Unit testing finds problems early in the development cycle. his includes both bugs in the programmer's implementation and flaws or missing parts of the specification for the unit. The process of writing a thorough set of tests forces the author to think through inputs, outputs, and error conditions, and thust more cresply define the unit's desired behavior. The cost of finding a bug before coding begins or when the code is first written is conisderably lower than the coset of detecting, identifying, and correcting the bug later. Bugs in released code may also cause costly problems for the end-users of the software. Code can be impossible or difficult to unit test if poorly written, thus unit testing can force developers to structure functions and objects in better ways. Unit testing provides a sort of living documentation of the system. Devleopers looking to learn what funcutionality is provided by a unit, and how to use it, can look at the unit tests to gain a basic understanding of the unit's interface. Unit testing may reduce uncertainty in the units themselves and can be used in a bottom-up testing style approach. By testing the parts of a program first and then testing the sum of its parts, integration testing becomes much easier. Developers looking to learn what functionality is provided by a unit and how to use it can look at the unit tests to gain a basic understanding for the unit API. Unit testing allows the programmer to refactor code at a later date, and make sure the module still works correctly. The procedure is to write test cases for all functions and methods so that whenver a change causes a fault, it can be quickly identified and fixed. Due to the modular nature of the unit testin, we cant test parts of the project without waiting for others to be completed.","title":"Advantages"},{"location":"knowledge/web-development/basics/unit-testing/#limitations-and-disadvantages","text":"Testing will not catch every error in the program , because it cannot evluate every execution path in any but the most trivial programs. This problem is a superset of the halting problem, which is undecidable. The same is tru for unit testing, Additionally, unit testing by definition only tests the functionality of the units themselves. Therefore, it will not catch integration errors or broader system-level errors (such as functions performed across multiple units, o non-functional test areas such as perfromace). TUnt testing should be done in conjunction with other software testin activities, as they only show the presence or absence of partiular errors; they cannot prove a complete absence of errors. to guarantee correct behavior for every execution path and every possible input and ensure the absence of errors, other techniques are required, namely the application of formal methods to proving that a software component has no unexpected behavior. Unit testing can't be expected to catch every error in a program. It is not possible to evaluate all execution paths even in the most trivial programs. Unit testing by its very nature focused on a unit of code. Hence it can't catch integration errors or broad system level errors.","title":"Limitations and disadvantages"},{"location":"knowledge/web-development/basics/unit-testing/#unit-testing-best-practices","text":"Unit Test cases should be independent. In case of any enhancements or change in requirements, unit test cases should not be affected. Test only one code at a time. Follow clear and consistent naming conventions for your unit tests. In case of a change in code in any module, eunsure there is a corresponding unit Test Case for the modeul, and the module passes the tests before changing the implementation. Bugs identified during unit testing must be fixed before proceeding to the next phase in SDLC Adopt a \"test as your code\" approach. The more code you write without testing, the more paths you have to check for errors.","title":"Unit Testing Best Practices"},{"location":"knowledge/web-development/basics/unit-testing/#unit-teating-mock-objects","text":"Unit testing relies on mock objects being created to est sections of code that are not yet part of a complete application. Mock objects fill in for the missing parts of the program. For example, you might have a function that needs variables or objects that are not created yet. In unit testing, those will be accounted for inthe form of mock objects created solely for the purpose of the unit testing done on that section of code.","title":"Unit Teating: Mock Objects"},{"location":"knowledge/web-development/basics/unit-testing/#unit-testing-tools","text":"There are several automated tools available to assist with unit testing. We will provide a few examples below: Jtest : Parasoft Jtest is an IDE plucgin that leverages open-source frameworks(Junit, Mockito, PowerMock, and Spring) with guided and easy one-click actions for creating, scaling, and maintaining unit tests. Junit : Junit is a free to use teing tool used for Java programming language. It provides assertions to identify test method. This tool test data first and then inserted in the piece of code. NUnit : NUnit is widely used unit-testing framework use for all .net languages. JMockit :: JMockit is open source Unit testing tool. It is a code coverage tool with line and path metrics. EMMA : EMMA is an open-souce toolkit for analyzing and reporting code written in Java language. Emma support coverge types like method, line, basic block.","title":"Unit Testing Tools"},{"location":"knowledge/web-development/basics/unit-testing/#test-driven-developmenttdd-unit-testing","text":"Unit testing in TDD involves an extensive use of testing framewotk. A unit test framework is used in order to create automated unit tests. Unit testing frameworks are not unique to TDD, but thwy are essential to it. Below we look at some of what TDD brings to the world of unit testing: Tests are written before the code. Rely heavily on testing frameworks. All classes in the applications are tested. Quick and easy integration is made possible.","title":"Test Driven Development(TDD) &amp; Unit Testing"},{"location":"knowledge/web-development/basics/unit-testing/#unit-testing-myth","text":"Myth: It requires time, and I am always oversheduled My code is rock solid! I do not need unit tests. Myths by their very nature are false assumptions. These assumptions lead to a vicious cycle as follows: Truth is Unit testing increase the speed of development. Once units are integrated, very simple errors which could have very easily found and fixed in unit tested take a very long time to be traced and fixed.","title":"Unit Testing Myth"},{"location":"knowledge/web-development/basics/unit-testing/#summary","text":"There can be alot involved in unit testing. It can be complex or rather simple depending on the application being tested and the testing strategies, tools and philosophies used. Unit testing is always necessary on some level. That is a certainty. Get more information at here .","title":"Summary"},{"location":"knowledge/web-development/nodejs/libuv/","text":"Libuv Introduction Libuv is a multi-platform support library with a focus on asynchronous I/O. It was primarily developed for use by Node.js, but it's also used by Luvit , Julia , pyuv and others . Features Full-featured event loop backed by epoll, k queue, IOCP,event ports. Asynchronous TCP and UDP sockets. Asynchronous DNS resolution. Asynchronous file and file system operations. File system events. ANSI escape code controlled TTY. IPC with socket sharing, using Unix domain sockets or named pipes (Windows). Child processes. Thread pool. Signal handling. High resolution clock. Threading and synchronization primitives. To be continued! Get more information at here .","title":"Libuv"},{"location":"knowledge/web-development/nodejs/libuv/#libuv","text":"","title":"Libuv"},{"location":"knowledge/web-development/nodejs/libuv/#introduction","text":"Libuv is a multi-platform support library with a focus on asynchronous I/O. It was primarily developed for use by Node.js, but it's also used by Luvit , Julia , pyuv and others .","title":"Introduction"},{"location":"knowledge/web-development/nodejs/libuv/#features","text":"Full-featured event loop backed by epoll, k queue, IOCP,event ports. Asynchronous TCP and UDP sockets. Asynchronous DNS resolution. Asynchronous file and file system operations. File system events. ANSI escape code controlled TTY. IPC with socket sharing, using Unix domain sockets or named pipes (Windows). Child processes. Thread pool. Signal handling. High resolution clock. Threading and synchronization primitives. To be continued! Get more information at here .","title":"Features"},{"location":"knowledge/web-development/nodejs/overview-of-blocking-and-non-blocking/","text":"Overview of Blocking and Non-Blocking \"I/O\" refers primatity to interation with the system's disk and network supported by libuv. Blocking Blocking is when the execution of additional JavaScript in the Node.js process must wait until a non-JavaScript operation completes. This heppens because the event loop is unable to continue running JavaScript while a blocking operation is occurring. In Node.js, JavaScript that exhibits poor performance due to being CPU intensive rather than waiting on a non-JavaScript operation, such as I/O, isn't typically referred to as blocking . Synchromous methods in the Node.js standard library that use libuv are the most commonly used blocking operations. Native modules may also have blocking methods. All of the I/O methods in the Node.js standard library provvide asynchronous versions, which are non-blocking , and accept callback functions. Some methods also have blocking counterparts, which have names that end with Sync . This is a synchronous file read: const fs = require(\"fs\"); const data = fs.readFileSync(\"/file.md\"); // blocks here until file is read And here is an equivalent asynchronous example: const fs = require(\"fs\"); const data = fs.readFileSync(\"/file.md\"); // blocks here until file is read Concurrency and Throughput JavaScript execution in in Node.js is single threaded, so concurrency refers to the event loop's capacity to execute JavaScript callback functions after completing other work. Any code that is expected to run in a concurrent manner must allow the event loop to continue running as non-JavaScript operations, like I/O, are occurring. The event loop is different than models in many other languages where additional threads may be created to handle concurrent work. Dangers of Mixing Blocking and Non-Blocking Code There are some patterns that should be avoided when dealing with I/O. Let's look at an example: const fs = require(\"fs\"); fs.readFile(\"/file.md\", (err, data) => { if (err) throw err; console.log(data); }); fs.unlinkSync(\"/file.md\"); In the above example, fs.unlinkSync() is likely to be run before fs.readFile() , which would delete file.md before it is actually read. A better way to weite this, which is completely non-blocking and guaranteed to execute in the correct order is: const fs = require(\"fs\"); fs.readFile(\"/file.md\", (readFileErr, data) => { if (readFileErr) throw readFileErr; console.log(data); fs.unlink(\"/file.md\", unlinkErr => { if (unlinkErr) throw unlinkErr; }); }); The above places a non-blocking cal to fs.unlink() within the callback of fs.readFile() which guarantees the correct order of operations. Additional Resources libuv About Node.js Get more information at here .","title":"Overview of Blocking and Non-Blocking"},{"location":"knowledge/web-development/nodejs/overview-of-blocking-and-non-blocking/#overview-of-blocking-and-non-blocking","text":"\"I/O\" refers primatity to interation with the system's disk and network supported by libuv.","title":"Overview of Blocking and Non-Blocking"},{"location":"knowledge/web-development/nodejs/overview-of-blocking-and-non-blocking/#blocking","text":"Blocking is when the execution of additional JavaScript in the Node.js process must wait until a non-JavaScript operation completes. This heppens because the event loop is unable to continue running JavaScript while a blocking operation is occurring. In Node.js, JavaScript that exhibits poor performance due to being CPU intensive rather than waiting on a non-JavaScript operation, such as I/O, isn't typically referred to as blocking . Synchromous methods in the Node.js standard library that use libuv are the most commonly used blocking operations. Native modules may also have blocking methods. All of the I/O methods in the Node.js standard library provvide asynchronous versions, which are non-blocking , and accept callback functions. Some methods also have blocking counterparts, which have names that end with Sync . This is a synchronous file read: const fs = require(\"fs\"); const data = fs.readFileSync(\"/file.md\"); // blocks here until file is read And here is an equivalent asynchronous example: const fs = require(\"fs\"); const data = fs.readFileSync(\"/file.md\"); // blocks here until file is read","title":"Blocking"},{"location":"knowledge/web-development/nodejs/overview-of-blocking-and-non-blocking/#concurrency-and-throughput","text":"JavaScript execution in in Node.js is single threaded, so concurrency refers to the event loop's capacity to execute JavaScript callback functions after completing other work. Any code that is expected to run in a concurrent manner must allow the event loop to continue running as non-JavaScript operations, like I/O, are occurring. The event loop is different than models in many other languages where additional threads may be created to handle concurrent work.","title":"Concurrency and Throughput"},{"location":"knowledge/web-development/nodejs/overview-of-blocking-and-non-blocking/#dangers-of-mixing-blocking-and-non-blocking-code","text":"There are some patterns that should be avoided when dealing with I/O. Let's look at an example: const fs = require(\"fs\"); fs.readFile(\"/file.md\", (err, data) => { if (err) throw err; console.log(data); }); fs.unlinkSync(\"/file.md\"); In the above example, fs.unlinkSync() is likely to be run before fs.readFile() , which would delete file.md before it is actually read. A better way to weite this, which is completely non-blocking and guaranteed to execute in the correct order is: const fs = require(\"fs\"); fs.readFile(\"/file.md\", (readFileErr, data) => { if (readFileErr) throw readFileErr; console.log(data); fs.unlink(\"/file.md\", unlinkErr => { if (unlinkErr) throw unlinkErr; }); }); The above places a non-blocking cal to fs.unlink() within the callback of fs.readFile() which guarantees the correct order of operations.","title":"Dangers of Mixing Blocking and Non-Blocking Code"},{"location":"knowledge/web-development/nodejs/overview-of-blocking-and-non-blocking/#additional-resources","text":"libuv About Node.js Get more information at here .","title":"Additional Resources"},{"location":"knowledge/web-development/nodejs/what-is-the-event-loop/","text":"The Node.js Event Loop, Timers, and process.nextTick() What is the Event Loop? The event loop is what allows Node.js to perform non-blocking I/O operations - despite the fact that JavaScript is single-threaded - by offloading operations to the system kernel whenever possible. Since most modern kernels are multi-threaded, they can handle multiple operations executing in the background. When on of these operations completes, the kernel tells Node.js so that the appropriate callback may be added to the poll queue to eventually be executed. Event Loop Explained When Node.js starts, it initializes the event loop, processes the provided input script (or drops into the REPL , which is not covered in this document) which may make async API calls, schedule timers, or call process.nextTick() , then begins processing the event loop. The following diagram shows a simplified overview of the event loop's order of operations. \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500>\u2502 timers \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 pending callbacks \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 idle, prepare \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 incoming: \u2502 \u2502 \u2502 poll \u2502<\u2500\u2500\u2500\u2500\u2500\u2524 connections, \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 data, etc. \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 check \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2514\u2500\u2500\u2524 close callbacks \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 note: each box will be referred to as a \"phase\" of the event loop. Each phase has a FIFO queuse of callbacks to execte. While each phase is special in its own way, generally, when the event loop enters a given phase, it will perform any operations specific to that phase, then execute callbacks in that phase's quese until the queue has been exhausted or the maximum number of callbacks has executed. When the queue has been exhausted or the callback limit is reached, the event loop will move to the next phase, and so on. Since any of these operations may schedule more operations and new events processed in the poll phase are queued by the kernel, poll events can be queued while polling events are being processed. As a result, long running callbacks can allow the poll phase to run much longer than timers's threshold. See the timers and poll sections for more details. NOTE : There is a slight discrepency between the Windows and the Unix/Linux implementation, but that's not important for this demonstration. There are actually seven or eight steps, but the ones we care about - ones that Node.js actuall uses - are those above. Phases Overview timers : this phase executes callbacks scheduled by setTimeout() and setInterval() . pending callbacks : executes I/O callbacks deferred to the next loop iteration. idle, prepare : only used internally. poll : retrieve new I/O events; execute I/O related callbacks(almost all with the exception of close callbacks, the ones scheduled by timers, and setImmediate() ); node will block here when appropriate. check : setImmediate() callbacks are invoked here. close callbacks : some close callbacks, e.g socket.on('close', ...) . Phases in Detail timers A timers specifies the threshold after which a provided callback my be executed rather than the exact time a person wants it to be executed. Timers callbacks will run as early as they can be scheduled after the specified amount of time has passed; however, Operating System scheduling or the running of other callbacks may delay them. Note : *Technically, the poll phase controls when timers are executed.* Note: To prevent the poll phase from starving the event loop, libuv (the C library that implements the Node.js event loop and all of the asynchronous behaviors of the platform) also has a hard maximum (system dependent) before it stops polling for more events. pending callbacks This phase executes callbacks for some system operations such as types of TCP errors. For example if a TCP socket receives ECONNREFUSED when attempting to connect, some *nix systems want to wait to report the error. This will be queused to execute the pending callbacks phase. poll The poll phase has two main functions: Calculating how long it should block and poll for I/O, then Processing events in the poll queue. Once the poll queue is empty the event loop will check for timers whose time thresholds have been reached. If one or more timers are ready, the event loop will wrap back to the timers phase to execute those timers's callbacks. check This phase allows a person to execute callbacks immediately after the poll phase has completed. If the poll phase becomes idle and scripts have been queued with setImmediate() , the event loop may contimue to the check phase rether than waiting. setImmediate() is actually a special timer that runs in a separate phase of the event loop. It uses a libuv API that schedules callbacks to execute after the poll phase has completed. close callbacks if a socket or handle is closed abruptly (e.g socket.destroy() ), the 'close' event will be emitted in this phase. Otherwise it will be emitted via process.nextTick() . The main advantage to using setImmediate() over setTimeout() is setImmediate() will always be executed before any timers if scheduled within an I/O cycle, independently of how many timers are present. process.nextTick() was not displayed in the diagram, even though it's a part of the asynchromous API. This is because process.nextTick is not technically part of the event loop. Instead, the nextTickQueue will be processed after the current operation is completed, regardless of the current phase of the event loop. Here, and operation is defined as a transition from the underlying C/C++ handler, and handing the JavaScript that needs to be executed. Why use process.nextTick() ? There are two main reasons: Allow users to handle errors, cleanup any then unneeded resources, or perhaps try the request again before the event loop continues. At times it's necessay to allow a callback to run after the call stack has unwound but before the event loop continues. Get more information at here .","title":"The Node.js Event Loop"},{"location":"knowledge/web-development/nodejs/what-is-the-event-loop/#the-nodejs-event-loop-timers-and-processnexttick","text":"","title":"The Node.js Event Loop, Timers, and process.nextTick()"},{"location":"knowledge/web-development/nodejs/what-is-the-event-loop/#what-is-the-event-loop","text":"The event loop is what allows Node.js to perform non-blocking I/O operations - despite the fact that JavaScript is single-threaded - by offloading operations to the system kernel whenever possible. Since most modern kernels are multi-threaded, they can handle multiple operations executing in the background. When on of these operations completes, the kernel tells Node.js so that the appropriate callback may be added to the poll queue to eventually be executed.","title":"What is the Event Loop?"},{"location":"knowledge/web-development/nodejs/what-is-the-event-loop/#event-loop-explained","text":"When Node.js starts, it initializes the event loop, processes the provided input script (or drops into the REPL , which is not covered in this document) which may make async API calls, schedule timers, or call process.nextTick() , then begins processing the event loop. The following diagram shows a simplified overview of the event loop's order of operations. \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500>\u2502 timers \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 pending callbacks \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 idle, prepare \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 incoming: \u2502 \u2502 \u2502 poll \u2502<\u2500\u2500\u2500\u2500\u2500\u2524 connections, \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 data, etc. \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 check \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2514\u2500\u2500\u2524 close callbacks \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 note: each box will be referred to as a \"phase\" of the event loop. Each phase has a FIFO queuse of callbacks to execte. While each phase is special in its own way, generally, when the event loop enters a given phase, it will perform any operations specific to that phase, then execute callbacks in that phase's quese until the queue has been exhausted or the maximum number of callbacks has executed. When the queue has been exhausted or the callback limit is reached, the event loop will move to the next phase, and so on. Since any of these operations may schedule more operations and new events processed in the poll phase are queued by the kernel, poll events can be queued while polling events are being processed. As a result, long running callbacks can allow the poll phase to run much longer than timers's threshold. See the timers and poll sections for more details. NOTE : There is a slight discrepency between the Windows and the Unix/Linux implementation, but that's not important for this demonstration. There are actually seven or eight steps, but the ones we care about - ones that Node.js actuall uses - are those above.","title":"Event Loop Explained"},{"location":"knowledge/web-development/nodejs/what-is-the-event-loop/#phases-overview","text":"timers : this phase executes callbacks scheduled by setTimeout() and setInterval() . pending callbacks : executes I/O callbacks deferred to the next loop iteration. idle, prepare : only used internally. poll : retrieve new I/O events; execute I/O related callbacks(almost all with the exception of close callbacks, the ones scheduled by timers, and setImmediate() ); node will block here when appropriate. check : setImmediate() callbacks are invoked here. close callbacks : some close callbacks, e.g socket.on('close', ...) .","title":"Phases Overview"},{"location":"knowledge/web-development/nodejs/what-is-the-event-loop/#phases-in-detail","text":"","title":"Phases in Detail"},{"location":"knowledge/web-development/nodejs/what-is-the-event-loop/#timers","text":"A timers specifies the threshold after which a provided callback my be executed rather than the exact time a person wants it to be executed. Timers callbacks will run as early as they can be scheduled after the specified amount of time has passed; however, Operating System scheduling or the running of other callbacks may delay them. Note : *Technically, the poll phase controls when timers are executed.* Note: To prevent the poll phase from starving the event loop, libuv (the C library that implements the Node.js event loop and all of the asynchronous behaviors of the platform) also has a hard maximum (system dependent) before it stops polling for more events.","title":"timers"},{"location":"knowledge/web-development/nodejs/what-is-the-event-loop/#pending-callbacks","text":"This phase executes callbacks for some system operations such as types of TCP errors. For example if a TCP socket receives ECONNREFUSED when attempting to connect, some *nix systems want to wait to report the error. This will be queused to execute the pending callbacks phase.","title":"pending callbacks"},{"location":"knowledge/web-development/nodejs/what-is-the-event-loop/#poll","text":"The poll phase has two main functions: Calculating how long it should block and poll for I/O, then Processing events in the poll queue. Once the poll queue is empty the event loop will check for timers whose time thresholds have been reached. If one or more timers are ready, the event loop will wrap back to the timers phase to execute those timers's callbacks.","title":"poll"},{"location":"knowledge/web-development/nodejs/what-is-the-event-loop/#check","text":"This phase allows a person to execute callbacks immediately after the poll phase has completed. If the poll phase becomes idle and scripts have been queued with setImmediate() , the event loop may contimue to the check phase rether than waiting. setImmediate() is actually a special timer that runs in a separate phase of the event loop. It uses a libuv API that schedules callbacks to execute after the poll phase has completed.","title":"check"},{"location":"knowledge/web-development/nodejs/what-is-the-event-loop/#close-callbacks","text":"if a socket or handle is closed abruptly (e.g socket.destroy() ), the 'close' event will be emitted in this phase. Otherwise it will be emitted via process.nextTick() . The main advantage to using setImmediate() over setTimeout() is setImmediate() will always be executed before any timers if scheduled within an I/O cycle, independently of how many timers are present. process.nextTick() was not displayed in the diagram, even though it's a part of the asynchromous API. This is because process.nextTick is not technically part of the event loop. Instead, the nextTickQueue will be processed after the current operation is completed, regardless of the current phase of the event loop. Here, and operation is defined as a transition from the underlying C/C++ handler, and handing the JavaScript that needs to be executed.","title":"close callbacks"},{"location":"knowledge/web-development/nodejs/what-is-the-event-loop/#why-use-processnexttick","text":"There are two main reasons: Allow users to handle errors, cleanup any then unneeded resources, or perhaps try the request again before the event loop continues. At times it's necessay to allow a callback to run after the call stack has unwound but before the event loop continues. Get more information at here .","title":"Why use process.nextTick()?"},{"location":"knowledge/web-development/ruby-on-rails/rails-active-record-callbacks/","text":"Active Recore Callbacks Get more information at here .","title":"Active Record Callbacks"},{"location":"knowledge/web-development/ruby-on-rails/rails-active-record-callbacks/#active-recore-callbacks","text":"Get more information at here .","title":"Active Recore Callbacks"},{"location":"knowledge/web-development/ruby-on-rails/rails-form-expand/","text":"Rails form_with - alternative to form_for and form_tag Rails 5.1 added form_with form helper method that provides capabilities of form_for and form_tag . Rails unified form_for and form_tag that provide similar interfaces to generate forms with form_with helper. - form_for was being used to generate for a new/existing model object. - form_tag was used to create form withour a model object by passing a URL to submit the form. Before Rails 5.1 form_for with model object - form_for was used when we had to create a form for a model object. <%= form_for User.new do |form| %> <%= form.text_field :name%> <%= form.text_field :email%> <%= form.submit%> <% end %> This generates DOM shown below. <form class=\"new_user\" id=\"new_user\" actionn=\"/users\" accept-charset=\"UTF-8\" method=\"post\"> <input name=\"utf8\" type=\"hidden\" value=\"&#x2712;\" /> <input type=\"hidden\" name=\"authenticity_token\" value=\"token_value\"/> <input type=\"text\" name=\"user[password]\" id=\"user_password\"/> <input type=\"text\" name=\"user[email]\" value=\"\" id=\"user_email\"/> <input type=\"submit\" name=\"submit\" value=\"create User\" data-disable-with=\"Create User\" /> </form> Behavior: We can see Rails automatically assigns id attribute value to the new_user if record is new. id attribute is set to edit_user_<id> , where <id> is the primary key of users table. It fills out the input values if the model object is not new. After Rails 5.1 form_with with a model object <%= form_with model: @user do |form| %> <%= form.text_field :email %> <%= form.submit %> <% end %> This generates DOM given below. <form action=\"/users\" accept-charset=\"UTF-8\" data-remote=\"true\" method=\"post\"> <input name=\"utf8\" type=\"hidden\" value=\"&#x2713\"/> <input type=\"hidden\" name=\"authenticaity_token\" value=\"token_value\"/> <input type=\"text\" name=\"email\" /> <input type=\"submit\" name=\"commit\" value=\"Save \" data-disable-with=\"SAVE \"> </form> Behavior: Automatic IDs for the form are gone. The above way of creating form works for both new and existing records. It fills out the input values if the model object is not new. Before Rails 5.1 form_tag Without model object - form_tag was used whrn we had to create to form without any model object providing URL endpoint to submit the form. <%= form_tag users_path do %> <%= text_field_tag :name %> <%= text_field_tag :email %> <%= submit_tag %> <% end %> Both helper methods used to create DOM of the form tag with necessary DOM content. <form action=\"/users\" accept-charset=\"UTF-8\" method=\"post\"> <input name=\"utf8\" type=\"hidden\" value=\"&#x2713\"/> <input type=\"hidden\" name=\"authenticity_token\" value=\"token_value\"/> <input type=\"text\" name=\"name\" id=\"name\"/> <input type=\"text\" name=\"email\" id=\"email\"/> <input type=\"submit\" name=\"commit\" value=\"Save Changes\" data-disable-with=\"Save Changes\"/> </form> After Rails 5.1 form_with withour a model object form_with provides an option to pass a URL to be used for the submit action. <%= form_with url: users_path do |form| %> <%= form.text_field :email%> <%= form.submit %> <% end %> This generates DOM given below. <form action=\"/users\" accept-charset=\"UTF-8\" data-remote=\"true\" method=\"post\"> <input name=\"utf8\" type=\"hidden\" value=\"&#x2713;\"> <input type=\"hidden\" name=\"authenticity_token\" value=\"token_value\"> <input type=\"text\" name=\"email\"> <input type=\"submit\" name=\"commit\" value=\"Save \" data-disable-with=\"Save \" /> </form> form_with with a scope (prefix) new, we can see that above code, does not prefix input fields as it does when generated with model object. to generate input fields with some prefix, Rails provides scope option. <%= form_with url: users_path, scope: :user do |form| %> <%= form.text_field :name %> <%= form.text_field :email %> <%= form.submit %> <% end %> This generates DOM given below. <form action=\"/users\" accept-charset=\"UTF-8\" data-remote=\"true\" method=\"post\"/> <input name=\"utf8\" type=\"hidden\" value=\"&#x2713\"/> <input type=\"hidden\" name=\"authenticity_token\" value=\"token_value\"/> <input type=\"text\" name=\"user[name]\"/> <input type=\"text\" name=\"userp[email]\"/> <input type=\"submit\" name=\"commit\" value=\"Save User\" data-disable-with=\"Save User\"/> </form> form_with ajax submit events As we can see form_with adds data-remote attribute with value set to true . This makes sure that form is submitted with AJAX if unobtrusive javascript driver like rails-ujs is used. if your application uses rails-ujs , the form will be submitted via ajax, and it listens on following events. - ajax:success : This event is called when Ajax response is success. - ajax:error : This event is called when Ajax response is failure. Event can be binded on form as given below. $(document).on('ajax:success','#new_user', functionn(e) { console.log('form_with: succcessfully submitted form via ajax'); }); $(document).on('ajax:error', '#new_user', function(e){ console.log('form_with: error submitting form via ajax'); })","title":"Rails Form Expand"},{"location":"knowledge/web-development/ruby-on-rails/rails-form-expand/#rails-form_with-alternative-to-form_for-and-form_tag","text":"Rails 5.1 added form_with form helper method that provides capabilities of form_for and form_tag . Rails unified form_for and form_tag that provide similar interfaces to generate forms with form_with helper. - form_for was being used to generate for a new/existing model object. - form_tag was used to create form withour a model object by passing a URL to submit the form.","title":"Rails form_with - alternative to form_for and form_tag"},{"location":"knowledge/web-development/ruby-on-rails/rails-form-expand/#before-rails-51","text":"form_for with model object - form_for was used when we had to create a form for a model object. <%= form_for User.new do |form| %> <%= form.text_field :name%> <%= form.text_field :email%> <%= form.submit%> <% end %> This generates DOM shown below. <form class=\"new_user\" id=\"new_user\" actionn=\"/users\" accept-charset=\"UTF-8\" method=\"post\"> <input name=\"utf8\" type=\"hidden\" value=\"&#x2712;\" /> <input type=\"hidden\" name=\"authenticity_token\" value=\"token_value\"/> <input type=\"text\" name=\"user[password]\" id=\"user_password\"/> <input type=\"text\" name=\"user[email]\" value=\"\" id=\"user_email\"/> <input type=\"submit\" name=\"submit\" value=\"create User\" data-disable-with=\"Create User\" /> </form>","title":"Before Rails 5.1"},{"location":"knowledge/web-development/ruby-on-rails/rails-form-expand/#behavior","text":"We can see Rails automatically assigns id attribute value to the new_user if record is new. id attribute is set to edit_user_<id> , where <id> is the primary key of users table. It fills out the input values if the model object is not new.","title":"Behavior:"},{"location":"knowledge/web-development/ruby-on-rails/rails-form-expand/#after-rails-51","text":"form_with with a model object <%= form_with model: @user do |form| %> <%= form.text_field :email %> <%= form.submit %> <% end %> This generates DOM given below. <form action=\"/users\" accept-charset=\"UTF-8\" data-remote=\"true\" method=\"post\"> <input name=\"utf8\" type=\"hidden\" value=\"&#x2713\"/> <input type=\"hidden\" name=\"authenticaity_token\" value=\"token_value\"/> <input type=\"text\" name=\"email\" /> <input type=\"submit\" name=\"commit\" value=\"Save \" data-disable-with=\"SAVE \"> </form>","title":"After Rails 5.1"},{"location":"knowledge/web-development/ruby-on-rails/rails-form-expand/#behavior_1","text":"Automatic IDs for the form are gone. The above way of creating form works for both new and existing records. It fills out the input values if the model object is not new.","title":"Behavior:"},{"location":"knowledge/web-development/ruby-on-rails/rails-form-expand/#before-rails-51_1","text":"form_tag Without model object - form_tag was used whrn we had to create to form without any model object providing URL endpoint to submit the form. <%= form_tag users_path do %> <%= text_field_tag :name %> <%= text_field_tag :email %> <%= submit_tag %> <% end %> Both helper methods used to create DOM of the form tag with necessary DOM content. <form action=\"/users\" accept-charset=\"UTF-8\" method=\"post\"> <input name=\"utf8\" type=\"hidden\" value=\"&#x2713\"/> <input type=\"hidden\" name=\"authenticity_token\" value=\"token_value\"/> <input type=\"text\" name=\"name\" id=\"name\"/> <input type=\"text\" name=\"email\" id=\"email\"/> <input type=\"submit\" name=\"commit\" value=\"Save Changes\" data-disable-with=\"Save Changes\"/> </form>","title":"Before Rails 5.1"},{"location":"knowledge/web-development/ruby-on-rails/rails-form-expand/#after-rails-51_1","text":"form_with withour a model object form_with provides an option to pass a URL to be used for the submit action. <%= form_with url: users_path do |form| %> <%= form.text_field :email%> <%= form.submit %> <% end %> This generates DOM given below. <form action=\"/users\" accept-charset=\"UTF-8\" data-remote=\"true\" method=\"post\"> <input name=\"utf8\" type=\"hidden\" value=\"&#x2713;\"> <input type=\"hidden\" name=\"authenticity_token\" value=\"token_value\"> <input type=\"text\" name=\"email\"> <input type=\"submit\" name=\"commit\" value=\"Save \" data-disable-with=\"Save \" /> </form> form_with with a scope (prefix) new, we can see that above code, does not prefix input fields as it does when generated with model object. to generate input fields with some prefix, Rails provides scope option. <%= form_with url: users_path, scope: :user do |form| %> <%= form.text_field :name %> <%= form.text_field :email %> <%= form.submit %> <% end %> This generates DOM given below. <form action=\"/users\" accept-charset=\"UTF-8\" data-remote=\"true\" method=\"post\"/> <input name=\"utf8\" type=\"hidden\" value=\"&#x2713\"/> <input type=\"hidden\" name=\"authenticity_token\" value=\"token_value\"/> <input type=\"text\" name=\"user[name]\"/> <input type=\"text\" name=\"userp[email]\"/> <input type=\"submit\" name=\"commit\" value=\"Save User\" data-disable-with=\"Save User\"/> </form> form_with ajax submit events As we can see form_with adds data-remote attribute with value set to true . This makes sure that form is submitted with AJAX if unobtrusive javascript driver like rails-ujs is used. if your application uses rails-ujs , the form will be submitted via ajax, and it listens on following events. - ajax:success : This event is called when Ajax response is success. - ajax:error : This event is called when Ajax response is failure. Event can be binded on form as given below. $(document).on('ajax:success','#new_user', functionn(e) { console.log('form_with: succcessfully submitted form via ajax'); }); $(document).on('ajax:error', '#new_user', function(e){ console.log('form_with: error submitting form via ajax'); })","title":"After Rails 5.1"}]}